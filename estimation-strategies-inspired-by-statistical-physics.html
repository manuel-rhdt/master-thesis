<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js Light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mutual Information between Trajectories</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
        <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var regex = /\\qquad\W*\(([0-9]+)\)/;
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    var tex_str = texText.data.replace(regex, "\\tag{$1}");
                    katex.render(tex_str, mathElements[i], {
                    displayMode: mathElements[i].classList.contains('display'),
                    throwOnError: false,
                    fleqn: false
                });
            }}});
        </script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "" : "Light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('Light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded"><a href="index.html" class=""><span class="header-section-number">1</span> Preface
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html" class=""><span class="header-section-number">2</span> Mutual Information for Trajectories in a Gaussian Framework
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#introduction" class=""><span class="header-section-number">2.1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">2.2</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#choice-of-covariance-matrices" class=""><span class="header-section-number">2.2.1</span> Choice of Covariance Matrices
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#direct-importance-sampling" class=""><span class="header-section-number">2.2.2</span> Direct Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#umbrella-sampling" class=""><span class="header-section-number">2.2.3</span> Umbrella Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">2.3</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#discussion" class=""><span class="header-section-number">2.4</span> Discussion
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html" class=""><span class="header-section-number">3</span> Information and Noise in Biological Systems
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#mutual-information-as-an-efficiency-measure-in-cell-signalling" class=""><span class="header-section-number">3.1</span> Mutual Information as an Efficiency Measure in Cell Signalling
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#monte-carlo-simulation" class=""><span class="header-section-number">3.2</span> Monte-Carlo simulation
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#estimating-the-likelihood" class=""><span class="header-section-number">3.3</span> Estimating the likelihood
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#the-probability-density-for-the-starting-point-of-a-trajectory" class=""><span class="header-section-number">3.3.1</span> The probability density for the starting point of a trajectory
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#the-transition-probabilities" class=""><span class="header-section-number">3.3.2</span> The transition probabilities
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#estimating-the-marginal-probability-of-response-trajectories" class=""><span class="header-section-number">3.3.3</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#chemical-master-equation" class=""><span class="header-section-number">3.4</span> Chemical Master Equation
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#jump-processes" class=""><span class="header-section-number">3.5</span> Jump Processes
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#simulating-a-biochemical-network-driven-by-an-external-signal" class=""><span class="header-section-number">3.6</span> Simulating a Biochemical Network Driven by an External Signal
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html" class="active"><span class="header-section-number">4</span> Estimation Strategies Inspired by Statistical Physics
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#borrowing-terminology-from-statistical-physics" class=""><span class="header-section-number">4.1</span> Borrowing Terminology from Statistical Physics
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#thermodynamic-integration" class=""><span class="header-section-number">4.2</span> Thermodynamic Integration
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#summary-of-ti" class=""><span class="header-section-number">4.2.1</span> Summary of TI
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#markov-chain-monte-carlo" class=""><span class="header-section-number">4.2.2</span> Markov Chain Monte Carlo
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#estimating-the-density-of-states" class=""><span class="header-section-number">4.3</span> Estimating the Density of States
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#wang-and-landau-algorithm" class=""><span class="header-section-number">4.4</span> Wang and Landau Algorithm
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#applying-wang-landau-to-the-computation-of-the-marginal-density" class=""><span class="header-section-number">4.4.1</span> Applying Wang-Landau to the Computation of the Marginal Density
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#connection-to-standard-monte-carlo-sampling" class=""><span class="header-section-number">4.4.2</span> Connection to Standard Monte-Carlo Sampling
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#example-results-for-a-wang-landau-simulation" class=""><span class="header-section-number">4.4.3</span> Example Results for a Wang-Landau Simulation
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#conclusion" class=""><span class="header-section-number">4.5</span> Conclusion
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="chapter_5.html" class="">
                    </a>
                    </li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                    </div>

                    <h1 class="menu-title">Mutual Information between Trajectories</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/manuel-rhdt/master-thesis" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <section id="estimation-strategies-inspired-by-statistical-physics" class="level1" data-number="1">
<h1 data-number="4"><span class="header-section-number">4</span> Estimation Strategies Inspired by Statistical Physics</h1>
<section id="borrowing-terminology-from-statistical-physics" class="level2" data-number="1.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span> Borrowing Terminology from Statistical Physics</h2>
<p>In the context of Bayesian Inference the terms of Bayes’ formula <span id="eq:bayes_thm"><span class="math display">
\mathrm P(\mathbf s | \mathbf x) = \frac{\mathrm P(\mathbf x|\mathbf s)\ \mathrm P(\mathbf s)}{\mathrm P(\mathbf x)}
\qquad(1)</span></span> are typically considered as <span class="math inline">\mathrm P(\mathbf s)</span> being the <em>prior</em> probability of…</p>
<p>In the framework employed by statistical physics, Bayes’ theorem corresponds to the canonical ensemble distribution of <span class="math inline">\mathbf s</span> (for <span class="math inline">\beta=1</span>) <span><span class="math display">
\mathrm P(\mathbf s | \mathbf x) = \frac{1}{Z(\mathbf x)}\exp\left[-E(\mathbf s, \mathbf x)\right]
\qquad(2)</span></span> where the <em>partition function</em> is defined by <span class="math inline">Z(\mathbf x) = \int \mathrm d\mathbf s\ \exp\left[-E(\mathbf s, \mathbf x)\right]</span> and <span class="math inline">E(\mathbf s, \mathbf x)</span> denotes the total energy of the system at state <span class="math inline">\mathbf s</span>. In this context <span class="math inline">\mathbf x</span> is considered a parameter vector for the specific model used to compute the energy. In classical problems of statistical physics (such as e.g. the <em>Ising model</em>) the state space spans the single particle states <span class="math inline">\mathbf{s} = (\sigma_1,\ldots,\sigma_n)\in\Omega^n</span> for all particles and the energy is given by the <em>Hamiltonian</em> <span class="math inline">\mathcal H(\sigma_1,\ldots,\sigma_n;\mathbf x)</span> where <span class="math inline">\mathbf x</span> could contain parameters describing e.g. the interaction strength between neighbouring spins. In our case however we define our energy function by comparison with eq. <a href="#eq:bayes_thm">1</a> as <span><span class="math display">E(\mathbf s, \mathbf x) = -\ln\mathrm P(\mathbf x|\mathbf s)-\ln\mathrm P(\mathbf s)\,.\qquad(3)</span></span> From this point of view the marginal density <span class="math inline">\mathrm P(\mathbf x) = Z(\mathbf x)</span> <em>is</em> the partition function of the canonical ensemble. In statistical physics the partition function is of central importance since its partial derivatives include all thermodynamic properties of a physical system. The free energy of the canonical ensemble is defined by <span class="math inline">F(\mathbf x) = -\ln Z(\mathbf x)</span> (for <span class="math inline">\beta=1</span>) such that using this terminology we can write the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> as an average over the <em>“free energies of response trajectories”</em> <span><span class="math display">
\mathrm H(\mathcal X) = \int\mathrm d\mathbf x\ \mathrm P(\mathbf x)\ F(\mathbf x) = \left\langle F(\mathbf x) \right\rangle_{\mathrm P(\mathbf x)}\,.
\qquad(4)</span></span></p>
<p>Since the computation of the partition function is central to the solution of many statistical problems there has been done considerable work on efficient estimation of the partition function, the free energy and other related quantities such as the <em>density of states</em>.</p>
</section>
<section id="thermodynamic-integration" class="level2" data-number="1.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span> Thermodynamic Integration</h2>
<p>One well-established technique to estimate free energy (differences) is by thermodynamic integration (TI) <span class="citation" data-cites="1998:Gelman">[<a href="#ref-1998:Gelman" role="doc-biblioref">1</a>]</span>. It allows the accurate computation of the ratio between the normalization constants of two different probability distributions using a continuous path in <em>distribution space</em> that connects both. Since this strategy uses random samples taken from many different distributions along this path it is especially robust when the two distributions have very little overlap. For the computation of the marginal density <span class="math inline">\mathrm P(\mathbf x)</span> we can (for a given <span class="math inline">\mathbf x</span>) define a suitable path in distribution space between <span class="math inline">\mathrm P(\mathbf s)</span> and <span class="math inline">\mathrm P(\mathbf s, \mathbf x)</span>. The normalization constants of these distributions are <span class="math inline">z_0 = 1</span> and <span class="math inline">z_1 = \mathrm P(\mathbf x)</span>, respectively such that the ratio <span class="math inline">r=z_1/z_0</span> of these normalization constants directly corresponds to the marginal density. Using TI we estimate this ratio using approximately independent samples from a <em>Markov chain Monte Carlo</em> (MCMC) simulation.</p>
<p>In the following sections we will give a quick summary of TI followed by an explanation of the Markov chain Monte Carlo simulation and a discussion of the resulting accuracy of the estimates.</p>
<section id="summary-of-ti" class="level3" data-number="1.2.1">
<h3 data-number="4.2.1"><span class="header-section-number">4.2.1</span> Summary of TI</h3>
<p>Let <span class="math inline">q_0</span> and <span class="math inline">q_1</span> be the unnormalized distribution functions and <span class="math inline">z_0, z_1</span> the corrsponding normalization constants such that <span class="math inline">z_i=\int\mathrm d\mathbf s\ q_i(\mathbf s)</span>. Next we construct a path between <span class="math inline">q_0</span> and <span class="math inline">q_1</span>, parametrized by <span class="math inline">\theta\in[0,1]</span> such that <span class="math inline">q_\theta</span> smoothly connects the end points. We similarly define <span class="math inline">z(\theta)</span> as the normalization constant of <span class="math inline">q_\theta</span>. A smooth path that can be constructed for any pair of distributions <span class="math inline">(q_0, q_1)</span> is the <em>geometric path</em> given by <span class="math inline">q_\theta=q^{1-\theta}_0\ q^\theta_1</span>. Note however that variance of the estimate depends on the chosen path and that the geometric path is not the optimal path in general.</p>
<p>For the estimation of free energy differences we are interested in the ratio <span class="math inline">r=z(1)/z(0)</span>. To find an estimate we differentiate the logarithm of <span class="math inline">z(\theta)</span> with respect to <span class="math inline">\theta</span> to arrive at <span><span class="math display">
\frac{\mathrm d\ln z(\theta)}{\mathrm d\theta} = \frac{1}{z(\theta)} \frac{\partial}{\partial\theta}  \int\mathrm d\mathbf s\ q_\theta(\mathbf s) = \int\mathrm d\mathbf s\ \frac{q_\theta(\mathbf s)}{z(\theta)} \frac{\partial}{\partial\theta} \ln q_\theta(\mathbf s) = \left\langle \frac{\partial}{\partial\theta} \ln q_\theta(\mathbf s) \right\rangle_{p_\theta(\mathbf s)}
\qquad(5)</span></span> where <span class="math inline">p_\theta(\mathbf s) = q_\theta(\mathbf s)/z(\theta)</span> is the normalized probability distribution corresponding to <span class="math inline">q_\theta</span>. By analogy to the potential in statistical physics we define <span><span class="math display">
U(\mathbf s, \theta) = -\frac{\partial}{\partial\theta} \ln q_\theta(\mathbf s)\,.
\qquad(6)</span></span> Now we can express the log-ratio <span class="math inline">\lambda=\ln r</span> by the integral <span id="eq:path_sampling_int"><span class="math display">
\lambda = \ln z(1) - \ln z(0) = -\int\limits^1_0 \mathrm d\theta\ \left\langle 
U(\mathbf s, \theta)
\right\rangle_{p_\theta(\mathbf s)}
\qquad(7)</span></span> which forms the basis of all thermodynamic integration estimates. One advantage of the TI estimators is that we directly estimate the log-ratio <span class="math inline">\lambda</span>, i.e. the free energy difference as opposed to the ratio of partition functions. Indeed, to eventually compute the marginal entropy <span class="math inline">\mathrm H(\mathcal X) = -\langle\ln P(\mathbf x)\rangle</span> we require the logarithm of the marginal density thus no further error is introduced by taking the logarithm of an estimated quantity.</p>
<p>Using the previous identities, one possible way to estimate <span class="math inline">\lambda</span> is to regard <span class="math inline">\theta</span> as a random variable with a density <span class="math inline">p(\theta)</span>, allowing us to compute the integral in eq. <a href="#eq:path_sampling_int">7</a> using the Monte Carlo estimator <span id="eq:lambda_mc"><span class="math display">
\hat{\lambda}_\text{MC} = -\frac{1}{n} \sum\limits^n_{i=1}\frac{U(\mathbf s_i, \theta_i)}{p(\theta_i)}
\qquad(8)</span></span> with draws <span class="math inline">(\mathbf s_1, \theta_1),\ldots,(\mathbf s_n, \theta_n)</span> from the joint probability density <span class="math inline">p(\mathbf s, \theta) = p_\theta(\mathbf s)\ p(\theta)</span>. Alternatively, we can perform numerical integration using the trapezoidal rule by evaluating the potential over values <span class="math inline">\theta_1\cdots\theta_{n-1}</span> between <span class="math inline">\theta_0=0</span> and <span class="math inline">\theta_n=1</span> <span id="eq:lambda_ni"><span class="math display">
\hat{\lambda}_\text{NI} = -\sum\limits^n_{i=1}\frac{
  \langle U(\mathbf s, \theta_{i-1}) \rangle_{p_{\theta_{i-1}}(\mathbf s) }
  + \langle U(\mathbf s, \theta_{i}) \rangle_{p_{\theta_{i}}(\mathbf s) }
  }{2} (\theta_i - \theta_{i-1})
\qquad(9)</span></span> where each average over the potential is performed using a Monte Carlo simulation.</p>
<p>To use these estimators for the computation of the marginal density <span class="math inline">\mathrm P(\mathbf x)</span> at a given <span class="math inline">\mathbf x</span> we need to construct a path between the densities <span class="math inline">q_0(\mathbf s) = \mathrm P(\mathbf s)</span> and <span class="math inline">q_1(\mathbf s) = \mathrm P(\mathbf s)\mathrm P(\mathbf x|\mathbf s)</span>. For simplicity and convenience we choose the geometric path <span class="math inline">q_\theta(\mathbf s) = \mathrm P(\mathbf s)\ [\mathrm P(\mathbf x|\mathbf s)]^\theta</span>. Taking the logarithm of this density we get <span class="math inline">\ln q_\theta(\mathbf s) = \ln P(\mathbf s) + \theta \ln \mathrm P(\mathbf x|\mathbf s)</span> which prompts us to define the “energy” of a signal trajecory with respect to <span class="math inline">\theta</span> as <span><span class="math display">
E(\mathbf s, \theta) = -\ln q_\theta(\mathbf s) = -\ln P(\mathbf s) - \theta \ln \mathrm P(\mathbf x|\mathbf s)\,.
\qquad(10)</span></span> For <span class="math inline">\theta = 1</span> this definition of the energy matches our previous definition by analogy with the canonical ensemble whereas for <span class="math inline">\theta = 0</span> this definition of the energy is equivalent to the energy of a signal trajectory for a system where <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> are completely independent and thus <span class="math inline">\mathrm P(\mathbf s|\mathbf x) = \mathrm P(\mathbf s)</span>. Thus, this nomenclature also motivates the name “potential” for the quantity <span><span class="math display">
U(\mathbf s, \theta) = - \frac{\partial}{\partial\theta} \ln q_\theta(\mathbf s) = \frac{\partial}{\partial\theta} E_\theta(\mathbf s) = -\ln \mathrm P(\mathbf x|\mathbf s)
\qquad(11)</span></span> i.e. <span class="math inline">\theta</span> acts as a <em>“knob”</em> that allows us to gradually turn the potential on or off. The potential term itself characterizes the amount of dependence between the random variables <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span>. Note that all energetic quantities depend on the specific response <span class="math inline">\mathbf x</span> (except at <span class="math inline">\theta=0</span>) even if this dependence is suppressed in the notation.</p>
<p>To use the TI estimators introduced in eqns. <a href="#eq:lambda_mc">8</a>, <a href="#eq:lambda_ni">9</a> we need to generate samples from arbitrary distributions along our chosen geometric path. Since we can compute the unnormalized densities of these distributions, we can use the Metropolis-Hastings algorithm as a very general method to sample from arbitrary distributions <span class="citation" data-cites="1970:Hastings">[<a href="#ref-1970:Hastings" role="doc-biblioref">2</a>]</span>.</p>
</section>
<section id="markov-chain-monte-carlo" class="level3" data-number="1.2.2">
<h3 data-number="4.2.2"><span class="header-section-number">4.2.2</span> Markov Chain Monte Carlo</h3>
<p>To generate approximately independent samples from a distribution given by the unnormalized density <span class="math inline">q_\theta</span> we start from an (in principle arbitrary) initial signal <span class="math inline">\mathbf s</span>. Next, a new signal <span class="math inline">\mathbf s^\prime</span> is proposed from the proposal distribution <span class="math inline">\mathrm T(\mathbf s \rightarrow \mathbf s^\prime)</span> which is typically chosen to yield a <span class="math inline">\mathbf s^\prime</span> close to <span class="math inline">\mathbf s</span>. Then with some probability <span class="math inline">A(\mathbf s^\prime, \mathbf s)</span> we <em>accept</em> the new configuration and our first generated sample is <span class="math inline">\mathbf s_1 = \mathbf s^\prime</span>. Otherwise we <em>reject</em> the new configuration and our first sample is equal to the initial signal <span class="math inline">\mathbf s_1 = \mathbf s</span>. For the next iteration of the algorithm we then set our new initial signal to be <span class="math inline">\mathbf s \leftarrow \mathbf s_1</span> such that when we repeat this procedure many times we generate a sequence of signals <span class="math inline">\mathbf s_1, \mathbf s_2, \ldots</span> where each sample is a random value only directly dependent on the immediately preceding sample. Thus we have defined a Markov process that generates a <em>chain</em> of signals with the transition probability given by <span class="math inline">\mathrm P(\mathbf s \rightarrow \mathbf s^\prime) = T(\mathbf s \rightarrow \mathbf s^\prime)\,A(\mathbf s^\prime, \mathbf s)</span>. We want to choose the acceptance probability <span class="math inline">A(\mathbf s^\prime, \mathbf s)</span> such that the stationary distribution of this Markov process is precisely <span class="math inline">q_\theta</span>. It can be shown that the <em>Metropolis choice</em> <span><span class="math display">
A(\mathbf s, \mathbf s^\prime) = \min\left( 1, \frac{q_\theta(\mathbf s^\prime)}{q_\theta(\mathbf s)} \frac{\mathrm T(\mathbf s^\prime \rightarrow \mathbf s)}{\mathrm T(\mathbf s \rightarrow \mathbf s^\prime)} \right)
\qquad(12)</span></span> leads to the correct stationary distribution given that the system is ergodic.</p>
<p>TODO: While this algorithm has some disadvantages (dependence of samples, yada yada) it often is the only sampling strategy that works at all in very high-dimensional spaces or complex distributions (is also well parallelizable)…</p>
<figure>
<img src="figures/monte_carlo_sims.svg" id="fig:monte_carlo_sims" alt="" /></img><figcaption>Figure 1: Visualization.</figcaption>
</figure>
<figure>
<img src="figures/mcmc_covariance_comparison.svg" id="fig:mcmc_covariance" alt="" /></img><figcaption>Figure 2: Comparison of the covariance matrices obtained a) by computing the empirical covariance of 1000 approximately uncorrelated samples taken from the MCMC procedure (for <span class="math inline">\theta = 1</span>) and b) by analytically computing the covariance matrix of the normal distribution <span class="math inline">\mathrm P(\mathbf s|\mathbf x)</span>. The proposal distribution is a multivariate normal distribution with covariance <span class="math inline">\Sigma=\sigma^{-2} \mathbb I</span>, with a value of <span class="math inline">\sigma=0.01</span>.</figcaption>
</figure>
<p>For the Gaussian system we choose the proposal distribution <span class="math inline">\mathrm T(\mathbf s \rightarrow \mathbf s^\prime)</span> to be a multivariate normal distribution centered around <span class="math inline">\mathbf s</span> and with uniform covariance <span class="math inline">\Sigma=\sigma^{-2} \mathbb I</span>. In fig. <a href="#fig:mcmc_covariance">2</a> we show that using the Metropolis-Hastings algorithm we can generate samples with an appropriate distribution that matches the analytical expectation. In fig. <a href="#fig:thermodynamic_int_results">3</a> we show the averaged potentials for 216 MCMC runs for different values of <span class="math inline">\theta</span>. From these potentials we can the compute the marginal density using the estimator from eq. <a href="#eq:lambda_mc">8</a>. The results are very promising since the estimated value differs by merely 0.012 % from the analytically correct value of <span class="math inline">\mathrm P(\mathbf x)</span>.</p>
<figure>
<img src="figures/mcmc_theromdynamic_integration.svg" id="fig:thermodynamic_int_results" alt="" /></img><figcaption>Figure 3: Samples of the averaged potential for different values of <span class="math inline">\theta</span>. There are 216 samples for values of <span class="math inline">\theta</span> chosen uniformly distributed in the interval <span class="math inline">[0, 1]</span>. Every point is an individual MCMC simulation with 1000 approximately independent draws. The bars on the right show a histogram of the log-likelihoods. The TI estimate is the integral from <span class="math inline">\theta=0</span> to <span class="math inline">1</span> of the curve that the individual samples approximate. Using the estimate from eq. <a href="#eq:lambda_mc">8</a>, the estimated value differs by merely 0.012 % from the analytically correct value of <span class="math inline">\mathrm P(\mathbf x)</span>. This shows that given enough samples, TI is able to provide very accurate results for the marginal density.</figcaption>
</figure>
</section>
</section>
<section id="estimating-the-density-of-states" class="level2" data-number="1.3">
<h2 data-number="4.3"><span class="header-section-number">4.3</span> Estimating the Density of States</h2>

<p>In the context of statistical physics we often look at configurations of a system that can be described by a parameter vector <span class="math inline">\mathbf{n}\in\Omega</span> where <span class="math inline">\Omega</span> is the state space of the system. We can typically assign a probability (density) to each configuration. For example, let’s consider the canonical ensemble for a given inverse temperature <span class="math inline">\beta</span> and Hamiltonian <span class="math inline">\mathcal H</span> <span id="eq:canonical_probability"><span class="math display">
\mathrm{P}(\mathbf{n}) = \frac{1}{Z(\beta)} e^{-\beta \mathcal H(\mathbf{n})}
\qquad(13)</span></span> with the <em>partition function</em> <span class="math inline">Z(\beta)=\int \mathrm{d}\mathbf{n}\ e^{-\beta H(\mathbf{n})}</span>. The Hamiltonian assigns an energy to every state, i.e. for every state <span class="math inline">\mathbf{n}</span> we have an associated energy <span class="math inline">\mathcal H(\mathbf{n})</span>. To learn more about the distribution of energies in our system we can now define the <em>density of states</em> <span class="math inline">g(E)</span> at a given energy <span class="math inline">E</span> as the probability density of a random<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> state <span class="math inline">\hat{\mathbf{n}}</span> to have energy <span class="math inline">\mathcal H(\hat{\mathbf{n}}) = E</span>. More precisely, let <span class="math inline">\mathcal{N}</span> be a random variable uniformly distributed in the state space, then <span><span class="math display">
g(E) = \mathrm{P}\left(\mathcal H(\mathcal N) = E\right)\,.
\qquad(14)</span></span></p>
<p>The density of states (<em>DOS</em>) thus describes the contribution of individual energy levels to the ensemble average of quantities that merely depend on the energy of a state. That is, we can compute the ensemble average <span class="math inline">\langle f(\mathcal H(\mathbf{n}))\rangle</span> of any function <span class="math inline">f</span> that depends only on the energy of a given state as <span id="eq:def_dos"><span class="math display">
\langle f(\mathcal H(\mathbf{n}))\rangle = \frac{\int_\Omega\mathrm d\mathbf n\ f(\mathcal H(\mathbf n)) e^{-\beta \mathcal H(\mathbf{n})}}{\int_\Omega\mathrm d\mathbf n\ e^{-\beta \mathcal H(\mathbf{n})}} = 
\frac{
\int\mathrm dE\ g(E) f(E) e^{-\beta E}
 }{
   \int\mathrm dE\ g(E) e^{-\beta E}
 }
\qquad(15)</span></span> where <span class="math inline">\int_\Omega\mathrm d\mathbf{n}</span> denotes an integral over phase space. Eq. <a href="#eq:def_dos">15</a> motivates the common way of specifying the DOS using the Dirac delta function <span id="eq:dirac_dos"><span class="math display">
g(E) = \int\limits_\Omega \mathrm d\mathbf n\ \delta(\mathcal H(\mathbf n) - E)
\qquad(16)</span></span> which matches the intuition of plotting an energy histogram for randomly chosen states. I.e. for discrete energies <span class="math inline">E_1\cdotsE_n</span> (the histogram bins) and random states <span class="math inline">\mathbf n_1,\ldots,\mathbf n_N</span> we can approximate the DOS as <span id="eq:dos_histogram"><span class="math display">
g_\text{discrete}(E_i) = \frac1N \sum\limits^N_{j=1} \delta_{\mathcal H(\mathbf n_j), E_i}
\qquad(17)</span></span> where <span class="math inline">\delta_{\epsilon, E_i}</span> is <span class="math inline">1</span> if the energy <span class="math inline">\epsilon</span> falls inside the <span class="math inline">i</span>-th histogram bin and <span class="math inline">0</span> otherwise. As the number of random states and the number of histogram bins grow towards infinity, <span class="math inline">g_\text{discrete}</span> converges to eq. <a href="#eq:dirac_dos">16</a>.</p>
<p>For us, the DOS is of relevance because can be used to compute the partition function <span id="eq:partition_fn_from_dos"><span class="math display">
Z(\beta) = \int\limits_\Omega \mathrm{d}\mathbf{n}\ e^{-\beta H(\mathbf{n})} = \int \mathrm dE\ g(E) e^{-\beta E}
\qquad(18)</span></span> and thus the free energy. In the following we will discuss the Wang and Landau algorithm to estimate the DOS and evaluate its usefulness for the computation of the marginal density of a trajectory.</p>

</section>
<section id="wang-and-landau-algorithm" class="level2" data-number="1.4">
<h2 data-number="4.4"><span class="header-section-number">4.4</span> Wang and Landau Algorithm</h2>
<p>Since the state spaces <span class="math inline">\Omega</span> are usually very large, one typically resorts to Monte-Carlo methods to estimate the density of states. There one generates a sequence of states <span class="math inline">\mathbf{n}_i</span> that are approximately independent and distributed according to <span class="math inline">\mathrm{P}(\mathcal{N})</span>, e.g. by using the Metropolis-Hastings algorithm. For every sampled state <span class="math inline">\mathbf{n}_i</span> we can compute the Energy <span class="math inline">\mathcal H(\mathbf{n}_i)</span> and then approximate the density of states by a histogram of the energy values as in eq. <a href="#eq:dos_histogram">17</a>. To get an accurate estimate of the density of states for energy values <span class="math inline">E</span> where <span class="math inline">g(E)</span> is very small we need a lot of iterations since we will on average pick very few samples with low probability.</p>
<p>The approach of Wang and Landau <span class="citation" data-cites="2001:Wangg8b">[<a href="#ref-2001:Wangg8b" role="doc-biblioref">3</a>]</span> is instead to not generate samples that are distributed according to the equilibrium distribution <span class="math inline">\mathrm{P}(\mathbf{n})</span> but to adaptively vary the sampling distribution throughout the simulation. This is done such that for every energy value <span class="math inline">E</span> approximately the same number of samples are acquired such that the DOS can be accurately estimated even in regions of low density <span class="math inline">g(E)</span>.</p>
<p>The main idea is to perform a random walk in energy space such that on average all energy levels in a predefined interval are visited equally often. If we switch from energy space to state space this implies that the probability density for this random walk to visit a state <span class="math inline">\mathbf n</span> is proportional to the reciprocal density of states <span class="math inline">1/g[\mathcal H(\mathbf n)]</span>. Of course we can’t sample directly using the reciprocal DOS since it is unknown. Instead, at each step of the algorithm we slightly alter our sampling distribution until the histogram of energy values becomes <em>flat</em>. Once the histogram is flat we conclude that the adaptively altered sampling distribution represents precisely the reciprocal DOS.</p>
<p>To perform Wang-Landau sampling we have to define energy bins <span class="math inline">E_1,\ldots,E_n</span> that represent the range of energies that we want to compute the DOS for. We start by setting <span class="math inline">g(E_i)=1</span> for all <span class="math inline">i=1,\ldots,n</span>. Then—similarly to Metropolis-Hastings sampling—we iteratively propose and selectively accept new configurations such that the transition probability from energy level <span class="math inline">E_i\rightarrow E_j</span> is <span><span class="math display">
p(E_i\rightarrow E_j) = \min\left( 1, \frac{g(E_i)}{g(E_j)} \right)\,.
\qquad(19)</span></span> After each proposal we update a histogram of visited energies <span class="math inline">H(E_j)\leftarrow H(E_j) + 1</span> and modify the density of states at <span class="math inline">E_j</span> by a constant factor <span class="math inline">g(E_j)\leftarrow f\ g(E_j)</span>. This updating of the sampling distribution during the simulation is precisely what makes the random walk non-Markovian and promises fast convergence towards the correct DOS. We start the procedure with the factor <span class="math inline">f=\exp(1)=e</span>. Once the histogram <span class="math inline">H(E)</span> is sufficiently flat (we use 95% flatness) we update <span class="math inline">f\leftarrow \sqrt{f}</span> and reset the histogram to continue sampling. We continue the simulation, iteratively reducing <span class="math inline">f</span> until it reaches a small predefined threshold value which allows us to adjust the tradeoff between accuracy and simulation speed.</p>
<p>One important consideration is the choice of energy bins. Since we are interested in the computation of the partition function using eq. <a href="#eq:partition_fn_from_dos">18</a>, the relevant energies are those where the product <span class="math inline">g(E)e^{-\beta E}</span> is not vanishingly small. Additionally we have to take into account that the estimate for the DOS is not normalized. To be able to correctly normalize the DOS we have to ensure that the range of energy bins includes all energies where the DOS is non-vanishing.</p>
<section id="applying-wang-landau-to-the-computation-of-the-marginal-density" class="level3" data-number="1.4.1">
<h3 data-number="4.4.1"><span class="header-section-number">4.4.1</span> Applying Wang-Landau to the Computation of the Marginal Density</h3>
<p>When working with statistical models such as the Ising model there is often a clear concept of the <em>state space</em> <span class="math inline">\Omega</span> and an associated volume of regions in state space such that integrals of the form <span class="math inline">\int_\Omega\mathrm d\mathbf n\ f(\mathbf n)</span> are well-defined for any <span class="math inline">f: \Omega\rightarrow\mathbb{R}</span>. However in the case where the individual states <span class="math inline">\mathbf n</span> represent stochastic trajectories it is not obvious what the meaning of such an integral should be. Therefore we use a modified DOS defined for a given response trajectory <span class="math inline">\mathbf x</span> by <span id="eq:modified_dos"><span class="math display">
\rho(U) = \int\mathrm d\mathbf s\ \mathrm P(\mathbf s)\,\delta(U(\mathbf s, \mathbf x) - U)
\qquad(20)</span></span> with <span class="math inline">U(\mathbf s, \mathbf x) = -\ln\mathrm P(\mathbf x|\mathbf s)</span>. In other words we are assigning a measure <span class="math inline">\mu</span> to our state space which is defined by the inherent probability density of the signals, such that <span class="math inline">\int\mu(\mathrm d\mathbf s) \equiv \int\mathrm d\mathbf s\ \mathrm P(\mathbf s)</span>. Thus we can express the marginal density of <span class="math inline">\mathbf x</span> analogously to eq. <a href="#eq:partition_fn_from_dos">18</a> by <span id="eq:modified_int"><span class="math display">
\mathrm P(\mathbf x) = \int\mathrm dU\ \rho(U)\,e^{-U}\,.
\qquad(21)</span></span></p>
<p>We have to slightly adapt the Wang-Landau procedure described above so that it produces an estimate of the modified DOS. To account for the density <span class="math inline">\mathrm P(\mathbf s)</span> in eq. <a href="#eq:modified_dos">20</a> we need ensure we propose configurations, asymptotically distributed according to <span class="math inline">\mathrm P(\mathbf s)</span>, which we then—in a second step—accept or reject using the inverse DOS. However we can combine both of these steps into a single one by combining a Metropolis acceptance step with the usual Wang-Landau procedure. Our algorithm therefore consists of the follwing steps:</p>
<ol type="1">
<li>Set all entries of the modified DOS to 1, <span class="math inline">\rho(U_i)=1, i=1,\ldots,n</span>.</li>
<li>Set all entries of the histogram to 0, <span class="math inline">H(U_i)=0, i=1,\ldots,n</span>.</li>
<li>Loop until <span class="math inline">f\epsilon</span>.
<ol type="1">
<li>Propose a new configuration <span class="math inline">\mathbf s^\prime\sim T(\mathbf s\rightarrow \mathbf s^\prime)</span>.</li>
<li>Let <span class="math inline">U_i</span> be the potential of state <span class="math inline">\mathbf s</span> and <span class="math inline">U_j</span> the potential of state <span class="math inline">\mathbf s^\prime</span>.</li>
<li>Accept the new configuration with probability <span id="eq:modified_acceptance_probability"><span class="math display">A(\mathbf s^\prime, \mathbf s) = \min\left[1, \frac{\mathrm P(\mathbf s^\prime)}{\mathrm P(\mathbf s)} \frac{\rho(U_i)}{\rho(U_j)} \frac{T(\mathbf s^\prime\rightarrow \mathbf s)}{T(\mathbf s\rightarrow \mathbf s^\prime)} \right]\,.
 \qquad(22)</span></span></li>
<li>Let <span class="math inline">U^\star</span> be either <span class="math inline">U_j</span> if <span class="math inline">\mathbf s^\prime</span> was accepted or <span class="math inline">U_i</span> otherwise.</li>
<li>Update the histogram <span class="math inline">H(U^\star)\leftarrow H(U^\star)+1</span> and the DOS <span class="math inline">\rho(U^\star)\leftarrow f\,\rho(U^\star)</span>.</li>
<li>If the histogram <span class="math inline">H</span> is flat, set <span class="math inline">f\leftarrow\sqrt f</span> and reset <span class="math inline">H(U_i)=0, i=1,\ldots,n</span>.</li>
</ol></li>
</ol>
<p>The proposal distribution <span class="math inline">T</span> can in principle be arbitrary. The definition of the acceptance probability in eq. <a href="#eq:modified_acceptance_probability">22</a> illustrates that—once the simulation is converged—<span class="math inline">\rho</span> describes how we have to modify the state space density <span class="math inline">\mathrm P(\mathbf s)</span> such that we sample uniformly over all potentials.</p>
<figure>
<img src="figures/normalized_densities.svg" id="fig:normalized_densities" alt="" /></img><figcaption>Figure 4: Illustrating the benefit of using the Wang-Landau algorithm for the multivariate normal system at different dimensionalities: The blue lines show the modified density of states from eq. <a href="#eq:modified_dos">20</a>, computed using a conventional MC simulation. The orange line represents the Boltzmann factor <span class="math inline">e^{-U}</span> and the green line shows the integrand of eq. <a href="#eq:modified_int">21</a> which is the product of the other two quantities. For visualization purposes, all functions where rescaled such that their integrals over the displayed interval equal 1. We see that especially at higher dimensionality there is very little overlap between the green and the blue line which leads to high inaccuracy in the computation of the marginal density. For all simulations we chose the covariance matrix using <span class="math inline">\Delta t = 64</span>.</figcaption>
</figure>
<p>Fig. <a href="#fig:normalized_densities">4</a> makes it clear why we expect Wang-Landau sampling to lead to a better estimate of the marginal density than the brute-force Monte-Carlo computation, especially in high-dimensional state spaces. For <span class="math inline">d=50</span> and <span class="math inline">d=200</span> most of the weight of the integral is in regions where <span class="math inline">\rho(E)\approx 0</span>. In these low density regions we usually get a very inaccurate estimation of the (modified) DOS by normal MC simulations since we only very occasionally sample a relevant state. The Wang-Landau algorithm ensures that for every energy there is a consistent sampling density and we get a good estimate of the DOS even in low-density regimes.</p>
<p>From fig. <a href="#fig:normalized_densities">4</a> we can also estimate for which range of potentials we must compute the DOS. Since the Boltzmann weight <span class="math inline">e^{-U}</span> strongly favours low-potential configurations it is important to compute the DOS for very low potentials even if it nearly vanishes there (i.e. in regions where the blue line vanishes but the green line has relevant weight).</p>
</section>
<section id="connection-to-standard-monte-carlo-sampling" class="level3" data-number="1.4.2">
<h3 data-number="4.4.2"><span class="header-section-number">4.4.2</span> Connection to Standard Monte-Carlo Sampling</h3>
<p>When we perform a standard Monte-Carlo estimate of <span class="math inline">\mathrm{P}(\mathbf{x})</span> we generate independent samples <span class="math inline">\mathbf{s}_1,\ldots,\mathbf{s}_M</span>, all identically distributed according to <span class="math inline">\mathrm P(\mathcal{S})</span> and then compute <span><span class="math display">
\hat{\mathrm{P}}(\mathbf{x}) = \frac{1}{M} \sum\limits^M_{i=1} \mathrm{P}(\mathbf x|\mathbf s_i) \,.
\qquad(23)</span></span> This estimate is essentially the same as performing the integral from eq. <a href="#eq:modified_int">21</a> where the density of states <span class="math inline">\rho(U)</span> is just approximated as the histogram of the potentials <span class="math inline">U(\mathbf{s}_1),\ldots,U(\mathbf{s}_M)</span>. Specifically in the limit of the width of histogram bins approaching 0, the approximate density of states becomes <span class="math inline">\hat{\rho}(U)=1/M\ \sum^M_{i=1} \delta(U-U(\mathbf{s}_i))</span> and therefore <span><span class="math display">
\int\mathrm{d}U\ \hat{\rho}(U) e^{-U} = \frac{1}{M}\int\mathrm dU \left[\sum^M_{i=1} \delta(U-U(\mathbf{s}_i)) e^{-U}\right] = \frac{1}{M} \sum\limits^M_{i=1} e^{-U(\mathbf{s}_i)} = \hat{\mathrm{P}}(\mathbf{x})\,.
\qquad(24)</span></span></p>
<p>For the purpose of comparing estimates we can therefore associate the standard MC approach with computing the empirical histogram of potential values when sampling signals according to their marginal distribution. In the next section we will compare this empirical histogram to the DOS as computed using the Wang-Landau algorithm.</p>
</section>
<section id="example-results-for-a-wang-landau-simulation" class="level3" data-number="1.4.3">
<h3 data-number="4.4.3"><span class="header-section-number">4.4.3</span> Example Results for a Wang-Landau Simulation</h3>
<figure>
<img src="figures/wl_dos.svg" id="fig:wl_dos" alt="" /></img><figcaption>Figure 5: Plots of estimates of the modified DOS from eq. <a href="#eq:modified_dos">20</a> compared on both linear and log scales. The blue line shows the Wang-Landau estimate while the orange line is a histogram estimate using unbiased sampling according to <span class="math inline">\mathrm P(\mathbf s)</span>. We see that especially in the low-potential regime the Wang-Landau estimate is much more accurate.</figcaption>
</figure>
<p>In fig. <a href="#fig:wl_dos">5</a> we display the estimated DOS using the Wang-Landau algorithm compared with a histogram estimate of the DOS using unbiased sampling. We see that in the highly relevant regime of low potential the Wang-Landau procedure allows us to get an accurate estimate of the DOS even though its density is as low as <span class="math inline">e^{-45}\approx 10^{-20}</span>. Using eq. <a href="#eq:modified_int">21</a> we compute the marginal density to be <span class="math inline">-664.01</span> whereas the “correct” value computed analytically is <span class="math inline">-664.24</span>. We thus find a relative error of <span class="math inline">0.03\%</span> in this estimate.</p>
<p>While we can achieve a very precise estimate for the marginal density <span class="math inline">\mathrm P(\mathbf x)</span> using the estimated DOS from the Wang-Landau algorithm there remain some practical difficulties. For maximum efficiency and accuracy the different parameters affecting the procedure such as the required histogram flatness, the updating scheme of the <span class="math inline">f</span> parameter, and the choice of potential bins have to be tuned for a given problem. After some tuning of these parameters for the Gaussian system for a fixed set of covariance matrices we still find the estimation to be at least one order of magnitude slower in CPU time compared to thermodynamic integration. Therefore, at least for the Gaussian system thermodynamic integration seems to be better suited to compute the marginal density.</p>
<p>With that said, we do expect the Wang-Landau procedure to perform especially well in cases were there are many local minima of the potential. Here using TI we might get <em>“stuck”</em> in a specific minimum and thus not sample all relevant states. Therefore we suggest that while TI should be the method of choice for the computation of the marginal density for high dimensional systems, in specific cases it may make sense to try other approaches that are well-established in statistical physics, such as Wang-Landau.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="1.5">
<h2 data-number="4.5"><span class="header-section-number">4.5</span> Conclusion</h2>
<p>Even if we can compute the mutual information efficiently using TI without estimating the DOS for individual responses it is worth noting that this framework shows very clearly why the brute-force Monte-Carlo estimates of the marginal probability density will fail for high-dimensional systems. Figs. <a href="#fig:normalized_densities">4</a>, <a href="#fig:wl_dos">5</a> illustrate why more advanced simulation methods are unavoidable for these problems.</p>
</section>
<section id="references" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="3">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-1998:Gelman">
<p>[1] A. Gelman, X.-L. Meng, Simulating normalizing constants: from importance sampling to bridge sampling to path sampling, Statistical Science. 13 (1998) 163–185. <a href="https://doi.org/10.1214/ss/1028905934">https://doi.org/10.1214/ss/1028905934</a>.</p>
</div>
<div id="ref-1970:Hastings">
<p>[2] W.K. Hastings, Monte Carlo sampling methods using Markov chains and their applications, Biometrika. 57 (1970) 97–109. <a href="https://doi.org/10.1093/biomet/57.1.97">https://doi.org/10.1093/biomet/57.1.97</a>.</p>
</div>
<div id="ref-2001:Wangg8b">
<p>[3] F. Wang, D.P. Landau, Determining the density of states for classical statistical models: A random walk algorithm to produce a flat histogram, Physical Review E. 64 (2001) 056101. <a href="https://doi.org/10.1103/physreve.64.056101">https://doi.org/10.1103/physreve.64.056101</a>.</p>
</div>
</div>
</section>
</section>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="information-and-noise-in-biological-systems.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="chapter_5.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="information-and-noise-in-biological-systems.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="chapter_5.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        

        

        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>