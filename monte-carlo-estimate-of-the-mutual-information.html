<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js Light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mutual Information between Trajectories</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
        <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var regex = /\\qquad\W*\(([0-9]+)\)/;
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    var tex_str = texText.data.replace(regex, "\\tag{$1}");
                    katex.render(tex_str, mathElements[i], {
                    displayMode: mathElements[i].classList.contains('display'),
                    throwOnError: false,
                    fleqn: false
                });
            }}});
        </script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "" : "Light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('Light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded"><a href="index.html" class=""><span class="header-section-number">1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html" class=""><span class="header-section-number">2</span> Information and Noise in Biological Systems
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-theory-in-the-context-of-cellular-signaling-networks" class=""><span class="header-section-number">2.1</span> Information Theory in the context of cellular signaling networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#mutual-information-as-an-efficiency-measure-in-cell-signaling" class=""><span class="header-section-number">2.1.1</span> Mutual Information as an Efficiency Measure in Cell signaling
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-transmission-for-time-varying-signals" class=""><span class="header-section-number">2.1.2</span> Information Transmission for Time-Varying Signals
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#stochastic-modeling-of-biochemical-networks" class=""><span class="header-section-number">2.2</span> Stochastic Modeling of Biochemical Networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#markov-processes" class=""><span class="header-section-number">2.2.1</span> Markov Processes
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#chemical-master-equation" class=""><span class="header-section-number">2.2.2</span> Chemical Master Equation
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#jump-processes" class=""><span class="header-section-number">2.2.3</span> Jump Processes
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#probability-densities-of-trajectories" class=""><span class="header-section-number">2.2.4</span> Probability Densities of Trajectories
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#generating-stochastic-trajectories-for-jump-processes" class=""><span class="header-section-number">2.2.5</span> Generating Stochastic Trajectories for Jump Processes
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#a-simple-model-of-gene-expression" class=""><span class="header-section-number">2.3</span> A Simple Model of Gene Expression
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html" class="active"><span class="header-section-number">3</span> Monte-Carlo Estimate of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">3.1</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">3.2</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#monte-carlo-simulations-for-trajectories" class=""><span class="header-section-number">3.3</span> Monte-Carlo Simulations for Trajectories
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#general-stochastic-dynamics-of-signals" class=""><span class="header-section-number">3.3.1</span> General Stochastic Dynamics of Signals
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generating-responses-for-time-varying-signals" class=""><span class="header-section-number">3.3.2</span> Generating Responses for Time-Varying Signals
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generation-of-response-trajectories-according-to-mathrm-pmathbf-xmathbf-s" class=""><span class="header-section-number">3.3.2.1</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generation-of-response-trajectories-according-to-mathrm-pmathbf-x" class=""><span class="header-section-number">3.3.2.2</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x)</span>
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#compuation-of-mathrm-pmathbf-xmathbf-s" class=""><span class="header-section-number">3.3.2.3</span> Compuation of <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>
                    </a>
                    </li>
                    </ol>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#practical-concerns" class=""><span class="header-section-number">3.4</span> Practical Concerns
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#computing-the-probability-distribution-of-the-initial-state" class=""><span class="header-section-number">3.4.1</span> Computing the probability distribution of the initial state
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#computing-probability-densities" class=""><span class="header-section-number">3.4.2</span> Computing Probability Densities
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#computing-the-likelihood" class=""><span class="header-section-number">3.4.3</span> Computing the likelihood
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#the-probability-density-for-the-starting-point-of-a-trajectory" class=""><span class="header-section-number">3.4.4</span> The probability density for the starting point of a trajectory
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-marginal-probability-of-response-trajectories" class=""><span class="header-section-number">3.4.5</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#computation-of-the-mutual-information-for-a-simple-model-of-gene-expression" class=""><span class="header-section-number">3.5</span> Computation of the Mutual Information for a Simple Model of Gene Expression
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#consistent-bias-in-comparisons-with-analytic-approximations" class=""><span class="header-section-number">3.5.1</span> Consistent Bias in Comparisons with Analytic Approximations
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html" class=""><span class="header-section-number">4</span> Mutual Information for Trajectories in a Gaussian Framework
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#introduction" class=""><span class="header-section-number">4.1</span> Introduction
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#choice-of-covariance-matrices" class=""><span class="header-section-number">4.1.1</span> Choice of Covariance Matrices
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#direct-importance-sampling" class=""><span class="header-section-number">4.1.2</span> Direct Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#umbrella-sampling" class=""><span class="header-section-number">4.1.3</span> Umbrella Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">4.2</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#discussion" class=""><span class="header-section-number">4.3</span> Discussion
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html" class=""><span class="header-section-number">5</span> Estimation Strategies Inspired by Statistical Physics
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#borrowing-terminology-from-statistical-physics" class=""><span class="header-section-number">5.1</span> Borrowing Terminology from Statistical Physics
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#thermodynamic-integration" class=""><span class="header-section-number">5.2</span> Thermodynamic Integration
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#summary-of-ti" class=""><span class="header-section-number">5.2.1</span> Summary of TI
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#markov-chain-monte-carlo" class=""><span class="header-section-number">5.2.2</span> Markov Chain Monte Carlo
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#estimating-the-density-of-states" class=""><span class="header-section-number">5.3</span> Estimating the Density of States
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#wang-and-landau-algorithm" class=""><span class="header-section-number">5.4</span> Wang and Landau Algorithm
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#applying-wang-landau-to-the-computation-of-the-marginal-density" class=""><span class="header-section-number">5.4.1</span> Applying Wang-Landau to the Computation of the Marginal Density
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#modified-dos" class=""><span class="header-section-number">5.4.1.1</span> Modified DOS
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#modified-wang-landau-algorithm" class=""><span class="header-section-number">5.4.1.2</span> Modified Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#comparison-to-usual-wang-landau-algorithm" class=""><span class="header-section-number">5.4.1.3</span> Comparison to usual Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#connection-to-standard-monte-carlo-sampling" class=""><span class="header-section-number">5.4.1.4</span> Connection to Standard Monte-Carlo Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#example-results-for-a-wang-landau-simulation" class=""><span class="header-section-number">5.4.2</span> Example Results for a Wang-Landau Simulation
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#conclusion" class=""><span class="header-section-number">5.5</span> Conclusion
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                    </div>

                    <h1 class="menu-title">Mutual Information between Trajectories</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/manuel-rhdt/master-thesis" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <section id="monte-carlo-estimate-of-the-mutual-information" class="level1" data-number="1">
<h1 data-number="3"><span class="header-section-number">3</span> Monte-Carlo Estimate of the Mutual Information</h1>
<p>Equipped with analytical formulae for the computation of trajectory probabilities and with methods for efficient stochastic simulation, we can start to develop estimates for the mutual information. The basis for our method is eq. <strong>¿eq:mi_form2?</strong>; specifically we separate the mutual information into two parts that are computed independently, the <em>marginal entropy</em> <span class="math inline">\mathrm H(\mathcal X)</span> and the <em>conditional entropy</em> <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>. Since signals and responses are time-varying, both entropies involve integrals over spaces of trajectories which are high-dimensional. At high dimensionality, direct numerical integration is not viable and instead, we use Monte-Carlo approaches based on random sampling of signals and responses.</p>
<p>While Monte-Carlo methods comprise a wide variety of approaches to stochastically evaluate integrals or sums the common idea is easily stated. We have a state space <span class="math inline">U</span> and a probability distribution <span class="math inline">p_U</span> on that state space. The problem is to numerically evaluate integrals of the form <span><span class="math display">
F = \langle f(u) \rangle = \int\limits_{U} \mathrm du\ p_U(u)\; f(u)
\qquad(1)</span></span> where <span class="math inline">f: U\rightarrow\mathbb R</span> is some function of interest. If <span class="math inline">U</span> is high-dimensional it is very time-consuming to estimate it by direct numerical integration. Instead, we generate random samples <span class="math inline">u_1,u_2,\ldots</span> from the probability distribution <span class="math inline">p_U</span> such that by the <em>law of large numbers</em> we have the equality <span id="eq:inf_mc_estimate"><span class="math display">
F = \lim_{N\rightarrow\infty} \frac{\sum^N_{i=1} f(u_i)}{N}
\qquad(2)</span></span> i.e. the sample average of random samples converges towards their mean. In practice we can’t evaluate the limit in eq. <a href="#eq:inf_mc_estimate">2</a> and we approximate <span class="math inline">F</span> using a finite number <span class="math inline">N</span> of random samples <span><span class="math display">
\hat{F} = \frac{\sum^N_{i=1} f(u_i)}{N}\,.
\qquad(3)</span></span> The variance of Monte-Carlo estimates typically decreases much faster than TODO. Monte-Carlo methods depend on the efficient and correct sampling of the corresponding probability distribution. In this thesis …</p>
<p>In this chapter we will show how to use Monte-Carlo integration to compute the mutual information between stochastic trajectories. The computation requires the estimation of two quantities, marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> and the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>. We will show how to perform Monte-Carlo estimates for both entropies, however as we will describe the marginal entropy turns out to be computationally much more difficult to estimate. We conclude the chapter with a thorough analysis of the challenges that arise by …</p>
<section id="monte-carlo-estimate-for-the-marginal-entropy" class="level2" data-number="1.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Monte-Carlo Estimate for the Marginal Entropy</h2>
<p>We start by considering an abstract system, consisting of two random variables <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> representing the possible signals and responses, respectively. We’ll assume to have efficient computational methods to generate random samples from either of these random variables. In this subsection we show how to use Monte-Carlo techniques to estimate the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> while in the following subsection we describe a similar method to estimate the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>.</p>
<p>We intend to compute the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> using Monte-Carlo (MC) sampling to evaluate the necessary integrals. First, given number of random samples <span class="math inline">(\mathbf x_i)_{i=1,\ldots,N_x}</span> taken from <span class="math inline">\mathcal X</span> we propose as an estimate for the entropy <span id="eq:mc_entropy"><span class="math display">
\mathrm H(\mathcal X) = -\int\mathrm d\mathbf x\ \mathrm P(\mathbf x)\ln \mathrm P(\mathbf x) \approx - \frac{\sum\limits_{i=1}^{N_x} \ln\mathrm P(\mathbf x_i)}{N_x} \,.
\qquad(4)</span></span> I.e. using a large number of response samples, we compute the sample average of their individual log-probabilities <span class="math inline">\ln\mathrm P(\mathbf x_i)</span>. As explained in the following section, from a biochemical of a cell we are not able to directly compute <span class="math inline">\mathrm P(\mathbf x_i)</span>. We do however have an efficient way of <em>generating</em> the responses <span class="math inline">\mathbf x_1,\mathbf x_2,\ldots</span> using the stochastic simulation algorithm.</p>
<p>Nonetheless we see from eq. <a href="#eq:mc_entropy">4</a> that we <em>do</em> have to evaluate <span class="math inline">\mathrm P(\mathbf x_i)</span> for every generated sample. To solve this problem we choose to evaluate <span class="math inline">\mathrm P(\mathbf x_i)</span> by doing another nested Monte-Carlo integration using signal samples <span class="math inline">(\mathbf s_j)_{j=1,\ldots,N_s}</span> from <span class="math inline">\mathcal S</span> to write <span id="eq:mc_marginal"><span class="math display">
\mathrm P(\mathbf x_i) = \int\mathrm d\mathbf s\ \mathrm P(\mathbf s)\ \mathrm P(\mathbf x_i|\mathbf s) \approx \frac{\sum\limits_{j=1}^{N_s} \mathrm P(\mathbf x_i | \mathbf s_j)}{N_s} \,.
\qquad(5)</span></span> Hence, for every response sample <span class="math inline">\mathbf x_i</span> we have to perform a sample average over the conditional probabilities <span class="math inline">\mathrm P(\mathbf x_i|\mathbf s_j)</span>. Again, anticipating results from the following section, we make use of the fact that a stochastic model for a biochemical network allows us to efficiently compute <span class="math inline">\mathrm P(\mathbf x_i|\mathbf s_j)</span>.</p>
<p>While for a low-dimensional signal space it is feasible to instead compute the marginalization integral eq. <a href="#eq:mc_marginal">5</a> using direct evaluation <span class="citation" data-cites="2019:Cepeda-Humerez">[<a href="#ref-2019:Cepeda-Humerez" role="doc-biblioref">1</a>]</span> we choose to use a nested MC simulation to also be able to handle high-dimensional signal spaces. This is crucial since time-varying signals are described by high-dimensional trajectories.</p>
<p>We can summarize the estimation procedure for the marginal entropy using the equation <span id="eq:mc_entropy_notation"><span class="math display">
\mathrm H(\mathcal X) = -\left\langle \ln \left\langle \mathrm P(\mathbf x | \mathbf s) \right\rangle_{\mathrm P(\mathbf s)} \right\rangle_{\mathrm P(\mathbf x)}
\qquad(6)</span></span> where we use the notation <span class="math inline">\langle f(x)\rangle_{g(x)}</span> for the expected value of <span class="math inline">f(x)</span> when <span class="math inline">x</span> is distributed according to the probability density given by <span class="math inline">g(x)</span>. Thus when thinking in mathematical terms we have the shorthand <span class="math inline">\langle f(x)\rangle_{g(x)} \equiv\int \mathrm dx\ g(x) f(x)</span>. We can also easily translate this notation into a Monte-Carlo estimate, i.e. <span class="math inline">\langle f(x)\rangle_{g(x)} = \lim\limits_{N\rightarrow\infty}\frac{\sum_{i=1}^N f(x_i)}{N}</span> where <span class="math inline">x_1, x_2,\ldots</span> are independent samples of the probability distribution given by <span class="math inline">g(x)</span>.</p>
</section>
<section id="estimating-the-conditional-entropy" class="level2" data-number="1.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Estimating the Conditional Entropy</h2>
<p>We can also estimate the <em>conditional entropy</em> using MC averages over trajectories. We express the conditional entropy using the notation introduced above <span><span class="math display">
\mathrm H(\mathcal X|\mathcal S) = -\iint \mathrm d\mathbf s\mathrm d\mathbf x\ \mathrm P(\mathbf s)\mathrm P(\mathbf x | \mathbf s) \ln\mathrm P(\mathbf x|\mathbf s) = -\left\langle\langle\ln\mathrm P(\mathbf x | \mathbf s)\rangle_{\mathrm P(\mathbf x | \mathbf s)} \right\rangle_{\mathrm P(\mathbf s)}
\qquad(7)</span></span> to show that we require nested Monte Carlo integrations to evaluate the integral. We first generate signal samples <span class="math inline">\mathbf s_1, \ldots, \mathbf s_{N_s}</span> from the density <span class="math inline">\mathrm P(\mathbf s)</span>. Let <span class="math inline">\mathbf x_i^1,\ldots,\mathbf x_i^{N_x}</span> be response samples generated from <span class="math inline">\mathrm P(\mathbf x | \mathbf s_i)</span>. The Monte Carlo estimate for the conditional entropy then reads <span id="eq:conditional_entropy_estimate"><span class="math display">
\mathrm H(\mathcal X|\mathcal S) \approx - \frac1{N_s N_x} \sum\limits_{i=1}^{N_s} \sum\limits_{j=1}^{N_x} \ln\mathrm P(\mathbf x_i^j | \mathbf s_i)\,.
\qquad(8)</span></span></p>
<p>Using Monte-Carlo computations we can in principle compute the mutual information between arbitrary random variables.</p>
</section>
<section id="monte-carlo-simulations-for-trajectories" class="level2" data-number="1.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> Monte-Carlo Simulations for Trajectories</h2>
<ul>
<li>Idea: Use SSA to generate response trajectories for given signals</li>
<li>The signals themselves are taken to be realizations of a given stochastic process</li>
<li>Using a set of predefined signals, we compute many responses</li>
<li>from a set of generated responses we can estimate the conditional entropy</li>
<li>and with some additional work the marginal entropy</li>
</ul>
<section id="general-stochastic-dynamics-of-signals" class="level3" data-number="1.3.1">
<h3 data-number="3.3.1"><span class="header-section-number">3.3.1</span> General Stochastic Dynamics of Signals</h3>
</section>
<section id="generating-responses-for-time-varying-signals" class="level3" data-number="1.3.2">
<h3 data-number="3.3.2"><span class="header-section-number">3.3.2</span> Generating Responses for Time-Varying Signals</h3>
<ul>
<li>Signals are generated from a known stochastic process</li>
</ul>
<p>Both, the estimate for the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> and the estimate for the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span> require the generation of appropriate responses to a given time-varying signal. To find stochastically correct responses for a given signal we have to understand how the biochemical network <em>couples</em> to the signal. In many cases the signal itself is a molecular species that participates in the reaction network. In this scenario, an abundance of signal directly leads to an increased rate of all reactions in which the signal molecule acts as a reactant. In other cases the signal may be a thermodynamic quantity as for example the environmental temperature. Changes in temperature usually effect the rates of <em>all</em> reactions in a chemical reaction network. Thus, in general we think of a signal as a quantity whose value affects some or all of the reaction rates of the biochemical network at a given instant. Since we model the signal as a time-varying process, for a given realization of the signal we can describe the corresponding responses using a master equation with time-dependent reaction rates.</p>
<p>We describe the coupling of the biochemical network to the signal through an explicit dependence of the reaction rates from eq. <strong>¿eq:general_master_eq?</strong> on the signal level <span class="math inline">s(t)</span>. Thus we write the master equation for the responses as <span id="eq:general_master_eq2"><span class="math display">
\frac{\partial \mathrm P(x, t|x_0, t_0, \mathbf s)}{\partial t} = \sum\limits_{x^\prime\in\mathcal U} \left[
w_t(x, x^\prime, s(t))\ \mathrm P(x^\prime, t|x_0, t_0, \mathbf s)
- w_t(x^\prime, x, s(t))\ \mathrm P(x, t|x_0, t_0, \mathbf s)
\right]
\qquad(9)</span></span> which shows the explicit dependence of the stochastic dynamics on the given signal trajectory <span class="math inline">\mathbf s</span>. Since we assume that the reaction rates are the <em>only</em> way in which the signal trajectory affects the response we conclude that stochastic realizations of the process described by eq. <a href="#eq:general_master_eq2">9</a> are distributed according to the conditional distribution <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>.</p>
<p>The formulation of the master equation with an explicit dependence on the signals makes it possible to perform the Monte-Carlo estimates as described above. Specifically, by making use of results derived in the previous chapter we can perform three crucial operations needed for the computational estimate of the mutual information.</p>
<section id="generation-of-response-trajectories-according-to-mathrm-pmathbf-xmathbf-s" class="level4" data-number="1.3.2.1">
<h4 data-number="3.3.2.1"><span class="header-section-number">3.3.2.1</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span></h4>
<p>As already described above, for any signal trajectory <span class="math inline">\mathbf s</span> we can formulate the master equation as written in eq. <a href="#eq:general_master_eq2">9</a> and use the stochastic simulation algorithm for time-dependent reaction rates to generate correctly distributed response trajectories.</p>
</section>
<section id="generation-of-response-trajectories-according-to-mathrm-pmathbf-x" class="level4" data-number="1.3.2.2">
<h4 data-number="3.3.2.2"><span class="header-section-number">3.3.2.2</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x)</span></h4>
<p>Similarly we can generate independent draws from the distribution <span class="math inline">\mathrm P(\mathbf x)</span> by first generating a stochastic realization <span class="math inline">\mathbf s</span> of the signal from the corresponding stochastic process and then generating exactly one response according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>.</p>
</section>
<section id="compuation-of-mathrm-pmathbf-xmathbf-s" class="level4" data-number="1.3.2.3">
<h4 data-number="3.3.2.3"><span class="header-section-number">3.3.2.3</span> Compuation of <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span></h4>
<p>Finally, for any signal-response pair of trajectories, we can compute the conditional probability <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> using the formulae for the trajectory probability derived before in eqns. <strong>¿eq:trajectory_probability_product?</strong>, <strong>¿eq:transition_probability?</strong> and inserting the reaction rates for the given signal trajectory <span class="math inline">\mathbf s</span>.</p>
<p>In summary, as long as we can formulate a stochastic process that describes the signal dynamics <em>and</em> we understand how the signal level affects the reaction rates of the biochemical network, we can in principle use the Monte-Carlo procedure described in the previous section to estimate the mutual information. From the signal’s SDE we generate stochastic realizations using appropriate numerical integrators for SDEs such as the <em>Euler–Maruyama method</em> or the <em>stochastic simulation algorithm</em>.</p>
</section>
</section>
</section>
<section id="practical-concerns" class="level2" data-number="1.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span> Practical Concerns</h2>
<section id="computing-the-probability-distribution-of-the-initial-state" class="level3" data-number="1.4.1">
<h3 data-number="3.4.1"><span class="header-section-number">3.4.1</span> Computing the probability distribution of the initial state</h3>
<p>We first look at the term <span class="math inline">P_0 = \mathrm P(x_0,t_0 | S)</span>. Since <span class="math inline">S</span> is a trajectory in time we can directly conclude from causality that</p>
<p><span><span class="math display">
\mathrm P(x_0, t_0 | S) = \mathrm P(x_0, t_0 | S_{t \leq t_0})
\qquad(10)</span></span></p>
<p>where <span class="math inline">S_{t \leq t_0}</span> is the temporal piece of the signal up to <span class="math inline">t_0</span>. We further suppose that the signal itself is markovian and therefore has no memory of its past. With this simplification we get</p>
<p><span><span class="math display">
\mathrm P(x_0, t_0 | S) = \mathrm P(x_0, t_0 | S_{t = t_0}) = \frac{\mathrm P((x_0, t_0), (s_0, t_0))}{\mathrm P(s_0, t_0)} \,.
\qquad(11)</span></span></p>
<p>We estimate <span class="math inline">P_0</span> using gaussian kernel density estimation to approximate both, the joint distribution of <span class="math inline">X_0, S_0</span> and the marginal distribution of <span class="math inline">S_0</span>.</p>
<p>Knowing the probabilities of the initial condition of both response and signal we can directly estimate the mutual information of <span class="math inline">\mathcal{X}_{t=t_0}</span> and <span class="math inline">\mathcal{S}_{t=t_0}</span>:</p>
<p><span><span class="math display">
\mathrm I(\mathcal{X}_{t=t_0}, \mathcal{S}_{t=t_0}) = \int ds_0\int dx_0\; \mathrm{P}(x_0, s_0)\; \ln \frac{\mathrm{P} (x_0, s_0)}{\mathrm{P} (x_0) \mathrm{P} (s_0)}
\qquad(12)</span></span></p>
</section>
<section id="computing-probability-densities" class="level3" data-number="1.4.2">
<h3 data-number="3.4.2"><span class="header-section-number">3.4.2</span> Computing Probability Densities</h3>
<p>For increasingly long trajectories this quantity will in many physically relevant cases either grow or decay exponentially (<em>TODO: explain why</em>). Thus sufficiently long trajectories, the numerical values of the likelihood will not be directly representable by conventional floating-point numbers.</p>
<p>This problem can be avoided if we compute the <em>log-likelihood</em> <span class="math inline">\ell(X|S) \equiv \ln\mathrm P(X|S)</span> instead. We can easily rephrase the equation for the likelihood:</p>
<p><span><span class="math display">
\ell(X|S) = \ln\left[ \mathrm P(x_0,t_0 | S) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S) \right] = \ln \mathrm P(x_0,t_0 | S) +\sum\limits^{N-1}_{n=1} \ln \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S)\,.
\qquad(13)</span></span></p>
<p>In practice (due to limited precision of floating-point arithmetic) it is only possible to evaluate the log-likelihood <span class="math inline">\ell(X|S) \equiv \ln\mathrm P(X|S)</span>. This means that the calculation of the averaged likelihood involves the quantity</p>
<p><span><span class="math display">
\ln \sum^{N_S}_{i=1} \mathrm P(X|S^{(i)}) = \ln \sum\limits^{N_S}_{i=1} \exp \ell(X|S^{(i)}) \equiv \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right)
\qquad(14)</span></span></p>
<p>where <span class="math inline">\mathrm{LSE} : \mathbb{R}^n \rightarrow \mathbb{R}</span> is called log-sum-exp . An interesting property of <span class="math inline">\mathrm{LSE}</span> is that it’s a smooth approximation to the <span class="math inline">\max</span> function. This means that for finite sample sizes the monte-carlo estimate of the averaged likelihood will always be too small!</p>
</section>
<section id="computing-the-likelihood" class="level3" data-number="1.4.3">
<h3 data-number="3.4.3"><span class="header-section-number">3.4.3</span> Computing the likelihood</h3>
<p>The Probability density of a markovian trajectory can be expressed as</p>
<p><span><span class="math display">
\mathrm P(X) = \mathrm P(x_0,t_0;x_1,t_1;\ldots;x_{N-1},t_{N-1}) = \mathrm P(x_0,t_0 ) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}) \,.
\qquad(15)</span></span></p>
<p>Therefore the problem of calculating the likelihood for a particular trajectory amounts to solving two independent problems:</p>
<ol type="1">
<li>estimating the probability density of the starting point <span class="math inline">\mathrm P (x_0, t_0)</span> of a response</li>
<li>calculating the transition probabilities <span class="math inline">\mathrm P(x_n,t_n|x_{n-1},t_{n-1})</span></li>
</ol>
<p>For a given chemical reaction network we can write down the chemical master equation. The chemical master equation contains all the information needed to compute the individual terms <span class="math inline">\mathrm P(x_n,t_n|x_{n-1},t_{n-1})</span> for the entire system.</p>
<p>To calculate the mutual information between <span class="math inline">\mathcal{S}</span> and <span class="math inline">\mathcal X</span> we have to consider the entire reaction network containing the components both in <span class="math inline">S</span> and in <span class="math inline">X</span>. The precise reaction dynamics of the response part of the chemical network crucially depend on the observed signal trajectory. Therefore the chemical master equation for the whole reaction network allows us to compute the likelihood of a response trajectory for a particular signal trajectory:</p>
<p><span><span class="math display">
\mathrm P(\mathcal X = X|\mathcal S = S) = \mathrm P(x_0,t_0;x_1,t_1;\ldots;x_{N-1},t_{N-1} | S) = \mathrm P(x_0,t_0 | S) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S) \,.
\qquad(16)</span></span></p>
</section>
<section id="the-probability-density-for-the-starting-point-of-a-trajectory" class="level3" data-number="1.4.4">
<h3 data-number="3.4.4"><span class="header-section-number">3.4.4</span> The probability density for the starting point of a trajectory</h3>
</section>
<section id="estimating-the-marginal-probability-of-response-trajectories" class="level3" data-number="1.4.5">
<h3 data-number="3.4.5"><span class="header-section-number">3.4.5</span> Estimating the marginal probability of response trajectories</h3>
<p>To calculate the mutual information between trajectories we need to have a good estimate for <span class="math inline">\ln\left\langle \mathrm P(X | S) \right\rangle_\mathcal{S}</span>. We calculate this average by sampling of trajectories <span class="math inline">(S^{(i)})_{i=1\ldots N_S}</span> from the probability distribution of <span class="math inline">\mathcal{S}</span>:</p>
<p><span><span class="math display">
\ln\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S} \approx \ln \frac{\sum^{N_S}_{i=1} \mathrm P(X|S^{(i)})}{N_S} = \ln \sum^{N_S}_{i=1} \mathrm P(X|S^{(i)}) - \ln N_S
\qquad(17)</span></span></p>
<p>Thus we find that it is enough to be able to compute the likelihood between trajectories to estimate the marginal distribution of trajectories.</p>
<p>In practice (due to limited precision of floating-point arithmetic) it is only possible to evaluate the log-likelihood <span class="math inline">\ell(X|S) \equiv \ln\mathrm P(X|S)</span>. This means that the calculation of the averaged likelihood involves the quantity</p>
<p><span><span class="math display">
\ln \sum^{N_S}_{i=1} \mathrm P(X|S^{(i)}) = \ln \sum\limits^{N_S}_{i=1} \exp \ell(X|S^{(i)}) \equiv \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right)
\qquad(18)</span></span></p>
<p>where <span class="math inline">\mathrm{LSE} : \mathbb{R}^n \rightarrow \mathbb{R}</span> is called log-sum-exp . An interesting property of <span class="math inline">\mathrm{LSE}</span> is that it’s a smooth approximation to the <span class="math inline">\max</span> function. This means that for finite sample sizes the monte-carlo estimate of the averaged likelihood will always be too small!</p>
<p>We approximate the mutual information between trajectories as</p>
<p><span><span class="math display">
\mathrm{I}(\mathcal{X}; \mathcal{S}) = \left\langle \ln \frac{\mathrm{P} ( X |  S)}{\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S}} \right\rangle_{\mathcal{X},\mathcal{S}} = \left\langle \ell ( X |  S) - \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right) + \ln N_S\right\rangle_{\mathcal{X},\mathcal{S}}
\qquad(19)</span></span></p>
<p>which means that for finite amount of signal samples we will <em>systematically over-estimate</em> the mutual information. <strong>TODO: Is that really true? What about <span class="math inline">\ln N_S</span>?</strong> Even worse: the longer the trajectories the bigger the error becomes since the dimensionality of the space of possible signals is growing.</p>
<p>Another way to phrase this insight is that to get a good approximation for the logarithmic average likelihood, our set of signals that we use for monte-carlo sampling should contain many signals that produce a high likelihood. <strong>Therefore it probably is necessary to come up with a scheme to specifically sample signal trajectories for which the likelihood of a particular trajectory is high</strong>. On the other hand the results do not seem to get significantly better when averaging over more trajectories.</p>
</section>
</section>
<section id="computation-of-the-mutual-information-for-a-simple-model-of-gene-expression" class="level2" data-number="1.5">
<h2 data-number="3.5"><span class="header-section-number">3.5</span> Computation of the Mutual Information for a Simple Model of Gene Expression</h2>
<ul>
<li>Show the reaction network</li>
<li>Show example trajectories</li>
</ul>
<section id="consistent-bias-in-comparisons-with-analytic-approximations" class="level3" data-number="1.5.1">
<h3 data-number="3.5.1"><span class="header-section-number">3.5.1</span> Consistent Bias in Comparisons with Analytic Approximations</h3>

</section>
</section>
<section id="references" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="2">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-2019:Cepeda-Humerez">
<p>[1] S.A. Cepeda-Humerez, J. Ruess, G. Tkačik, Estimating information in time-varying signals., PLoS Computational Biology. 15 (2019) e1007290. <a href="https://doi.org/10.1371/journal.pcbi.1007290">https://doi.org/10.1371/journal.pcbi.1007290</a>.</p>
</div>
</div>
</section>
</section>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="information-and-noise-in-biological-systems.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="mutual-information-for-trajectories-in-a-gaussian-framework.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="information-and-noise-in-biological-systems.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="mutual-information-for-trajectories-in-a-gaussian-framework.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        

        

        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>