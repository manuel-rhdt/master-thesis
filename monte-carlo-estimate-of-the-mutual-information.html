<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js Light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mutual Information between Trajectories</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
        <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var regex = /\\qquad\W*\(([0-9]+)\)/;
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    var tex_str = texText.data.replace(regex, "\\tag{$1}");
                    katex.render(tex_str, mathElements[i], {
                    displayMode: mathElements[i].classList.contains('display'),
                    throwOnError: false,
                    fleqn: false
                });
            }}});
        </script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "" : "Light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('Light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded"><a href="index.html" class=""><span class="header-section-number">1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="acknowledgments.html" class="">Acknowledgments
                    </a>
                    </li><li class="expanded"><a href="introduction.html" class=""><span class="header-section-number">2</span> Introduction
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="introduction.html#goal-of-the-thesis" class=""><span class="header-section-number">2.1</span> Goal of the Thesis
                    </a>
                    </li><li class="expanded"><a href="introduction.html#structure-of-the-thesis" class=""><span class="header-section-number">2.2</span> Structure of the Thesis
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html" class=""><span class="header-section-number">3</span> Modeling Cell Signaling Networks as Information Processing Devices
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#information-theory-in-the-context-of-cellular-signaling-networks" class=""><span class="header-section-number">3.1</span> Information Theory in the context of cellular signaling networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#mutual-information-as-an-efficiency-measure-in-cell-signaling" class=""><span class="header-section-number">3.1.1</span> Mutual Information as an Efficiency Measure in Cell signaling
                    </a>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#information-transmission-for-time-varying-signals" class=""><span class="header-section-number">3.1.2</span> Information Transmission for Time-Varying Signals
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#stochastic-modeling-of-biochemical-networks" class=""><span class="header-section-number">3.2</span> Stochastic Modeling of Biochemical Networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#markov-processes" class=""><span class="header-section-number">3.2.1</span> Markov Processes
                    </a>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#chemical-master-equation" class=""><span class="header-section-number">3.2.2</span> Chemical Master Equation
                    </a>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#jump-processes" class=""><span class="header-section-number">3.2.3</span> Jump Processes
                    </a>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#probability-densities-of-trajectories" class=""><span class="header-section-number">3.2.4</span> Probability Densities of Trajectories
                    </a>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#sec:ssa" class=""><span class="header-section-number">3.2.5</span> Generating Stochastic Trajectories for Jump Processes
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="modeling-cell-signaling-networks-as-information-processing-devices.html#sec:simple_model" class=""><span class="header-section-number">3.3</span> A Simple Model of Gene Expression
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html" class="active"><span class="header-section-number">3</span> Monte Carlo Estimate of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">3.1</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">3.2</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#monte-carlo-simulations-for-trajectories" class=""><span class="header-section-number">3.3</span> Monte-Carlo Simulations for Trajectories
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#general-stochastic-dynamics-of-signals" class=""><span class="header-section-number">3.3.1</span> General Stochastic Dynamics of Signals
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generating-responses-for-time-varying-signals" class=""><span class="header-section-number">3.3.2</span> Generating Responses for Time-Varying Signals
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generation-of-response-trajectories-according-to-mathrm-pmathbf-xmathbf-s" class=""><span class="header-section-number">3.3.2.1</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generation-of-response-trajectories-according-to-mathrm-pmathbf-x" class=""><span class="header-section-number">3.3.2.2</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x)</span>
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#compuation-of-mathrm-pmathbf-xmathbf-s" class=""><span class="header-section-number">3.3.2.3</span> Compuation of <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#sec:initial_condition" class=""><span class="header-section-number">3.3.3</span> Probability Distribution of the Initial State
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#returning-to-the-simple-model-of-gene-expression" class=""><span class="header-section-number">3.4</span> Returning to the Simple Model of Gene Expression
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#simulating-signals-and-responses" class=""><span class="header-section-number">3.4.1</span> Simulating Signals and Responses
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#consistent-bias-in-comparisons-with-analytic-approximations" class=""><span class="header-section-number">3.4.2</span> Consistent Bias in Comparisons with Analytic Approximations
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#gaussian-approximation" class=""><span class="header-section-number">3.5</span> Gaussian Approximation
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#time-discretization" class=""><span class="header-section-number">3.5.1</span> Time Discretization
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#direct-importance-sampling" class=""><span class="header-section-number">3.5.2</span> Direct Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#sec:umbrella" class=""><span class="header-section-number">3.5.3</span> Umbrella Sampling
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-conditional-entropy-1" class=""><span class="header-section-number">3.5.4</span> Estimating the Conditional Entropy
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#discussion" class=""><span class="header-section-number">3.6</span> Discussion
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html" class=""><span class="header-section-number">4</span> Directed Sampling in Trajectory Space
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="directed-sampling-in-trajectory-space.html#previous-work-on-the-computation-of-the-marginal-likelihood" class=""><span class="header-section-number">4.1</span> Previous Work on the Computation of the Marginal Likelihood
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="directed-sampling-in-trajectory-space.html#importance-sampling" class=""><span class="header-section-number">4.1.1</span> Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#estimating-the-marginal-density-from-posterior-samples" class=""><span class="header-section-number">4.1.2</span> Estimating the Marginal Density from Posterior Samples
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#bridge-sampling-and-beyond" class=""><span class="header-section-number">4.1.3</span> Bridge Sampling and Beyond
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#borrowing-terminology-from-statistical-physics" class=""><span class="header-section-number">4.2</span> Borrowing Terminology from Statistical Physics
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#thermodynamic-integration" class=""><span class="header-section-number">4.3</span> Thermodynamic Integration
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="directed-sampling-in-trajectory-space.html#summary-of-the-technique" class=""><span class="header-section-number">4.3.1</span> Summary of the Technique
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#markov-chain-monte-carlo" class=""><span class="header-section-number">4.3.2</span> Markov Chain Monte Carlo
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#estimating-the-density-of-states" class=""><span class="header-section-number">4.4</span> Estimating the Density of States
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="directed-sampling-in-trajectory-space.html#wang-and-landau-algorithm" class=""><span class="header-section-number">4.4.1</span> Wang and Landau Algorithm
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#applying-wang-landau-to-the-computation-of-the-marginal-density" class=""><span class="header-section-number">4.4.2</span> Applying Wang-Landau to the Computation of the Marginal Density
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="directed-sampling-in-trajectory-space.html#modified-dos" class=""><span class="header-section-number">4.4.2.1</span> Modified DOS
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#modified-wang-landau-algorithm" class=""><span class="header-section-number">4.4.2.2</span> Modified Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#comparison-to-usual-wang-landau-algorithm" class=""><span class="header-section-number">4.4.2.3</span> Comparison to usual Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#connection-to-standard-monte-carlo-sampling" class=""><span class="header-section-number">4.4.2.4</span> Connection to Standard Monte-Carlo Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#example-results-for-a-wang-landau-simulation" class=""><span class="header-section-number">4.4.3</span> Example Results for a Wang-Landau Simulation
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#generating-proposal-trajectories-from-general-stochastic-dynamics" class=""><span class="header-section-number">4.5</span> Generating Proposal Trajectories from General Stochastic Dynamics
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#marginalizing-out-individual-components-of-the-biochemical-network" class=""><span class="header-section-number">4.6</span> Marginalizing Out Individual Components of the Biochemical Network
                    </a>
                    </li><li class="expanded"><a href="directed-sampling-in-trajectory-space.html#discussion" class=""><span class="header-section-number">4.7</span> Discussion
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="conclusion.html" class=""><span class="header-section-number">5</span> Conclusion
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="conclusion.html#summary-of-main-results" class=""><span class="header-section-number">5.1</span> Summary of Main Results
                    </a>
                    </li><li class="expanded"><a href="conclusion.html#outlook" class=""><span class="header-section-number">5.2</span> Outlook
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="references.html" class="">References
                    </a>
                    </li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                    </div>

                    <h1 class="menu-title">Mutual Information between Trajectories</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/manuel-rhdt/master-thesis" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <section id="monte-carlo-estimate-of-the-mutual-information" class="level1" data-number="1">
<h1 data-number="3"><span class="header-section-number">3</span> Monte Carlo Estimate of the Mutual Information</h1>
<p>Equipped with analytical formulae for the computation of trajectory probabilities and with methods for efficient stochastic simulation, we can start to develop estimates for the mutual information. The basis for our method is eq. <strong>¿eq:mi_form2?</strong>; specifically we separate the mutual information into two parts that are computed independently, the <em>marginal entropy</em> <span class="math inline">\mathrm H(\mathcal X)</span> and the <em>conditional entropy</em> <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>. Since signals and responses are time-varying, both entropies involve integrals over spaces of trajectories which are high-dimensional. At high dimensionality, direct numerical integration is not viable and instead, we use Monte-Carlo approaches based on random sampling of signals and responses.</p>
<p>While Monte-Carlo methods comprise a wide variety of approaches to stochastically evaluate integrals or sums the common idea is easily stated. We have a state space <span class="math inline">U</span> and a probability distribution <span class="math inline">p_U</span> on that state space. The problem is to numerically evaluate integrals of the form <span><span class="math display">
F = \langle f(u) \rangle = \int\limits_{U} \mathrm du\ p_U(u)\; f(u)
\qquad(1)</span></span> where <span class="math inline">f: U\rightarrow\mathbb R</span> is some function of interest. If <span class="math inline">U</span> is high-dimensional it is very time-consuming to estimate it by direct numerical integration. Instead, we generate random samples <span class="math inline">u_1,u_2,\ldots</span> from the probability distribution <span class="math inline">p_U</span> such that by the <em>law of large numbers</em> we have the equality <span id="eq:inf_mc_estimate"><span class="math display">
F = \lim_{N\rightarrow\infty} \frac{\sum^N_{i=1} f(u_i)}{N}
\qquad(2)</span></span> i.e. the sample average of random samples converges towards their mean. In practice, we can’t evaluate the limit in eq. <a href="#eq:inf_mc_estimate">2</a> and we approximate <span class="math inline">F</span> using a finite number <span class="math inline">N</span> of random samples <span><span class="math display">
\hat{F} = \frac{\sum^N_{i=1} f(u_i)}{N}\,.
\qquad(3)</span></span> The variance of Monte-Carlo estimates typically decreases much faster than TODO. Monte-Carlo methods depend on the efficient and correct sampling of the corresponding probability distribution. In this thesis …</p>
<p>In this chapter we will show how to use Monte-Carlo integration to compute the mutual information between stochastic trajectories. The computation requires the estimation of two quantities, marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> and the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>. We will show how to perform Monte-Carlo estimates for both entropies, however as we will describe the marginal entropy turns out to be computationally much more difficult to estimate. We conclude the chapter with a thorough analysis of the challenges that arise by …</p>
<section id="monte-carlo-estimate-for-the-marginal-entropy" class="level2" data-number="1.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Monte-Carlo Estimate for the Marginal Entropy</h2>
<p>We start by considering an abstract system, consisting of two random variables <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> representing the possible signals and responses, respectively. We will assume the availability of efficient computational methods to generate random samples from either of these random variables. Later in this chapter we will describe some methods for random sample generation. In this subsection we show how to use Monte-Carlo techniques to estimate the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> while in the following subsection we describe a similar method to estimate the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>.</p>
<p>We intend to compute the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> using Monte-Carlo (MC) sampling to evaluate the necessary integrals. First, given a number of random samples <span class="math inline">(\mathbf x_i)_{i=1,\ldots,N_x}</span> taken from <span class="math inline">\mathcal X</span> we propose as an estimate for the entropy <span id="eq:mc_entropy"><span class="math display">
\mathrm H(\mathcal X) = -\int\mathrm d\mathbf x\ \mathrm P(\mathbf x)\ln \mathrm P(\mathbf x) \approx - \frac{\sum\limits_{i=1}^{N_x} \ln\mathrm P(\mathbf x_i)}{N_x} \,.
\qquad(4)</span></span> I.e. using numerous response samples, we compute the sample average of their individual log-probabilities <span class="math inline">\ln\mathrm P(\mathbf x_i)</span>. As explained in the following section, from a biochemical of a cell we are not able to directly compute <span class="math inline">\mathrm P(\mathbf x_i)</span>. We do however have an efficient way of <em>generating</em> the responses <span class="math inline">\mathbf x_1,\mathbf x_2,\ldots</span> using the stochastic simulation algorithm.</p>
<p>Nonetheless, we see from eq. <a href="#eq:mc_entropy">4</a> that we <em>do</em> have to evaluate <span class="math inline">\mathrm P(\mathbf x_i)</span> for every generated sample. To solve this problem we choose to evaluate <span class="math inline">\mathrm P(\mathbf x_i)</span> by doing another nested Monte-Carlo integration using signal samples <span class="math inline">(\mathbf s_j)_{j=1,\ldots,N_s}</span> from <span class="math inline">\mathcal S</span> to write <span id="eq:mc_marginal"><span class="math display">
\mathrm P(\mathbf x_i) = \int\mathrm d\mathbf s\ \mathrm P(\mathbf s)\ \mathrm P(\mathbf x_i|\mathbf s) \approx \frac{\sum\limits_{j=1}^{N_s} \mathrm P(\mathbf x_i | \mathbf s_j)}{N_s} \,.
\qquad(5)</span></span> Hence, for every response sample <span class="math inline">\mathbf x_i</span> we have to perform a sample average over the conditional probabilities <span class="math inline">\mathrm P(\mathbf x_i|\mathbf s_j)</span>. Again, anticipating results from the following section, we make use of the fact that a stochastic model for a biochemical network allows us to efficiently compute <span class="math inline">\mathrm P(\mathbf x_i|\mathbf s_j)</span>. Therefore it seems viable to use eq. <a href="#eq:mc_marginal">5</a> to compute the marginal probability for a given response.</p>
<p>While for a low-dimensional signal space it is feasible to instead compute the marginalization integral eq. <a href="#eq:mc_marginal">5</a> using direct evaluation <span class="citation" data-cites="2019.Cepeda-Humerez">[<a href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">1</a>]</span> we choose to use a nested MC simulation to also be able to handle high-dimensional signal spaces. This is crucial since time-varying signals are described by high-dimensional trajectories.</p>
<p>We can summarize the estimation procedure for the marginal entropy using the equation <span id="eq:mc_entropy_notation"><span class="math display">
\mathrm H(\mathcal X) = -\left\langle \ln \left\langle \mathrm P(\mathbf x | \mathbf s) \right\rangle_{\mathrm P(\mathbf s)} \right\rangle_{\mathrm P(\mathbf x)}
\qquad(6)</span></span> where we use the notation <span class="math inline">\langle f(x)\rangle_{g(x)}</span> for the expected value of <span class="math inline">f(x)</span> when <span class="math inline">x</span> is distributed according to the probability density given by <span class="math inline">g(x)</span>. Thus when thinking in mathematical terms we have the shorthand <span class="math inline">\langle f(x)\rangle_{g(x)} \equiv\int \mathrm dx\ g(x) f(x)</span>. We can also easily translate this notation into a Monte-Carlo estimate, i.e. <span class="math inline">\langle f(x)\rangle_{g(x)} = \lim\limits_{N\rightarrow\infty}\frac{\sum_{i=1}^N f(x_i)}{N}</span> where <span class="math inline">x_1, x_2,\ldots</span> are independent samples of the probability distribution given by <span class="math inline">g(x)</span>.</p>
</section>
<section id="estimating-the-conditional-entropy" class="level2" data-number="1.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Estimating the Conditional Entropy</h2>
<p>We can also estimate the <em>conditional entropy</em> using MC averages over trajectories. We express the conditional entropy using the notation introduced in eq. <a href="#eq:mc_entropy_notation">6</a> <span><span class="math display">
\mathrm H(\mathcal X|\mathcal S) = -\iint \mathrm d\mathbf s\mathrm d\mathbf x\ \mathrm P(\mathbf s)\mathrm P(\mathbf x | \mathbf s) \ln\mathrm P(\mathbf x|\mathbf s) = -\left\langle\langle\ln\mathrm P(\mathbf x | \mathbf s)\rangle_{\mathrm P(\mathbf x | \mathbf s)} \right\rangle_{\mathrm P(\mathbf s)}
\qquad(7)</span></span> to show that we require nested Monte Carlo integrations to evaluate the integral. We first generate signal samples <span class="math inline">\mathbf s_1, \ldots, \mathbf s_{N_s}</span> from the density <span class="math inline">\mathrm P(\mathbf s)</span>. Let <span class="math inline">\mathbf x_i^1,\ldots,\mathbf x_i^{N_x}</span> be response samples generated from <span class="math inline">\mathrm P(\mathbf x | \mathbf s_i)</span>. The Monte Carlo estimate for the conditional entropy then reads <span id="eq:conditional_entropy_estimate"><span class="math display">
\mathrm H(\mathcal X|\mathcal S) \approx - \frac1{N_s N_x} \sum\limits_{i=1}^{N_s} \sum\limits_{j=1}^{N_x} \ln\mathrm P(\mathbf x_i^j | \mathbf s_i)\,.
\qquad(8)</span></span></p>
<p>Using Monte-Carlo computations we can in principle compute the mutual information between arbitrary random variables.</p>
</section>
<section id="monte-carlo-simulations-for-trajectories" class="level2" data-number="1.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> Monte-Carlo Simulations for Trajectories</h2>
<ul>
<li>Idea: Use SSA to generate response trajectories for given signals</li>
<li>The signals themselves are taken to be realizations of a given stochastic process</li>
<li>Using a set of predefined signals, we compute many responses</li>
<li>from a set of generated responses we can estimate the conditional entropy</li>
<li>and with some additional work the marginal entropy</li>
</ul>
<section id="general-stochastic-dynamics-of-signals" class="level3" data-number="1.3.1">
<h3 data-number="3.3.1"><span class="header-section-number">3.3.1</span> General Stochastic Dynamics of Signals</h3>
<p>In the previous section we found that both, the Monte-Carlo computation of the marginal entropy and of the conditional entropy require the generation of stochastic samples of the signal. This is not surprising since the stochastic dynamics of the signal reflect the amount of information that it contains. For example, if the signal is governed by mostly noise, then even the most efficient biochemical network could not extract a lot of meaningful information out of it. Therefore, to compute the mutual information between the signal and the responses, apart from modeling the biochemical network that processes the signal we also have to model the signal-generating process.</p>
<p>Generally, any kind of noisy signal can be modeled by a <em>stochastic process</em>. In many physically relevant cases, these processes arise as the solutions of SDEs that describe the underlying deterministic physics of the signal together with a <em>noise term</em> that gives rise to the stochastic nature of the signal trajectories. An example for such a process is the <em>Ornstein-Uhlenbeck process</em> that describes the random motion of a diffusing particle in a harmonic potential well <span class="citation" data-cites="2009.Gardiner">[<a href="#ref-2009.Gardiner" role="doc-biblioref">2</a>]</span>. It represents a combination of a deterministic harmonic oscillator together with fluctuations arising from the diffusion process. For the kinds of stochastic processes that are described by a SDE there exist many numerical methods to generate approximate signals trajectories. A simple, yet effective method to generate stochastic realizations of SDEs is the <em>Euler–Maruyama method</em> that is a generalization of the Euler method for integrating ODEs <span class="citation" data-cites="1992.Kloeden">[<a href="#ref-1992.Kloeden" role="doc-biblioref">3</a>]</span>. Using such methods we can in principle choose any integrable SDE as the signal-generating process for the computation of the mutual information.</p>
<p>If we want to investigate the simple reaction network for gene expression given in eq. <strong>¿eq:simple_reaction_network?</strong> we find that the signal dynamics are themselves described by a reaction network. In that case it is possible to generate <em>exact</em> stochastic realizations of signal trajectories using a stochastic simulation algorithm as discussed in sec. <strong>¿sec:ssa?</strong>.</p>
</section>
<section id="generating-responses-for-time-varying-signals" class="level3" data-number="1.3.2">
<h3 data-number="3.3.2"><span class="header-section-number">3.3.2</span> Generating Responses for Time-Varying Signals</h3>
<p>Both, the estimate for the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> and the estimate for the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span> require the generation of appropriate responses to a given time-varying signal. To find stochastically correct responses for a given signal we have to understand how the biochemical network interacts with the signal. In many cases the signal itself is a molecular species that participates in the reaction network. In this scenario, an abundance of signal directly leads to an increased rate of all reactions in which the signal molecule acts as a reactant. In other cases the signal may be a thermodynamic quantity as for example the environmental temperature. Changes in temperature usually effect the rates of <em>all</em> reactions in a chemical reaction network. Thus, in general we think of a signal as a quantity whose value affects some or all of the reaction rates of the biochemical network at a given instant. Since we model the signal as a time-varying process, for a given realization of the signal we can describe the corresponding responses using a master equation with time-dependent reaction rates.</p>
<p>We describe the coupling of the biochemical network to the signal through an explicit dependence of the reaction rates from eq. <strong>¿eq:general_master_eq?</strong> on the signal level <span class="math inline">s(t)</span>. Thus we write the master equation for the responses as <span id="eq:general_master_eq2"><span class="math display">
\frac{\partial \mathrm P(x, t|x_0, t_0, \mathbf s)}{\partial t} = \sum\limits_{x^\prime\in\mathcal U} \left[
w_t(x, x^\prime, s(t))\ \mathrm P(x^\prime, t|x_0, t_0, \mathbf s)
- w_t(x^\prime, x, s(t))\ \mathrm P(x, t|x_0, t_0, \mathbf s)
\right]
\qquad(9)</span></span> which shows the explicit dependence of the stochastic dynamics on the given signal trajectory <span class="math inline">\mathbf s</span>. Since we assume that the reaction rates are the <em>only</em> way in which the signal trajectory affects the response we conclude that stochastic realizations of the process described by eq. <a href="#eq:general_master_eq2">9</a> are distributed according to the conditional distribution <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>.</p>
<p>The formulation of the master equation with an explicit dependence on the signals makes it possible to perform the Monte-Carlo estimates as described above. Specifically, by making use of results derived in the previous chapter we can perform three crucial operations needed for the computational estimate of the mutual information.</p>
<section id="generation-of-response-trajectories-according-to-mathrm-pmathbf-xmathbf-s" class="level4" data-number="1.3.2.1">
<h4 data-number="3.3.2.1"><span class="header-section-number">3.3.2.1</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span></h4>
<p>As already described above, for any signal trajectory <span class="math inline">\mathbf s</span> we can formulate the master equation as written in eq. <a href="#eq:general_master_eq2">9</a> and use the stochastic simulation algorithm for time-dependent reaction rates to generate correctly distributed response trajectories.</p>
</section>
<section id="generation-of-response-trajectories-according-to-mathrm-pmathbf-x" class="level4" data-number="1.3.2.2">
<h4 data-number="3.3.2.2"><span class="header-section-number">3.3.2.2</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x)</span></h4>
<p>Similarly we can generate independent draws from the distribution <span class="math inline">\mathrm P(\mathbf x)</span> by first generating a stochastic realization <span class="math inline">\mathbf s</span> of the signal from the corresponding stochastic process and then generating exactly one response according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>.</p>
</section>
<section id="compuation-of-mathrm-pmathbf-xmathbf-s" class="level4" data-number="1.3.2.3">
<h4 data-number="3.3.2.3"><span class="header-section-number">3.3.2.3</span> Compuation of <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span></h4>
<p>Finally, for any signal-response pair of trajectories, we can compute the conditional probability <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> using the formulae for the trajectory probability derived before in eqns. <strong>¿eq:trajectory_probability_product?</strong>, <strong>¿eq:transition_probability?</strong> and inserting the reaction rates for the given signal trajectory <span class="math inline">\mathbf s</span>.</p>
<ul>
<li>Note that the definition of the signal and responses as done in this section implies a strict separation of signal and response (make figure?). We will discuss this issue at the end of this thesis in section TODO</li>
</ul>

</section>
</section>
<section id="sec:initial_condition" class="level3" data-number="1.3.3">
<h3 data-number="3.3.3"><span class="header-section-number">3.3.3</span> Probability Distribution of the Initial State</h3>
<p>We have shown that we can use the stochastic simulation algorithm (SSA) to simulate response trajectories for a given signal. Note however, that the SSA needs us to specify the initial copy numbers of all species in the biochemical network. From this initial state we can then evolve the stochastic dynamics of the reactions in time. Hence, we have to think about the correct choice of the initial condition. In some cases it might be reasonable to have the same predefined initial condition for all responses that are simulated, e.g. in the beginning there is no response <span class="math inline">x(t_0) = 0</span>. In many cases however, the initial states <span class="math inline">x_0</span> will be distributed according to some probability density <span class="math inline">\mathrm P(x_0,t_0)</span>. A common example are systems that start out in a <em>steady state</em>, i.e. at <span class="math inline">t_0</span> both the signal and the response are distributed according to their joint stationary distribution <span class="math inline">p_{S,X}(s,x)</span>.</p>
<p>In that case, to set up the initial conditions correctly, we start by generating one signal trajectory whose initial point <span class="math inline">s_0</span> is drawn from the signal’s stationary distribution <span class="math inline">p_S(s_0)</span>. Thus, to ensure that the initial condition is drawn from <span class="math inline">p_{S,X}(s,x)</span>, we need to pick the initial condition of the response <span class="math inline">x_0</span> from the conditional distribution <span class="math inline">p_{X|S}(x_0|s_0)</span>. For our simulations, we estimate that conditional distribution in a separate computation for every signal that is generated.</p>
<p>If the signal dynamics in steady state are time-reversible, we can estimate the distribution function <span class="math inline">p_{X|S}(x_0|s_0)</span> for a given <span class="math inline">s_0</span> by generating time-reversed signal trajectories. The idea is to generate <span class="math inline">N_\text{past}</span> signal trajectories of duration <span class="math inline">t_\text{past}</span>, <em>backwards in time</em>, starting from <span class="math inline">s_0</span> at time <span class="math inline">t_0</span> that represent possible “pasts” of the system. For every such trajectory we then generate a response trajectory of the same duration <em>forward in time</em>. That is, the response starts at time <span class="math inline">t_0-t_\text{past}</span> with some arbitrary initial state <span class="math inline">x_\text{past}</span> and continues until time <span class="math inline">t_0</span>. Since we generate <span class="math inline">N_\text{past}</span> signals into the past, we end up with <span class="math inline">N_\text{past}</span> response trajectories, whose end points at <span class="math inline">t_0</span> are distributed according to <span class="math inline">p_{X|S}(x_0|s_0)</span>.</p>
<p>From these points <span class="math inline">x^{(1)}_0, \ldots, x^{(N_\text{past})}_0</span> we estimate the distribution function <span class="math inline">p_{X|S}(x_0|s_0)</span> using Gaussian kernel density estimation (KDE) <span class="citation" data-cites="1956.Rosenblatt 1962.Parzen">[<a href="#ref-1956.Rosenblatt" role="doc-biblioref">4</a>,<a href="#ref-1962.Parzen" role="doc-biblioref">5</a>]</span>. The KDE yields a smooth estimate of the conditional distribution <span class="math inline">p_{X|S}(x_0|s_0)</span> from the sample points. This estimated distribution is used in two different ways 1. TODO 2. TODO</p>
<p>In summary, as long as we can formulate a stochastic process that describes the signal dynamics <em>and</em> we understand how the signal level affects the reaction rates of the biochemical network, we can in principle use the Monte-Carlo procedure described in the previous section to estimate the mutual information. From the signal’s SDE we generate stochastic realizations using appropriate numerical integrators for SDEs such as the <em>Euler–Maruyama method</em> or the <em>stochastic simulation algorithm</em>. For every signal realization we have shown how to use the SSA to generate corresponding response trajectories. In the following section we will use these methods on a simple model of gene expression to see how the method is implemented in practice.</p>

</section>
</section>
<section id="returning-to-the-simple-model-of-gene-expression" class="level2" data-number="1.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span> Returning to the Simple Model of Gene Expression</h2>
<p>To evaluate the estimation procedure described so far in this chapter, we used it to compute the mutual information for the simple biochemical network introduced in sec. <strong>¿sec:simple_model?</strong>. As the initial condition we assumed that the system is in the steady state such that we were able to compare our Monte Carlo estimates with the analytical results from eq. <strong>¿eq:analytical_mi?</strong>. We generated all signals and responses using a stochastic simulation algorithm (SSA) for the reaction network in eq. <strong>¿eq:simple_reaction_network?</strong>. The reaction rates were chosen as specified in tbl. <strong>¿tbl:k?</strong> and the computed mutual information is always presented in units of <em>nats</em>, where <span class="math inline">1\ \text{nat} = (1/\ln 2)\, \text{bits}</span>.</p>
<section id="simulating-signals-and-responses" class="level3" data-number="1.4.1">
<h3 data-number="3.4.1"><span class="header-section-number">3.4.1</span> Simulating Signals and Responses</h3>
<p>In the model, the signal dynamics arise from the reactions <span class="math inline">\emptyset\xrightarrow{\kappa} S, S\xrightarrow{\lambda}\emptyset</span> that describe a simple birth-death process. Therefore, to generate signal trajectories we used the SSA with the initial copy numbers distributed according to the stationary distribution <span class="citation" data-cites="2009.Gardiner">[<a href="#ref-2009.Gardiner" role="doc-biblioref">2</a>]</span> <span><span class="math display">P_S(s_0) = e^{-\kappa/\lambda} \frac{(\kappa/\lambda)^{s_0}}{s_0!}\qquad(10)</span></span> for <span class="math inline">s_0\in\mathbb{N}_0</span>. The trajectories generated by the SSA consist of piecewise constant segments in between the transition times <span class="math inline">t_0\ldotst_{N}</span>. We can thus describe a trajectory <span class="math inline">\mathbf s</span> as a function <span class="math inline">s: [t_0, t_N]\rightarrow\mathbb{N}_0</span> given by <span id="eq:piecewise_const"><span class="math display">
s(t) = \sum^{N-1}_{i=0} s_i\ \theta(t-t_{i}) \theta(t_{i+1}-t) 
\qquad(11)</span></span> where <span class="math inline">s_1,\ldots,s_{N-1}</span> are the sequence of copy numbers of <span class="math inline">S</span> and <span class="math inline">\theta</span> is the Heaviside function. When we generate response trajectories corresponding to a given signal trajectory, the transition rate <span class="math inline">w_t(x+1, x)</span> for the reaction <span class="math inline">S\xrightarrow{\rho} S+X</span> is proportional to the function in eq. <a href="#eq:piecewise_const">11</a>, i.e.  <span id="eq:transition_rate"><span class="math display">w_t(x+1, x) = \rho\ s(t)\,.\qquad(12)</span></span> Eq. <a href="#eq:transition_rate">12</a> thus precisely specifies how a given signal trajectory interacts with the response. We use eqns. <strong>¿eq:inverse_function_method?</strong>, <a href="#eq:piecewise_const">11</a>, <a href="#eq:transition_rate">12</a> to compute the stochastic waiting times inside the SSA for the response trajectories.</p>
<p>To use the SSA for the generation of response trajectories, we need to solve eq. <strong>¿eq:inverse_function_method?</strong> which includes performing a time integral of the transition rates. The expression in eq. <a href="#eq:piecewise_const">11</a> allows us to integrate a signal trajectory with respect to time to arrive at <span id="eq:piecewise_linear"><span class="math display">
\int^t_{t_0} \mathrm d\tau\ s(\tau) = \sum^{N-1}_{i=0} s_i \left[ (t-t_i) \ \theta(t-t_i) - (t-t_{i+1}) \ \theta(t-t_{i+1}) \right]
\qquad(13)</span></span> which naturally is a piecewise linear function of <span class="math inline">t</span>. Using eq. <a href="#eq:piecewise_linear">13</a> it is then straightforward to solve eq. <strong>¿eq:inverse_function_method?</strong> for <span class="math inline">u</span> and use the SSA as described in sec. <strong>¿sec:ssa?</strong> to generate response trajectories. For response trajectories, we set the initial copy number <span class="math inline">x_0</span> of <span class="math inline">X</span> according to the empirically generated distribution <span class="math inline">\mathrm P(x_0|s_0)</span>.</p>
<p>Similarly, for the computation of the likelihood <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> we use eqns. <strong>¿eq:transition_probability?</strong>, <strong>¿eq:trajectory_probability_product?</strong> which also depend on time integrals of transition rates. Hence using the results in this section, we can generate stochastic realizations of signal trajectories and corresponding response trajectories. In other words, we can draw samples from the distributions <span class="math inline">\mathrm P(\mathbf s)</span> and <span class="math inline">\mathrm P(\mathbf x| \mathbf s)</span> which we will use to estimate the mutual information.</p>
</section>
<section id="consistent-bias-in-comparisons-with-analytic-approximations" class="level3" data-number="1.4.2">
<h3 data-number="3.4.2"><span class="header-section-number">3.4.2</span> Consistent Bias in Comparisons with Analytic Approximations</h3>
<p>The Monte Carlo estimate of the mutual information (MI) was performed using two independent computations, a) an average over the <em>likelihoods</em> <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> of random responses with respect to their corresponding signals in eq. <a href="#eq:conditional_entropy_estimate">8</a> that yields the conditional entropy and b) an average over the <em>marginal probability densities</em> <span class="math inline">\mathrm P(\mathbf x)</span> of sampled responses in eq. <a href="#eq:mc_entropy">4</a> for the marginal entropy. The difference of these two quantities is our estimate of the MI <span class="math inline">\mathrm I(\mathcal S, \mathcal X) = \mathrm H(\mathcal X) - \mathrm H(\mathcal X|\mathcal S)</span>.</p>
<p>It is crucial to note that the two independent computations involved in the MI are not of equal difficulty. The evaluation of <span class="math inline">\mathrm P(\mathbf x)</span> requires a nested Monte Carlo simulation eq. <a href="#eq:mc_marginal">5</a> for every generated response. The inner compuation of <span class="math inline">\mathrm P(\mathbf x)</span> can suffer especially high Monte Carlo variance since in eq. <a href="#eq:mc_marginal">5</a> the sampling distribution <span class="math inline">\mathrm P(\mathbf s)</span> may have only very little overlap with the integrand <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>. Specifically, it might be the case that most of the generated signals <span class="math inline">\mathbf s_1,\ldots,\mathbf s_{N_S}\sim\mathrm P(\mathcal S)</span> contribute very weakly to the integral since their likelihoods <span class="math inline">\mathrm P(\mathbf x|\mathbf s_i)\approx 0</span> for almost all <span class="math inline">i</span>. Consequently, only a very small number of signals may actually determine the estimate of <span class="math inline">\mathrm P(\mathbf x)</span> which leads to a high variance of the estimate. In conclusion, we expect the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> to be more difficult to estimate than the conditional entropy <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>. This expectation is confirmed by the results in this chapter.</p>
<p>In order to understand how the inner Monte Carlo compuation of <span class="math inline">\mathrm P(\mathbf x)</span> influences the estimate of the MI we performed an array of simulations with different values of <span class="math inline">N_S</span>. The number <span class="math inline">N_S</span> specifies how many signals we generate for every nested Monte Carlo computation of <span class="math inline">\mathrm P(\mathbf x)</span>. We expect that increasing <span class="math inline">N_S</span> leads to a better estimate of the marginal density for every response and therefore improves the estimate of the MI. We compare the simulation results with a reference value for the MI that was obtained by a Gaussian approximation. Using eq. <strong>¿eq:analytical_mi?</strong> we can analytically compute the MI <span class="math inline">\mathrm I(\mathcal S_T, \mathcal X_T)</span> between trajectories of duration <span class="math inline">T</span> and using eq. <strong>¿eq:analytical_rate?</strong> the information rate <span class="math inline">I_R</span> between <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span>. The analytically obtained values for the MI and the information rate are shown as red dotted lines in figs. <a href="#fig:a">1</a>, <a href="#fig:b">2</a>. From a single simulation of the MI with a trajectory duration of <span class="math inline">T_\text{max}</span> we can similarly compute the MI <span class="math inline">\mathrm I(\mathcal S_T, \mathcal X_T)</span> for any <span class="math inline">T\in[0, T_\text{max}]</span> by simply truncating all trajectories to duration <span class="math inline">T</span>. An estimate for the information rate can be obtained by approximating the limit in eq. <strong>¿eq:analytical_rate?</strong> <span><span class="math display">
\hat I_R = \frac{\mathrm I(\mathcal S_{T_\text{max}}, \mathcal X_{T_\text{max}})}{T_\text{max}}
\qquad(14)</span></span> assuming <span class="math inline">T_\text{max} \gg \tau</span> where <span class="math inline">\tau</span> is the longest timescale in the system.</p>
<figure>
<img src="figures/mutual_information_traj.svg" id="fig:a" alt="" /></img><figcaption>Figure 1: Estimates of the mutual information <span class="math inline">\mathrm I(\mathcal S_T, \mathcal X_T)</span> for varying trajectory duration <span class="math inline">T</span>. Every line shows an estimate of <span class="math inline">\mathrm I(\mathcal S_T, \mathcal X_T) = \mathrm H(\mathcal X_T) - \mathrm H(\mathcal X_T|\mathcal S_T)</span> where the marginal and the conditional entropy were computed in separate simulations. For the marginal entropy we generated <span class="math inline">2\times 10^5</span> independent responses per simulation and for every response <span class="math inline">\mathbf x</span> computed the marginal density <span class="math inline">\mathrm P(\mathbf x)</span> using <span class="math inline">N_S</span> signal trajectories. Hence, this figure shows how the nested estimate of the marginal density influences the overall result. While increasing <span class="math inline">N_S</span> helps, all simulations consistently over-estimate the mutual information when compared to the analytic reference result computed using eq. <strong>¿eq:analytical_mi?</strong>. The estimation error appears to increase linearly with trajectory length and therefore leads to a biased estimate of the information rate. For the conditional entropy we generated <span class="math inline">2\times 10^5</span> signals trajectories and for every signal we generated <span class="math inline">10^3</span> response trajectories for the Monte Carlo average in eq. <a href="#eq:conditional_entropy_estimate">8</a>.</figcaption>
</figure>
<p>In fig. <a href="#fig:a">1</a> we show the the results of the different simulations to compute the MI between trajectories of varying durations <span class="math inline">T</span>. We start by describing the features that all simulations have in common. In all cases we see a linear increase of the mutual information with trajectory length. The linearity is to be expected for trajectory durations <span class="math inline">T\gg\tau</span> where <span class="math inline">\tau</span> is the correlation time of the system <span class="citation" data-cites="2010.Tostevin">[<a href="#ref-2010.Tostevin" role="doc-biblioref">6</a>]</span>. If the signal is much longer than the correlation time of the system it is impossible for any future response to still gain information about the beginning of the signal. Hence, for short trajectories, the increase in MI per unit time may be smaller than the asymptotic information rate since the signal has not yet reached the maximal integration length of the system. Thus, while there exist situations where for small <span class="math inline">T</span> the MI increases faster than linear, analytic computations using the Gaussian approximation confirm a fully linear increase of the MI from the start for the simple reaction network under consideration. In fig. <a href="#fig:a">1</a>, we show the result of eq. <strong>¿eq:analytical_mi?</strong> for the Gaussian approximation as the dotted “reference” line.</p>
<p>For trajectory length <span class="math inline">T = 0</span> fig. <a href="#fig:a">1</a> shows that all estimates—including the analytical result—coincide at a small non-zero value for their mutual information. This value is the mutual information between the initial conditions of the signal and the response. The agreement of all simulations shows that using the approach detailed in sec. <a href="#sec:initial_condition">1.3.3</a> we were able correctly estimate the conditional distribution <span class="math inline">\mathrm P(x_0|s_0)</span>. For every response needed during our simulations we first generated 1000 signals backwards in time for <span class="math inline">T_\text{past} = 500</span> to choose a correct starting point for the response and to correctly compute the likelihood <span class="math inline">\mathrm P(x_0|s_0)</span> of the initial condition.</p>
<figure>
<img src="figures/information_rate_traj.svg" id="fig:b" alt="" /></img><figcaption>Figure 2: Estimated information rate <span class="math inline">I_R=\mathrm I(\mathcal S_T, \mathcal X_T)/T</span> from the same data that was used in fig. <a href="#fig:a">1</a>. The reference line represents the analytical result <span class="math inline">I_R = 0.00183</span> from eq. <strong>¿eq:analytical_rate?</strong>. For a correct Monte Carlo simulation we expect the estimated information rate to converge towards the reference line as <span class="math inline">T\rightarrow\infty</span>. However, while we see that the estimates converge towards a stable value for large <span class="math inline">T</span>, all simulations considerably over-estimate the information rate. Even though for increasing <span class="math inline">N_S</span> the estimates <em>do</em> improve we can not acquire reasonable results for the simulations shown.</figcaption>
</figure>
<p>Thus, we see from fig. <a href="#fig:a">1</a> that the estimated MI grows linearly with <span class="math inline">T</span> and that all estimates agree on the MI for <span class="math inline">T=0</span>. However, we see that each of the MI curves has a different slope. All Monte Carlo simulations significantly over-estimate the slope of the analytical reference line. As <span class="math inline">N_S</span> increases we see a slight decrease of the MI slope, however even for largest simulation with <span class="math inline">N_S=1000</span> the slope differs by more than 50% from the analytical reference. Further increasing <span class="math inline">N_S</span> seems to yield diminishing returns. Hence, from the simulations we can’t seem to accurately compute the information rate since it strongly depends on estimating the MI slopes correctly.</p>
<p>Asymptotically for <span class="math inline">T\rightarrow\infty</span> the slope of the MI with respect to the trajectory duration <span class="math inline">T</span> converges towards the information rate <span class="math inline">I_R</span>. In fig. <a href="#fig:b">2</a> we see that all simulations show the convergence of the estimated information rate <span class="math inline">I_R\approx \mathrm I(\mathcal S_T, \mathcal X_T) / T</span> to a stable value already at <span class="math inline">T\approx 150</span>. We again find that we over-estimate the information rate for simulations with <span class="math inline">N_S\in\{10, 20, 50, 100, 200, 500, 1000\}</span> even though we see slight improvement for larger <span class="math inline">N_S</span>. We propose that even further increasing <span class="math inline">N_S</span> and thus spending more CPU time per simulation is not worthwhile to improve the estimates.</p>
<figure>
<img src="figures/information_rate_traj_long.svg" id="fig:c" alt="" /></img><figcaption>Figure 3: Estimated information rate <span class="math inline">I_R=\mathrm I(\mathcal S_T, \mathcal X_T)/T</span> for three estimate with very large <span class="math inline">N_S</span>. The reference line represents the analytical result <span class="math inline">I_R = 0.00183</span> from eq. <strong>¿eq:analytical_rate?</strong>. This plot highlights that even strong increases of <span class="math inline">N_S</span> beyond <span class="math inline">10^3</span> do not yield accurate results for the information rate. Our conclusion from this is that the simple Monte Carlo approach is too naïve and we have to find another way to improve accuracy, instead of spending more CPU time.</figcaption>
</figure>
<p>To support that claim, in fig. <a href="#fig:c">3</a> we show the results of two additional simulation with very high <span class="math inline">N_S</span> and compare them to the analytical reference and the MI for <span class="math inline">N_S=1000</span>. Even for these very resource-intensive simulations we don’t get better approximations for the information rate than for <span class="math inline">N_S=1000</span>. From these results we conclude that our approach is plagued by some more fundamental problem that needs to be solved with other means than merely using more computational resources.</p>
<p>While we expect the main cuprit to be the nested Monte Carlo estimates of the marginal density <span class="math inline">\mathrm P(\mathbf x)</span>, it is difficult to obtain convincing evidence for that claim using the results shown so far. For an indivdual response trajectory <span class="math inline">\mathbf x</span> we don’t have a way to judge the accuracy of our estimate of <span class="math inline">\mathrm P(\mathbf x)</span>. The only analytical result that we can use to check our results is the MI from the Gaussian approximation which is shown as the reference line in figs. <a href="#fig:a">1</a>-<a href="#fig:c">3</a>. Therefore we decided to use the same Monte Carlo procedure to compute the MI with a simpler stochastic description of the signal and response trajectories which allows us to analytically validate all intermediate results. Thus, instead of using the SSA to fully simulate the signal and response dynamics we make the Gaussian approximation as introduced in sec. <strong>¿sec:simple_model?</strong>. Using that approximation, generating a signal or response amounts to taking a draw from a multivariate normal distributions with known covariance matrix.</p>
</section>
</section>
<section id="gaussian-approximation" class="level2" data-number="1.5">
<h2 data-number="3.5"><span class="header-section-number">3.5</span> Gaussian Approximation</h2>
<p>Clearly, there is an issue with the computation of the mutual information (MI) for trajectories that prevents the accurate estimation using the simple Monte Carlo strategy shown above. In figs. <a href="#fig:a">1</a>-<a href="#fig:c">3</a> we saw that we consistently over-estimate the information rate regardless of the amount of computing time that was invested. To allow a deeper analysis of the underlying issue we repeated the estimates of the MI using a simpler, Gaussian model for signals and responses. Instead of simulating trajectories exactly in continuous time using the SSA, we choose a small time-interval <span class="math inline">\Delta t</span> which we use to discretize the system. Conversely, while the SSA simulates individual reactions that lead to a discrete jumps in the trajectories, in the Gaussian approximation the copy numbers are approximated by continuous variables. Hence, we switch from a <em>discrete states and continuous time</em> description to a <em>continuous states and discrete time</em> approximation of the biochemical network.</p>
<section id="time-discretization" class="level3" data-number="1.5.1">
<h3 data-number="3.5.1"><span class="header-section-number">3.5.1</span> Time Discretization</h3>
<p>We discretize the time axis using a set of equidistant points <span class="math inline">t_n = n\Delta t</span> for <span class="math inline">n=1,\ldots,d</span>. A signal trajectory is described a <span class="math inline">d</span>-dimensional vector <span class="math inline">\mathbf s = (s(t_1), \ldots, s(t_d))^T</span> where <span class="math inline">s(t_i)</span> describes the deviation of the signal from its mean value <span class="math inline">\bar s</span> at time <span class="math inline">t_i</span>. The null vector therefore represents a trajectory that remains constant at <span class="math inline">\bar s</span> for its entire duration whereas a non-zero vector indicates fluctuations. The Gaussian approximation for a biochemical network uses the correlation functions to derive a multivariate normal distribution such that the discretized system dynamics approximate the true stochastic dynamics of the network. Given a time-discretization we can use eq. <strong>¿eq:covariance_from_corr?</strong> compute the covariance matrices <span class="math inline">C_{ss}, C_{sx}, C_{xs}, C_{xx}</span> from the correlation functions in eq. <strong>¿eq:correlation_functions?</strong>. From these covariance matrices we can derive all stochastic properties of the Gaussian approximation.</p>
<p>The signal dynamics are given by the probability distribution <span><span class="math display">
\mathrm P(\mathbf{s}) = \frac{1}{\sqrt{\left( 2\pi  \right)^{d} \det C_{ss}}} \;\exp\left[-\frac12\ \mathbf s^T\ C_{ss}^{-1}\ \mathbf s\right]\,,
\qquad(15)</span></span> and similarly the response dynamics are determined by the marginal distribution <span id="eq:analytical_marginal"><span class="math display">
\mathrm P(\mathbf{x}) = \frac{1}{\sqrt{\left( 2\pi  \right)^{d} \det C_{xx}}} \;\exp\left[-\frac12\ \mathbf x^T\ C_{xx}^{-1}\ \mathbf x\right]\,.
\qquad(16)</span></span> Notably, eq. <a href="#eq:analytical_marginal">16</a> allows us to analytically compute the marginal density <span class="math inline">\mathrm P(\mathbf x)</span> without performing a Monte Carlo estimate of the integral <span class="math inline">\int\mathrm d\mathbf s\ \mathrm P(\mathbf s)\,\mathrm P(\mathbf x|\mathbf s)</span>. The joint distribution of <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> is given in eq. <strong>¿eq:joint_multivariate?</strong> which is a <span class="math inline">2d</span>-dimensional multivariate normal density function with covariance matrix <span id="eq:corr_z2"><span class="math display">
Z =  \begin{pmatrix}
C_{ss}  C_{xs} \\
C_{sx}  C_{xx}
\end{pmatrix}\,.
\qquad(17)</span></span></p>
<figure>
<img src="matrix_plots.png" id="fig:corr" alt="" /></img><figcaption>Figure 4: Matrix plots of the full correlation matrix <span class="math inline">Z</span> from eq. <a href="#eq:corr_z2">17</a> for different values of dimensionality <span class="math inline">d</span> and <span class="math inline">\Delta t</span>. Brighter colors indicate higher matrix element values. We can clearly observe the block structure of <span class="math inline">Z</span> in every matrix plot. For every matrix plot, the element with coordinates <span class="math inline">(i,j)</span> in the top left quadrant shows the correlations <span class="math inline">\langle s(i\Delta t) s(j \Delta t)\rangle</span>. In the top right quadrant we see the correlations <span class="math inline">\langle s(i\Delta t) x(j \Delta t)\rangle</span> and in the lower quadrants we see <span class="math inline">\langle x(i\Delta t) s(j \Delta t)\rangle</span> and <span class="math inline">\langle x(i\Delta t) x(j \Delta t)\rangle</span> on the left and right side respectively. The product <span class="math inline">T = d\Delta t</span> is the duration of the signal and response trajectories. The quantity <span class="math inline">d\Delta t</span> also serves as a rough measure of the sparsity of the correlation matrices (i.e. the fraction of matrix elements lower than some cutoff). Along diagonals from bottom left to top right, the product <span class="math inline">T = d \Delta t</span> is constant. Indeed, we see that along these diagonals the sparsity is roughly constant. Yet, as we move along such a diagonal of equal sparsity <span class="math inline">d\Delta t</span> but increasing dimensionality <span class="math inline">d</span> we see correlation matrices that display the same features in a gradually more refined and smooth way.</figcaption>
</figure>
<p>Using this discretization we have two parameters left to tune. We can freely choose the number <span class="math inline">d</span> and offsets <span class="math inline">\Delta t</span> of our time samples. The duration of the trajectories <span class="math inline">\mathbf s</span> and <span class="math inline">\mathbf x</span> is given by the product <span class="math inline">T=d\Delta t</span>. In fig. <a href="#fig:corr">4</a> we show matrix plots of the joint covariance matrix <span class="math inline">Z</span> for different values of <span class="math inline">d</span> and <span class="math inline">\Delta t</span>. We can also observe that <span class="math inline">d</span> determines the dimensionality of the problem while the product <span class="math inline">d \Delta t</span> serves as a measure for the sparsity of the correlation matrices.</p>
<p>Note that the choice of <span class="math inline">\Delta t</span> affects how well the discretized trajectories approximate physical continuous-time trajectories. Thus, usually we would have to choose <span class="math inline">\Delta t</span> to be relatively small such that the discretized process has similar dynamics compared to the continuous Gaussian process. For our purposes, this consideration is of secondary importance. We are merely comparing the Monte-Carlo estimates for the mutual information to the analytic results, obtained by computing the determinants of the covariance matrices. For a consistent Monte-Carlo procedure we should expect convergence of the estimates to the analytic results given enough computing time. That is, while the choices of <span class="math inline">d</span> and <span class="math inline">\Delta t</span> should not affect the correctness of our procedure, it very well may strongly affect the computational resources needed for efficient estimation.</p>
</section>
<section id="direct-importance-sampling" class="level3" data-number="1.5.2">
<h3 data-number="3.5.2"><span class="header-section-number">3.5.2</span> Direct Importance Sampling</h3>
<p>We want to use this fully Gaussian model to understand how the sample sizes of the different Monte Carlo steps affect the estimate and whether there exists a bias in the approximation. We calculate the marginal entropy as a Monte Carlo average over the logarithms of the marginal distribution densities of <span class="math inline">N_x</span> sampled responses as shown in eq. <a href="#eq:mc_entropy">4</a>. The evaluation of the marginal density itself requires a Monte Carlo average over <span class="math inline">N_s</span> sampled signals (eq. <a href="#eq:mc_marginal">5</a>). Hence to evaluate the marginal density we need to perform nested averaging as shown in eq. <a href="#eq:mc_entropy_notation">6</a>. We performed this procedure for various values of <span class="math inline">N_s</span> and <span class="math inline">N_x</span> and compared the estimate with reference results using the analytical expression for the entropy of a multivariate Gaussian distribution.</p>
<p>Both, increase of <span class="math inline">N_x</span> and increase of <span class="math inline">N_s</span> should lead to an improved estimate of <span class="math inline">\mathrm H(\mathcal X)</span>. To understand the accuracy of an estimate with a given <span class="math inline">N_s</span> and <span class="math inline">N_x</span> we repeat the estimation procedure multiple times and compute the mean and the standard deviation of the individual estimation results.</p>
<figure>
<img src="relative_error_responses.svg" id="fig:rel_err_responses" alt="" /></img><figcaption>Figure 5: Top: relative error for the marginal entropy as a function of <span class="math inline">1/N_x</span>. Bottom: empirical variance of ensembles of 144 estimates. The solid lines show a linear extrapolation of the data points for <span class="math inline">N_x \rightarrow\infty</span>. All estimates were performed using a constant number of signal samples <span class="math inline">N_s = 400</span> and for <span class="math inline">d = 200</span>. The linear extrapolation in the bottom plot indicates that we do predict the variance of the results to vanish in the limit of infinite sampling. This behavior is generally expected for Monte Carlo estimates. Strikingly however, we find that there is a consistent offset of the average estimate from the correct result, even in the limit <span class="math inline">N_x \rightarrow\infty</span>. We see that the bias scales with the sparsity of the covariance matrices. The relative error is computed using <span class="math inline">\mathrm H_\text{estimate}/\mathrm H_\text{analytical} - 1</span> where <span class="math inline">\mathrm H_\text{estimate}=\sum^{M}_{i=1} \hat{\mathrm H}_i/M</span> is the average over the results of the <span class="math inline">M=144</span> marginal entropy estimates that were performed using eq. <a href="#eq:mc_entropy_notation">6</a> and <span class="math inline">\mathrm H_\text{analytical}</span> is the value resulting from an analytical computation of the marginal entropy. The empirical variance shown is <span class="math inline">\sum^{M}_{i=1} (\hat{\mathrm H}_i - \mathrm H_\text{estimate})^2/M</span>.</figcaption>
</figure>
<p>In fig. <a href="#fig:rel_err_responses">5</a> we see how the relative error of our estimate varies with the number of simulated responses <span class="math inline">N_x</span>. Here use the same number of signals per response <span class="math inline">N_s</span> for all estimates. While—as expected—the variance of the estimate decreases when we increase <span class="math inline">N_x</span> we find that especially for very sparse covariance matrices we consistently over-estimate the marginal entropy. Indeed, we find that the systematic bias in our results seems to be independent of <span class="math inline">N_x</span>.</p>
<p>We found that the bias is stronger for correlation matrices with higher sparsities <span class="math inline">d\Delta t</span>. Since the sparsity grows with trajectory duration we can expect an increasingly strong over-estimation for longer trajectories. The sparsity can be increased either by decreasing the time-resolution or by increasing the dimensions of the covariance matrix. To understand how these parameters relate to each other we tested how the estimation error changes when we increase the dimensionality of the correlation matrices while the sparsity remains constant.</p>
<p>Fig. <a href="#fig:sparsity">6</a> shows how large the estimation error for the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> is on average for different levels of sparsity. We see that in all cases that increasing the sparsity leads to larger errors in the estimates. Additionally we find that for a given sparsity value, the estimates with high-dimensional covariance matrices are slightly worse. As we keep increasing the number of dimensions <span class="math inline">d</span> at constant sparsity <span class="math inline">d\Delta t</span>, thus decreasing <span class="math inline">\Delta t</span>, the matrices gradually become a more faithful representation of the continuous correlation functions of the system (see fig. <a href="#fig:corr">4</a>). Extrapolating the lines in fig. <a href="#fig:sparsity">6</a> we project that for very large covariance matrices, the sparsity is the only determining factor of the estimation bias.</p>
<figure>
<img src="sparsity.svg" id="fig:sparsity" alt="" /></img><figcaption>Figure 6: Absolute error of marginal entropy estimates for different values of the sparsity <span class="math inline">d\Delta t</span> of the correlation matrices. We see that for high dimensionality the lines of constant sparsity become increasingly flat. This indicates that for high-dimensional systems the sparsity of the covariance matrix is a good measure for the difficulty of correct estimation. We therefore claim that the bias of the entropy estimate for the Gaussian system primarily depends on the sparsity of the covariance matrix. Note that for lower numbers of dimensions the covariance matrices of along the diagonals of equal sparsity look more blocky (see fig. <a href="#fig:corr">4</a>). That may be an indicator why the estimation error is not constant for a given sparsity at lower dimensions.</figcaption>
</figure>
<figure>
<img src="error_grid.svg" id="fig:error_regression" alt="" /></img><figcaption>Figure 7: Relative error <span class="math inline">\mathrm H_\text{estimate}/\mathrm H_\text{analytical} - 1</span> as a function of <span class="math inline">1/N_s</span>. We can see that the relative error in the marginal entropy estimate increases with the sparsity <span class="math inline">d\Delta t</span> (i.e. with trajectory duration). The linear extrapolating lines emphasize that there is a noticeable but very slight decrease in error as <span class="math inline">N_s\to\infty</span>. This seems puzzling since for infinite sampling we should expect the error to vanish. Apparently for high-sparsity covariance matrices we need extraordinarily many signal samples to achieve unbiased estimates.</figcaption>
</figure>
<p>As a next step we investigated how changes in the sampling for the marginal density <span class="math inline">\mathrm P(\mathbf x_i)</span> affect the estimation bias. Thus in fig. <a href="#fig:error_regression">7</a> we show how the increase of simulated signals per response <span class="math inline">N_s</span> improves the estimate of the marginal entropy. Here we again see that for high number of dimensions we over-estimate the marginal entropy. An increase of <span class="math inline">N_s</span> does lead to slightly less over-estimation but the linear extrapolation indicates that even if we choose enormously high values for <span class="math inline">N_s</span> we can not expect to reduce the bias substantially.</p>
<p>For a given number of samples, the fraction of the trajectory space probed by the Monte Carlo scheme is lower for longer durations. Therefore, for a given number of Monte Carlo samples we expect the estimate to become worse for longer trajectories, i.e. when the sparsity of the covariance matrix is high. This is confirmed by our results. Furthermore and more surprisingly we find that we consistently over-estimate the marginal entropy and while increasing <span class="math inline">N_s</span> <em>does</em> reduce the bias slightly it appears to require an astronomically high sampling in signal trajectory space to reach arbitrary low errors. An increase in <span class="math inline">N_x</span> however reduces the variance of the results but does not influence the bias at all.</p>
<p>Thus we are lead to believe that the main difficulty in estimating the marginal entropy is the Monte-Carlo marginalization of the probability density function. To estimate <span class="math inline">\mathrm P(\mathbf x)</span> we sample signals from the marginal distribution <span class="math inline">\mathrm P(\mathbf s)</span> and average over the likelihoods <span class="math inline">\mathrm P(\mathbf x | \mathbf s)</span>. However as the space of signals becomes increasingly vast for longer trajectories, it becomes more and more unlikely to sample a signal <span class="math inline">\mathbf s</span> where <span class="math inline">\mathbf x</span> has a non-vanishing likelihood of occurring. Hence the duration of the trajectories strongly influences the bias of the marginal entropy computation and we are well advised to keep the trajectories as short as possible. To capture the essential system dynamics however, the trajectories must be at least as long as the longest timescale <span class="math inline">\tau</span> in the system. If we are interested in the marginal entropy for timescales longer than the longest timescale in the system we can then use the fact that trajectory pieces of duration <span class="math inline"> \tau</span> become independent of each other and we can add the individual entropies of these pieces to get the full entropy for such a long trajectory. For example, if we were interested in the marginal entropy of trajectories of duration <span class="math inline">T=\ell\tau</span> where <span class="math inline">\ell\in\mathbb N^+</span> we could approximate it by estimating the entropy <span class="math inline">\hat{H}_\tau</span> of trajectories of duration <span class="math inline">\tau</span> and then take <span class="math inline">\ell \hat{H}_\tau</span> as an approximation for <span class="math inline">\hat{H}_{\ell\tau}</span>. While performing the estimate <span class="math inline">\hat{H}_\tau</span> is computationally much cheaper than directly estimating <span class="math inline">\hat{H}_{\ell\tau}</span> and therefore for a given amount of CPU-time we get a lower systematic bias of the estimate <span class="math inline">\hat{H}_\tau</span>, the bias of <span class="math inline">\ell \hat{H}_\tau</span> of course still scales linearly with <span class="math inline">\ell</span>. Therefore it is still in our best interest to reduce the systematic bias of the marginal entropy as much as possible.</p>
</section>
<section id="sec:umbrella" class="level3" data-number="1.5.3">
<h3 data-number="3.5.3"><span class="header-section-number">3.5.3</span> Umbrella Sampling</h3>
<p>To get a better estimate of <span class="math inline">\mathrm P(\mathbf x)</span> we decided to use <em>umbrella sampling</em>, i.e. to bias our sampling strategy towards signals that we expect to have a high likelihood for the given response. Since Monte-Carlo estimates depend on the distribution of the chosen samples we must correct our estimate by re-weighing the samples accordingly.</p>
<p>More specifically, given a sampling distribution <span class="math inline">w(\mathbf s)</span> (which is normalized like a probability density function) we can write <span><span class="math display">
\mathrm P(\mathbf x) = \int \mathrm d\mathbf s\ w(\mathbf s)\frac{\mathrm P(\mathbf s)\mathrm P(\mathbf x | \mathbf s)}{w(\mathbf s)} = \left\langle \frac{\mathrm P(\mathbf s)\mathrm P(\mathbf x | \mathbf s)}{w(\mathbf s)} \right\rangle_{w(\mathbf s)}
\qquad(18)</span></span> which shows us how to compute <span class="math inline">\mathrm P(\mathbf x_i)</span> from signal trajectories <span class="math inline">\mathbf s_1^w, \ldots, \mathbf s_{N_s}^w</span> which are distributed according to the sampling distribution given by <span class="math inline">w</span>: <span><span class="math display">
\mathrm P(\mathbf x_i) \approx \frac1{N_s} \sum\limits_{j=1}^{N_s} \frac{\mathrm P(\mathbf s_j^w)\mathrm P(\mathbf x_i | \mathbf s_j^w)}{w(\mathbf s_j^w)} \equiv P_{\mathbf x_i, N_s}^w \,.
\qquad(19)</span></span></p>
<p>The choice of the sampling distribution <span class="math inline">w</span> has a direct impact on the variance of an ensemble of estimates <span class="math inline">P_{\mathbf x_i, N_s}^w</span>. Indeed, for any given <span class="math inline">\mathbf x_i</span> there is an optimal choice for the sampling distribution <span class="math inline">w_\text{opt}</span> such that the variance of the estimates vanishes. This optimal choice is given by <span class="math inline">w_\text{opt}(\mathbf s) = \mathrm P(\mathbf s | \mathbf x_i)</span> which is easily confirmed by the calculation <span id="eq:opt_sampling"><span class="math display">
P_{\mathbf x_i, N_s}^{w_\text{opt}} = \frac1{N_s}\sum\limits_{j=1}^{N_s} \frac{\mathrm P(\mathbf s_j^w)\mathrm P(\mathbf x_i | \mathbf s_j^w)}{\mathrm P(\mathbf s_j^w | \mathbf x_i)} = \frac1{N_s}\sum\limits_{j=1}^{N_s} \mathrm P(\mathbf x_i)
\qquad(20)</span></span> where in the last step we applied Bayes’ rule. Since the expression above is completely independent of the chosen signal samples the result is deterministic and thus has zero variance. We also see from eq. <a href="#eq:opt_sampling">20</a> that in practice we can’t directly use <span class="math inline">w_\text{opt}(\mathbf s) = \mathrm P(\mathbf s | \mathbf x_i)</span> as our sampling distribution since the evaluation of <span class="math inline">\mathrm P(\mathbf s | \mathbf x_i) = \frac{\mathrm P(\mathbf x_i|\mathbf s) \mathrm P(\mathbf s)}{\mathrm P(\mathbf x_i)}</span> itself depends on <span class="math inline">\mathrm P(\mathbf x_i)</span> which is precisely the quantity we are interested in estimating.</p>
<figure>
<img src="sampling2.svg" id="fig:rel_err_opt" alt="" /></img><figcaption>Figure 8: Relative error as a function of the dimensionality <span class="math inline">d</span>. The solid lines show the results using non-optimized sampling while the dashed lines show the results when using a sampling distribution close to the optimal distribution <span class="math inline">\mathrm P(\mathbf s|\mathbf x)</span>. We see that with optimized sampling there is no consistent over-estimation anymore. All estimated were done using <span class="math inline">d = 200</span> dimensional covariance matrices.</figcaption>
</figure>
<p>Instead, we can try to obtain a sampling distribution that is as close as possible to <span class="math inline">w_\text{opt}(\mathbf s)</span>. A known approach involves using random samples from <span class="math inline">\mathrm P(\mathbf s | \mathbf x_i)</span> to pick the most optimal sampling distribution from a family of candidate distributions <span class="citation" data-cites="2011.Chan">[<a href="#ref-2011.Chan" role="doc-biblioref">7</a>]</span>. Generating so-called <em>posterior samples</em> from <span class="math inline">\mathrm P(\mathbf s | \mathbf x_i) \sim \mathrm P(\mathbf x_i|\mathbf s) \mathrm P(\mathbf s)</span> is generally possible without knowledge of the normalization factor <span class="math inline">\mathrm P(\mathbf x_i)</span> e.g. by using Metropolis-Sampling <span class="citation" data-cites="1991.Müller 1994.Tierney">[<a href="#ref-1991.Müller" role="doc-biblioref">8</a>,<a href="#ref-1994.Tierney" role="doc-biblioref">9</a>]</span>. To test within the Gaussian framework whether such an approach to importance sampling could work in principle, we generate 400 posterior samples by directly sampling from the analytically known posterior distribution <span class="math inline">\mathrm P(\mathbf s | \mathbf x_i)</span>. We compute the empirical mean <span class="math inline">\bar{\mathbf s}</span> and the empirical covariance <span class="math inline">\bar C_{\mathbf s|\mathbf x_i}</span> of these samples as parameter estimates for a multivariate Gaussian <span class="math inline">\mathcal N(\bar{\mathbf s}, \bar C_{\mathbf s|\mathbf x_i})</span> and use the latter as an optimized sampling distribution.</p>
<p>In fig. <a href="#fig:rel_err_opt">8</a> we show that using optimized sampling we can strongly reduce the systematic bias in marginal entropy estimation. As expected, importance sampling is especially useful when the sparsity is very high, i.e. the trajectories are long. It is clear that for longer trajectories we expect <span class="math inline">\mathrm P(\mathbf s | \mathbf x_i)</span> to be a much more narrow sampling distribution than <span class="math inline">\mathrm P(\mathbf s)</span> whenever the <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> are not completely independent. Consequently, it becomes more and more unlikely to obtain a sample <span class="math inline">s^\prime</span> from <span class="math inline">\mathrm P(\mathbf s)</span> such that <span class="math inline">\mathrm P(s^\prime | \mathbf x_i)  \epsilon</span> for any <span class="math inline">\epsilon  0</span> and therefore more difficult to accurately estimate <span class="math inline">\mathrm P(\mathbf x_i)</span> using an unbiased sampling distribution.</p>

</section>
<section id="estimating-the-conditional-entropy-1" class="level3" data-number="1.5.4">
<h3 data-number="3.5.4"><span class="header-section-number">3.5.4</span> Estimating the Conditional Entropy</h3>
<figure>
<img src="conditional.svg" id="fig:conditional" alt="" /></img><figcaption>Figure 9: Comparison of the relative error of conditional entropy estimates versus marginal error estimates. The relative errors are shown on a logarithmic scale as a function of the sparsity. We can see that the relative error for the estimate of the conditional entropy is a few orders of magnitude smaller than the estimates of the marginal entropy. All estimates were performed with <span class="math inline">N_x=25600</span> and <span class="math inline">N_s=1000</span>.</figcaption>
</figure>
<p>For both, marginal entropy and conditional entropy we have to evaluate the likelihood <span class="math inline">\mathrm P(\mathbf x| \mathbf s)</span> a total of <span class="math inline">N_s N_x</span> times. To compare the accuracy of we performed estimates of the marginal entropy with and without optimized sampling together with estimates of the conditional entropy for <span class="math inline">N_s = 1000</span> and <span class="math inline">N_x = 25600</span>. In fig. <a href="#fig:conditional">9</a> we show the relative error of both, marginal and conditional entropy estimates as a function of the sparsity. We find that the estimate of the conditional entropy is very accurate regardless of sampling size. Even with optimized sampling the marginal entropy estimate is roughly two orders of magnitude worse than a comparable conditional entropy estimate.</p>
</section>
</section>
<section id="discussion" class="level2" data-number="1.6">
<h2 data-number="3.6"><span class="header-section-number">3.6</span> Discussion</h2>
<p>Since both the estimate of the marginal entropy and of the conditional entropy require the computation of two nested Monte-Carlo averages one could expect the results of both estimates to be of similar accuracy. Yet we find that computing the marginal entropy is much more challenging than computing the conditional entropy. While analyzing the estimation procedure for the marginal entropy we found the main source of error to arise from the computation of the marginal probability density <span class="math inline">\mathrm P(\mathbf x)</span>. While the computation of this density suffers from high Monte-Carlo variance when we are not carefully optimizing our trajectory sampling procedure, the real issue arises in the next step when we have to <em>compute the logarithm of the marginal probability density</em> to estimate the marginal entropy <span class="math inline">\mathrm H(\mathcal X) \approx \sum^{N_x}_{i=1}-\ln\mathrm P(\mathbf x_i) / N_x</span> from our sampled response trajectories (see also eq. <a href="#eq:mc_entropy_notation">6</a>). For us, computing <span class="math inline">\ln\mathrm P(\mathbf x_i)</span> means computing the logarithm of an average. Taking the logarithm—which is concave function—of a Monte-Carlo average leads to a consistent bias. We see the existence of this bias in our results since we consistently over-estimate the marginal entropy. Indeed, the reason why it is much easier to get a good estimate of the conditional entropy is that in the latter case we have to average the logarithm of a quantity (see eq. <strong>¿eq:conditional_entropy?</strong>) rather than taking the logarithm of an average, as we need for the marginal entropy.</p>
<p>We can thus conclude that the main difficulty of obtaining a good estimate for the mutual information between trajectories lies in the efficient and accurate computation of the marginal entropy. A viable approach for this seems to be to use importance sampling in the signal space in the computation of the marginal probability density. Our results indicate that such an approach could also work for trajectories generated using a fully stochastic model of a biochemical network. Another direction to pursue might be to use the replica trick, based on the mathematical identity that <span class="math inline">\ln Z = \lim_{n\to 0} (Z^n-1) / n</span>. This may allow us to eliminate the systematic bias by circumventing the need to take the logarithm of an estimate.</p>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-2019.Cepeda-Humerez">
<p>[1] S.A. Cepeda-Humerez, J. Ruess, G. Tkačik, Estimating information in time-varying signals., PLoS Computational Biology. 15 (2019) e1007290. <a href="https://doi.org/10.1371/journal.pcbi.1007290">https://doi.org/10.1371/journal.pcbi.1007290</a>.</p>
</div>
<div id="ref-2009.Gardiner">
<p>[2] C. Gardiner, Stochastic Methods, 4th ed., Springer-Verlag, Berlin Heidelberg, 2009.</p>
</div>
<div id="ref-1992.Kloeden">
<p>[3] P.E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations, 1992.</p>
</div>
<div id="ref-1956.Rosenblatt">
<p>[4] M. Rosenblatt, Remarks on Some Nonparametric Estimates of a Density Function, The Annals of Mathematical Statistics. 27 (1956) 832–837. <a href="https://doi.org/10.1214/aoms/1177728190">https://doi.org/10.1214/aoms/1177728190</a>.</p>
</div>
<div id="ref-1962.Parzen">
<p>[5] E. Parzen, On Estimation of a Probability Density Function and Mode, The Annals of Mathematical Statistics. 33 (1962) 1065–1076. <a href="https://doi.org/10.1214/aoms/1177704472">https://doi.org/10.1214/aoms/1177704472</a>.</p>
</div>
<div id="ref-2010.Tostevin">
<p>[6] F. Tostevin, P.R. ten Wolde, Mutual information in time-varying biochemical systems, Physical Review E. 81 (2010) 061917. <a href="https://doi.org/10.1103/physreve.81.061917">https://doi.org/10.1103/physreve.81.061917</a>.</p>
</div>
<div id="ref-2011.Chan">
<p>[7] J.C.C. Chan, D.P. Kroese, Improved cross-entropy method for estimation, Statistics and Computing. 22 (2011) 1031–1040. <a href="https://doi.org/10.1007/s11222-011-9275-7">https://doi.org/10.1007/s11222-011-9275-7</a>.</p>
</div>
<div id="ref-1991.Müller">
<p>[8] P. Müller, A Generic Approach to Posterior Integration and Gibbs Sampling, Purdue University, 1991.</p>
</div>
<div id="ref-1994.Tierney">
<p>[9] L. Tierney, Markov Chains for Exploring Posterior Distributions, The Annals of Statistics. 22 (1994) 1701–1728. <a href="https://doi.org/10.1214/aos/1176325750">https://doi.org/10.1214/aos/1176325750</a>.</p>
</div>
</div>
</section>
</section>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="modeling-cell-signaling-networks-as-information-processing-devices.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="directed-sampling-in-trajectory-space.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="modeling-cell-signaling-networks-as-information-processing-devices.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="directed-sampling-in-trajectory-space.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        

        

        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>