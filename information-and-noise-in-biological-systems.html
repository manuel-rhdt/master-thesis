<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js Light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mutual Information between Trajectories</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
        <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var regex = /\\qquad\W*\(([0-9]+)\)/;
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    var tex_str = texText.data.replace(regex, "\\tag{$1}");
                    katex.render(tex_str, mathElements[i], {
                    displayMode: mathElements[i].classList.contains('display'),
                    throwOnError: false,
                    fleqn: false
                });
            }}});
        </script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "" : "Light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('Light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded"><a href="index.html" class=""><span class="header-section-number">1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html" class="active"><span class="header-section-number">2</span> Information and Noise in Biological Systems
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-theory-in-the-context-of-cellular-signaling-networks" class=""><span class="header-section-number">2.1</span> Information Theory in the context of cellular signaling networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#mutual-information-as-an-efficiency-measure-in-cell-signaling" class=""><span class="header-section-number">2.1.1</span> Mutual Information as an Efficiency Measure in Cell signaling
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-transmission-for-time-varying-signals" class=""><span class="header-section-number">2.1.2</span> Information Transmission for Time-Varying Signals
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#stochastic-modeling-of-biochemical-networks" class=""><span class="header-section-number">2.2</span> Stochastic Modeling of Biochemical Networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#chemical-master-equation" class=""><span class="header-section-number">2.2.1</span> Chemical Master Equation
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#jump-processes" class=""><span class="header-section-number">2.2.2</span> Jump Processes
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#generating-stochastic-trajectories-for-jump-processes" class=""><span class="header-section-number">2.3</span> Generating Stochastic Trajectories for Jump Processes
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#direct-gillespie-algorithm" class=""><span class="header-section-number">2.3.1</span> Direct Gillespie Algorithm
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#monte-carlo-computation-of-the-mutual-information" class=""><span class="header-section-number">2.4</span> Monte-Carlo computation of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">2.4.1</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">2.4.2</span> Estimating the Conditional Entropy
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html" class=""><span class="header-section-number">3</span> Monte-Carlo in Trajectory space
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-in-trajectory-space.html#computing-the-likelihood" class=""><span class="header-section-number">3.0.1</span> Computing the likelihood
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#the-probability-density-for-the-starting-point-of-a-trajectory" class=""><span class="header-section-number">3.0.2</span> The probability density for the starting point of a trajectory
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#estimating-the-marginal-probability-of-response-trajectories" class=""><span class="header-section-number">3.0.3</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#simulating-a-biochemical-network-driven-by-an-external-signal" class=""><span class="header-section-number">3.1</span> Simulating a Biochemical Network Driven by an External Signal
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#monte-carlo-computation-of-the-mutual-information" class=""><span class="header-section-number">3.2</span> Monte-Carlo computation of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-in-trajectory-space.html#computing-the-likelihood-1" class=""><span class="header-section-number">3.2.1</span> Computing the likelihood
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#the-probability-density-for-the-starting-point-of-a-trajectory-1" class=""><span class="header-section-number">3.2.2</span> The probability density for the starting point of a trajectory
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#estimating-the-marginal-probability-of-response-trajectories-1" class=""><span class="header-section-number">3.2.3</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#references" class=""><span class="header-section-number">3.3</span> References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html" class=""><span class="header-section-number">4</span> Mutual Information for Trajectories in a Gaussian Framework
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#introduction" class=""><span class="header-section-number">4.1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">4.2</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#choice-of-covariance-matrices" class=""><span class="header-section-number">4.2.1</span> Choice of Covariance Matrices
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#direct-importance-sampling" class=""><span class="header-section-number">4.2.2</span> Direct Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#umbrella-sampling" class=""><span class="header-section-number">4.2.3</span> Umbrella Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">4.3</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#discussion" class=""><span class="header-section-number">4.4</span> Discussion
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html" class=""><span class="header-section-number">5</span> Estimation Strategies Inspired by Statistical Physics
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#borrowing-terminology-from-statistical-physics" class=""><span class="header-section-number">5.1</span> Borrowing Terminology from Statistical Physics
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#thermodynamic-integration" class=""><span class="header-section-number">5.2</span> Thermodynamic Integration
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#summary-of-ti" class=""><span class="header-section-number">5.2.1</span> Summary of TI
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#markov-chain-monte-carlo" class=""><span class="header-section-number">5.2.2</span> Markov Chain Monte Carlo
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#estimating-the-density-of-states" class=""><span class="header-section-number">5.3</span> Estimating the Density of States
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#wang-and-landau-algorithm" class=""><span class="header-section-number">5.4</span> Wang and Landau Algorithm
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#applying-wang-landau-to-the-computation-of-the-marginal-density" class=""><span class="header-section-number">5.4.1</span> Applying Wang-Landau to the Computation of the Marginal Density
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#modified-dos" class=""><span class="header-section-number">5.4.1.1</span> Modified DOS
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#modified-wang-landau-algorithm" class=""><span class="header-section-number">5.4.1.2</span> Modified Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#comparison-to-usual-wang-landau-algorithm" class=""><span class="header-section-number">5.4.1.3</span> Comparison to usual Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#connection-to-standard-monte-carlo-sampling" class=""><span class="header-section-number">5.4.1.4</span> Connection to Standard Monte-Carlo Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#example-results-for-a-wang-landau-simulation" class=""><span class="header-section-number">5.4.2</span> Example Results for a Wang-Landau Simulation
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#conclusion" class=""><span class="header-section-number">5.5</span> Conclusion
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                    </div>

                    <h1 class="menu-title">Mutual Information between Trajectories</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/manuel-rhdt/master-thesis" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <section id="information-and-noise-in-biological-systems" class="level1" data-number="1">
<h1 data-number="2"><span class="header-section-number">2</span> Information and Noise in Biological Systems</h1>
<p><em>Noise</em> is inherent across diverse biological systems and remains relevant at all biological scales. From <em>stochastic gene expression</em> and random <em>action potential spikes</em> in neuronal networks at the cellular scale to the <em>development of multicellular organisms</em> all the way to the <em>variations in population level</em> of competing species in whole ecosystems, we find examples of processes which can only be described precisely by taking into account noise as an intrinsic feature <span class="citation" data-cites="2002:Elowitz 2008:Faisal 1990:Parsons 2011:Hallatschek 2014:Tsimring">[<a href="#ref-2002:Elowitz" role="doc-biblioref">1</a>–<a href="#ref-2014:Tsimring" role="doc-biblioref">5</a>]</span>. In this thesis we focus on the smaller end of this scale, namely on the stochastic description of <em>biochemical networks</em>. These comprise among others <em>gene expression</em>, <em>gene regulatory networks</em>, and <em>cell signaling</em> networks, all of which exhibit noise due to small copy numbers of participating components. Additionally, since biological processes often happen out of equilibrium even macroscopic quantities can exhibit large fluctuations. The main source of noise at the cellular level may be fluctuations in <em>gene expression</em> which propagate to higher levels of biological organization <span class="citation" data-cites="2014:Tsimring">[<a href="#ref-2014:Tsimring" role="doc-biblioref">5</a>]</span>. The abundance of noise in all these systems invites the questions of how cells can reliably make correct decisions, even in complex and changing environments and how cells are able to <em>encode</em> the information about their environment using biochemical networks.</p>
<p>To successfully function, cells must generally correctly react to environmental changes. This requires processing the environmental cues they receive through their receptors and thereby filtering out the useful information from the noisy signal. The processing of information through biochemical networks can be quite elaborate with individual network components being able to perform analogous functions as silicon computational devices such as transistors <span class="citation" data-cites="2020:Leifer">[<a href="#ref-2020:Leifer" role="doc-biblioref">6</a>]</span>. It is thus tempting to think that optimization of information processing drives the evolution of cellular signaling networks. To understand information processing from the cell’s point of view we employ the very general framework of <em>Information Theory</em> which was originally developed to address questions of the reliability of telecommunications <span class="citation" data-cites="1948:Shannon">[<a href="#ref-1948:Shannon" role="doc-biblioref">7</a>]</span>. Information theory has been successfully used in biology to study cellular communication, embryonic development and other biological systems <span class="citation" data-cites="2009:Tkačik 2020:Uda">[<a href="#ref-2009:Tkačik" role="doc-biblioref">8</a>,<a href="#ref-2020:Uda" role="doc-biblioref">9</a>]</span> [citation needed]. Notably, there are many parallels between biological signal processing and <em>noisy channels</em> which are used for example to describe information transmission across a telephone line.</p>
<p>A crucial feature of <em>Information Theory</em> is that its results are broadly applicable, irrespective of the nature of the communication channel or the medium used to transmit a signal. The communication channel merely describes any kind of abstract device that processes an <em>input</em> in a probabilistic way to yield a corresponding <em>output</em>. It turns out that the study of information transmission through such a channel is closely related to the study of noise since the amount of noise introduced by a communication channel sets an upper bound to the amount of data that can be transmitted through it, known as the <em>channel capacity</em> <span class="citation" data-cites="2006:Cover">[<a href="#ref-2006:Cover" role="doc-biblioref">10</a>]</span>. Consequently, the output can be described as a deterministic, lossless transformation of the input <em>plus</em> some random noise from the channel which leads to a loss of information. Note that in biological systems the signal itself is typically a fluctuating quantity such that the noise in the output is a combination of the channel noise and the signal noise. Since in cell signaling both, input and output are time-varying quantities we require a description of our system that allows for deterministic <em>and</em> stochastic time evolution.</p>
<p><em>Differential equations</em> are generally extremely useful to describe any kind of system that evolves deterministically. Therefore it is natural to try to extend the framework of <em>ordinary differential equations</em> (ODEs) to also include the ability to describe the effects of noise. Historically, this approach to modeling stochastic dynamics has first been formulated heuristically by Langevin to describe <em>Brownian motion</em> <span class="citation" data-cites="1908:Langevin">[<a href="#ref-1908:Langevin" role="doc-biblioref">11</a>]</span>. Later the theory of <em>stochastic differential equations</em> (SDEs) was put on solid mathematical footing by Itô and Stratonovich through the development of <em>stochastic calculus</em> which has been successfully used for applications in physics, biology, economics and others <span class="citation" data-cites="2010:Kunita 1997:Bunkin">[<a href="#ref-2010:Kunita" role="doc-biblioref">12</a>,<a href="#ref-1997:Bunkin" role="doc-biblioref">13</a>]</span> [citation needed]. The solutions to SDEs are not ordinary functions like for ODEs but <em>stochastic processes</em> that describe the probabilities for the system to be in any state for every instant in time. Consequently, a stochastic process contains the probabilities for any possible individual sequence of states in time, i.e. the probabilities for individual <em>trajectories</em>. Since SDEs contain a complete account of noise in the system, information theoretic concepts like the <em>entropy</em> and the <em>mutual information</em>—which we are going to use to understand information transmission in cell signaling—can be applied to stochastic processes. While SDEs can be formulated to describe the evolution of biochemical networks in a discrete state space it is generally more useful to use a less general but simpler <em>chemical master equation</em> for these kinds of problems <span class="citation" data-cites="2009:Gardiner">[<a href="#ref-2009:Gardiner" role="doc-biblioref">14</a>]</span>.</p>
<p>The chemical master equation is a description of a subset of stochastic processes by deriving the <em>time-evolution</em> of the probability distribution over the discrete state space. I.e. instead of describing the stochastic change to an individual state at a given time it focuses on the <em>deterministic</em> evolution of the whole probability distribution over all states. Conveniently, for a given set of chemical reactions that form a reaction network, we can easily find the corresponding chemical master equation that describes the stochastic dynamics of this network given some assumptions of homogeneity. The stochastic process that emerges of this formulation describes the probabilities for the individual counts of all species and how these probabilities change with time. The ease with which the chemical master equation allows the construction of a stochastic process for any kind of biochemical netork makes the idea very attractive to try to use <em>master equations</em> as the basis for information theoretic computations. If we can <em>solve</em> the master equation we in principle have access to all stochastic (and therefore information theoretic) properties of the corresponding biochemical network. E.g. in <span class="citation" data-cites="2010:Tostevin">[<a href="#ref-2010:Tostevin" role="doc-biblioref">15</a>]</span> it is shown how by analytically solving some very simple biochemical networks (using some approximations) it is possible to compute the <em>mutual information</em> between time-varying signals and corresponding responses of these networks.</p>
<p>In most cases however, chemical master equations cannot be solved analytically and thus require averaging over an ensemble of <em>stochastic trajectories</em>. For instance, in Shannon’s information theory the amount of communicated information is not a function of individual signal-response pairs but an averaged quantity that depends on the probability of seeing <em>any</em> random signal-response-pair. Hence the time-efficient generation of stochastic realizations for a given master equation is a central requirement for the exact computation of information processing in chemical networks. A very well known algorithm for the <em>exact</em> generation of trajectories from a given initial condition is the <em>stochastic simulation algorithm</em> (SSA) also known by the name of it’s inventor as the <em>Gillespie algorithm</em> <span class="citation" data-cites="1976:Gillespie">[<a href="#ref-1976:Gillespie" role="doc-biblioref">16</a>]</span>. The most widely used variant is the <em>direct Gillespie method</em> which works by alternatingly a) computing the time during which no reactions happen and b) choosing which of the available reactions to perform next. As a result we generate a list of times where some reaction happens and a corresponding list of reactions that specifies the exact trajectory that was generated. This algorithm works quite well in practice and is also used for the work presented in this thesis. It is still worth mentioning that for systems that evolve at many different time scales simultaneously, the direct Gillespie method can be computationally inefficient since by its design it always operates at the smallest time scale. Therefore there have been developed further trajectory-generation algorithms that can generate <em>approximately</em> correct trajectories by accumulating various reactions into a single time step such as the <span class="math inline">\tau</span>-leap method <span class="citation" data-cites="2001:Gillespie">[<a href="#ref-2001:Gillespie" role="doc-biblioref">17</a>]</span>.</p>
<section id="information-theory-in-the-context-of-cellular-signaling-networks" class="level2" data-number="1.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Information Theory in the context of cellular signaling networks</h2>
<section id="mutual-information-as-an-efficiency-measure-in-cell-signaling" class="level3" data-number="1.1.1">
<h3 data-number="2.1.1"><span class="header-section-number">2.1.1</span> Mutual Information as an Efficiency Measure in Cell signaling</h3>
<figure>
<img src="figures/information_cartoon.svg" id="fig:information_cartoon" alt="" /></img><figcaption>Figure 1: Abstracting cell signaling as an information channel. The channel’s input is an environmental signal that the cell needs to respond to. The signal processing happens through a biochemical network which “computes” a response which is the output of the information channel. The mutual information between signals and responses quantifies the cell’s ability to discern between different signals and choose appropriate responses.</figcaption>
</figure>
<p>In a general sense, cells sense chemical <em>signals</em> from their environment e.g. through receptors on their membrane. These signals provide the cell with important information e.g. about current environmental conditions, their position inside a structure or the location of food. To translate the signal into a useful response (such as expressing a certain gene or changing movement direction) cells have evolved biochemical signaling networks that recognize and process the signals. We use a general description of the cell that is depicted in fig. <a href="#fig:information_cartoon">1</a> where the signal acts as the input of the information channel. The processing of the signal that yields a response is assumed to be a known biochemical network and the response is one species of the biochemical network that acts as the “result” of the computation and represents the reaction of the cell to the signal.</p>
<p>For any given signal there are many stochastically possible responses. Conversely, for any given response there is a range of signals that could have produced it. Both of these statements of uncertainty can be quantified using a single quantity: the <em>mutual information</em>. In information theoretic terms we quantify the <em>uncertainty</em> of a random variable <span class="math inline">\mathcal S</span> by the <em>entropy</em> <span id="eq:signal_entropy"><span class="math display">
\mathrm H(\mathcal S)=-\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s)\,\ln\mathrm P(\mathbf s)
\qquad(1)</span></span> where <span class="math inline">\sigma(\mathcal S)</span> is the set of possible realizations of <span class="math inline">\mathcal S</span> and we use <span class="math inline">\mathrm P(\mathbf s)</span> to denote the probility (density) of <span class="math inline">\mathbf s</span> with respect to the distribution of <span class="math inline">\mathcal S</span>. If <span class="math inline">\mathcal S</span> describes the signal then a large entropy signifies that there is a large range of possible signals that could be expected by the cell. Now given the response <span class="math inline">\mathbf x</span> to a signal <span class="math inline">\mathbf s</span>, we expect <span class="math inline">\mathbf x</span> to contain information about <span class="math inline">\mathbf s</span> such that the uncertainty about the signal is reduced. The conditional entropy <span class="math inline">\mathrm H(\mathcal S|\mathcal X)</span> denotes the <em>average</em> remaining uncertainty of a signal after observing the response, hence it reads <span id="eq:conditional_entropy"><span class="math display">
\mathrm H(\mathcal S|\mathcal X)=-
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x)
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s|\mathbf x)\ln\mathrm P(\mathbf s|\mathbf x) \,.
\qquad(2)</span></span> <span class="math inline">\mathrm P(\mathbf s|\mathbf x)</span> is the conditional distribution of the signals for a given response <span class="math inline">\mathbf x</span> which encodes the transmission characteristics of the communication channel. So we have eq. <a href="#eq:signal_entropy">1</a> which describes the distribution of signals and eq. <a href="#eq:conditional_entropy">2</a> which describes how information is processed. Combining eqns. <a href="#eq:signal_entropy">1</a>, <a href="#eq:conditional_entropy">2</a> we can express the <em>average</em> amount of information gained on the signal by observing the response <span id="eq:mi_form1"><span class="math display">
\mathrm I(\mathcal S,\mathcal X) = \mathrm H(\mathcal S) - \mathrm H(\mathcal S|\mathcal X) = 
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x)
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s|\mathbf x)
\ln \frac{\mathrm P(\mathbf s|\mathbf x)}{\mathrm P(\mathbf s)}
\qquad(3)</span></span> which is precisely the <em>mutual information between <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span></em>. It depends on both, characteristics of the communication channel and the statistics of the input signal. Notably the <span class="math inline">\mathrm I(\mathcal S,\mathcal X)</span> is symmetric under exchange of <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> such that we can express it as <span id="eq:mi_form2"><span class="math display">
\mathrm I(\mathcal S,\mathcal X) = \mathrm H(\mathcal X) - \mathrm H(\mathcal X|\mathcal S) = 
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s)
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x|\mathbf s)
\ln \frac{\mathrm P(\mathbf x|\mathbf s)}{\mathrm P(\mathbf x)}\,,
\qquad(4)</span></span> resulting in a more useful formula for the Monte-Carlo estimation of the MI. Therefore, on average, a response reduces the uncertainty about the signal by exactly the same amount that a signal reduces the uncertainty about the possible responses.</p>

</section>
<section id="information-transmission-for-time-varying-signals" class="level3" data-number="1.1.2">
<h3 data-number="2.1.2"><span class="header-section-number">2.1.2</span> Information Transmission for Time-Varying Signals</h3>
<p>Biochemical networks may store information about the signal in the time-dependency of the response, for example in cellular Ca<sup>2+</sup> signaling information seems to be encoded in the timing and duration of Calcium bursts <span class="citation" data-cites="2010:Tostevin 2008:Boulware 2020:Richards">[<a href="#ref-2010:Tostevin" role="doc-biblioref">15</a>,<a href="#ref-2008:Boulware" role="doc-biblioref">18</a>,<a href="#ref-2020:Richards" role="doc-biblioref">19</a>]</span>. For the case where the signal can be regarded as slowly changing with respect to the response, Cepeda-Humerez, et. al. propose a Monte-Carlo technique for the estimation of the MI that includes information that is stored in the full temporal dynamics of the response <span class="citation" data-cites="2019:Cepeda-Humerez">[<a href="#ref-2019:Cepeda-Humerez" role="doc-biblioref">20</a>]</span>. As described in the article, that method is limited to situations where there can only be finitely many discrete signals.</p>
<p>Often however, biochemical networks not only respond to instantaneous signal levels but also to changes in the signal over time. [reference needed]. Therefore, we build on the technique in <span class="citation" data-cites="2019:Cepeda-Humerez">[<a href="#ref-2019:Cepeda-Humerez" role="doc-biblioref">20</a>]</span> by extending it to allow for time-varying, stochastic signals as well. In this way we aim to find a novel way to compute the MI for time-varying signals <em>and</em> responses for general biochemical networks.</p>
<p>The study of time-varying quantities motivates the use of the <em>information rate</em> which is the asymptotic rate at which the MI between signal and response increases <span class="citation" data-cites="2010:Tostevin">[<a href="#ref-2010:Tostevin" role="doc-biblioref">15</a>]</span> <span><span class="math display">
\mathrm I_R = \lim\limits_{T\rightarrow\infty} \frac{\mathrm I(\mathcal S_T,\mathcal X_T)}{T}
\qquad(5)</span></span> where <span class="math inline">\mathcal S_T</span> and <span class="math inline">\mathcal X_T</span> are random variables over <em>trajectories</em> of length <span class="math inline">T</span>. Since it describes the information gained by the cell in a unit time interval it may be an important quantity for the cell to optimize for.</p>
<p>Hence we find that the fundamental building block for the computation of the information rate at the cellular level is to estimate the mutual information between trajectories of finite length. In the rest of this thesis, we describe methods to compute the mutual information between trajectories <span class="math inline">\mathcal S_T,\mathcal X_T</span> based on a stochastic model of the biochemical signaling network.</p>

</section>
</section>
<section id="stochastic-modeling-of-biochemical-networks" class="level2" data-number="1.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Stochastic Modeling of Biochemical Networks</h2>
<section id="chemical-master-equation" class="level3" data-number="1.2.1">
<h3 data-number="2.2.1"><span class="header-section-number">2.2.1</span> Chemical Master Equation</h3>
<p>As a model for the biochemical processing that takes place inside a cell we suppose that all interactions can be described by a chemical networks composed of different molecular species and reactions between them. Such networks can be described by a <em>chemical master equation</em> which makes it possible to compute all the probabilities associated with the time-evolution of such a system.</p>
<p>For illustrative purposes, let’s consider a highly simplified model of gene expression consisting of two components and four reactions <span><span class="math display"> \begin{gathered}
\emptyset \xrightarrow{\kappa} S \xrightarrow{\lambda} \emptyset\\
S \xrightarrow{\rho} S + X\\
X \xrightarrow{\mu}\emptyset\,.
\end{gathered} \qquad(6)</span></span> Here the chemical species <span class="math inline">S</span> and <span class="math inline">X</span> describe the signal and the response respectively. The constants <span class="math inline">\kappa, \lambda, \rho, \mu</span> determine the rates at which the individual reactions occur. Hence, for every signal particle there is a constant rate <span class="math inline">\rho</span> to be sensed by the cell which triggers the creation of an <span class="math inline">X</span>. Additionally, <span class="math inline">X</span> particles decays by themselves over time with a per-particle rate of <span class="math inline">\mu</span>. Assuming a well stirred system in thermal equilibrium, it can be shown that the probabilities for the individual reactions happening at least once in the time interval <span class="math inline">[t, t+\mathrm\delta t]</span> are</p>
<p><span id="eq:transition_probabilities"><span class="math display">
\begin{aligned}
p^{(\kappa)}_{[t, t+\mathrm\delta t]} = \kappa\delta t + \mathcal{O}(\delta t^2)\\
p^{(\lambda)}_{[t, t+\mathrm\delta t]}(s) = s\lambda\delta t + \mathcal{O}(\delta t^2)\\
p^{(\rho)}_{[t, t+\mathrm\delta t]}(s) = s\rho\delta t + \mathcal{O}(\delta t^2)\\
p^{(\mu)}_{[t, t+\mathrm\delta t]}(x) = x\mu\delta t + \mathcal{O}(\delta t^2)
\end{aligned}
\qquad(7)</span></span></p>
<p>where <span class="math inline">s</span> and <span class="math inline">x</span> denote the particle numbers of the respective species at time <span class="math inline">t</span>. Consequently, the probability for <em>any</em> of the reactions to occur at least once in the time interval <span class="math inline">[t, t+\mathrm\delta t]</span> is</p>
<p><span id="eq:exit_probability"><span class="math display">p_{[t, t+\mathrm\delta t]}(s, x) = (\kappa + s\lambda + s\rho + x\mu)\ \mathrm \delta t + \mathcal{O}(\delta t^2)\qquad(8)</span></span></p>
<p>Using these expressions we can write down the so-called <em>chemical master equation</em> for this network. Let <span class="math inline">\mathrm P_{s,x}(t)</span> be the probability that the system is in state <span class="math inline">(s, x)</span> at time <span class="math inline">t</span>. Assuming that at most one reaction happens in the small time interval <span class="math inline">[t, t+\delta t]</span> we can use the transition probabilities from eqns. <a href="#eq:transition_probabilities">7</a>, <a href="#eq:exit_probability">8</a> to write</p>
<p><span><span class="math display">
\begin{aligned}
\mathrm P_{s,x}(t + \delta t) = \phantom{+}p^{(\kappa)}_{[t, t+\mathrm\delta t]}\ \mathrm P_{s-1,x}(t)\\ +
p^{(\lambda)}_{[t, t+\mathrm\delta t]}(s + 1)\ \mathrm P_{s+1,x}(t)\\ +
p^{(\rho)}_{[t, t+\mathrm\delta t]}(s)\ \mathrm P_{s,x-1}(t)\\ +
p^{(\mu)}_{[t, t+\mathrm\delta t]}(x + 1)\ \mathrm P_{s, x + 1}(t)\\ + 
\left[1 - p_{[t, t+\mathrm\delta t]}(s, x)\right]\ \mathrm P_{s,x}(t)
\end{aligned}
\qquad(9)</span></span></p>
<p>and by taking the limit <span class="math inline">\delta t\rightarrow 0</span> we arrive at the chemical master equation</p>
<p><span id="eq:chemical_master_equation"><span class="math display">
\begin{aligned}
\frac{\partial \mathrm P_{s,x}(t)}{\partial t} = \lim\limits_{\delta t\rightarrow 0} \frac{\mathrm P_{s,x}(t + \delta t) -  \mathrm P_{s,x}(t)}{\delta t}\\
= \kappa\ \mathrm P_{s-1,x}(t) +
(s+1)\lambda\ \mathrm P_{s+1,x}(t) +
s\rho\ \mathrm P_{s,x-1}(t) +
(x+1)\mu\ \mathrm P_{s, x + 1}(t)\\ \phantom{=} - 
(\kappa + s\lambda + s\rho + x\mu)\ \mathrm P_{s,x}(t)
\end{aligned}
\qquad(10)</span></span> In an analogous way a chemical master equation can be derived for any biochemical network <span class="citation" data-cites="2009:Gardiner">[<a href="#ref-2009:Gardiner" role="doc-biblioref">14</a>]</span> and thus forms the basis for our further computations. Stochastic processes that can be described by a master equation are generally called <em>jump processes</em> and provide a useful abstraction for the processes that we want to consider.</p>
</section>
<section id="jump-processes" class="level3" data-number="1.2.2">
<h3 data-number="2.2.2"><span class="header-section-number">2.2.2</span> Jump Processes</h3>
<p>Since particle counts can’t ever become negative, eq. <a href="#eq:chemical_master_equation">10</a> describes a Markov process in continuous time with the state space <span class="math inline">\{(s, x) | s\in\mathbb{N}_0, x\in\mathbb{N}_0\}</span>. In general, every continuous-time Markov process with a discrete state space obeys a master equation. Such processes are also commonly called <em>jump processes</em> since they generate discontinuous sample paths <span class="citation" data-cites="2017:Weber">[<a href="#ref-2017:Weber" role="doc-biblioref">21</a>]</span>.</p>
<p>A jump process with state space <span class="math inline">\mathcal{U}</span> and an initial state <span class="math inline">\mathbf{x}_0\in\mathcal{U}</span> at time <span class="math inline">t_0</span> generates trajectories that can be described by a sequence of pairs <span class="math inline">(\mathbf{x}_i, t_i)_{i=1,2,\ldots}</span> where at every <em>transition time</em> <span class="math inline">t_i</span> there occurs a jump in state space <span class="math inline">\mathbf{x}_{i-1}\rightarrow \mathbf{x}_{i}</span>. As illustrated in eq. <a href="#eq:chemical_master_equation">10</a> the master equation encodes the transition rates for all possible state changes in the system. Similarly, for an arbitrary jump process we can formulate the master equation <span id="eq:general_master_eq"><span class="math display">
\frac{\partial \mathrm P(\mathbf x, t|\mathbf x_0, t_0)}{\partial t} = \sum\limits_{\mathbf x^\prime\in\mathcal U} \left[
w_t(\mathbf x, \mathbf x^\prime)\ \mathrm P(\mathbf x^\prime, t|\mathbf x_0, t_0)
- w_t(\mathbf x^\prime, \mathbf x)\ \mathrm P(\mathbf x, t|\mathbf x_0, t_0)
\right]
\qquad(11)</span></span> where <span class="math inline">w_t(\mathbf x^\prime, \mathbf x)</span> specifies the rate for the transition <span class="math inline">\mathbf x \rightarrow \mathbf x^\prime</span> at time <span class="math inline">t</span> and <span class="math inline">w_t(\mathbf x, \mathbf x) = 0</span>. The first term of the sum in eq. <a href="#eq:general_master_eq">11</a> is known as the <em>gain</em> since it describes how probability flows from other states into the current one while the second term is called <em>loss</em> as it expresses how much probability flows away from the current state. This form of the master equation allows us to find a relatively simple expression for the probability of individual trajectories of jump processes.</p>
<p>By the probability of a trajectory with <span class="math inline">N</span> jumps we denote joint probability density <span class="math inline">\mathrm P(\mathbf x_0, t_0;\ldots;\mathbf x_N,t_N)</span>. We include the initial state <span class="math inline">\mathbf x_0, t_0</span> in the probability since the initial condition itself is usually given by a probability distribution. Using conditional probabilities we can write <span id="eq:trajectory_probability"><span class="math display">
\begin{aligned}
\mathrm P(\mathbf x_0, t_0;\ldots;\mathbf x_N,t_N) =\; \mathrm P(\mathbf x_0,t_0)\,\mathrm P(\mathbf x_1,t_1|\mathbf x_0,t_0)\,\mathrm P(\mathbf x_2,t_2|\mathbf x_1,t_1;\mathbf x_0,t_0)\cdots\\
\mathrm P(\mathbf x_N,t_N|\mathbf x_{N-1},t_{N-1};\ldots;\mathbf x_0,t_0)\,.
\end{aligned}
\qquad(12)</span></span> We can make use of the fact that jump processes are Markov processes to simplify eq. <a href="#eq:trajectory_probability">12</a> such that every conditional probability only explicitly depends on the immediately preceding state <span><span class="math display">
\begin{aligned}
\mathrm P(\mathbf x_0, t_0;\ldots;\mathbf x_N,t_N) = \mathrm P(\mathbf x_0,t_0)\,\mathrm P(\mathbf x_1,t_1|\mathbf x_0,t_0)\,\mathrm P(\mathbf x_2,t_2|\mathbf x_1,t_1)\cdots \mathrm P(\mathbf x_N,t_N|\mathbf x_{N-1},t_{N-1})\\
= \mathrm P(\mathbf x_0,t_0)\,\prod\limits^{N}_{i=1} \mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1})
\,.
\end{aligned}
\qquad(13)</span></span> Hence the expression for the probability of a trajectory contains two distinct kinds of probability distributions, a) the probability of the initial condition <span class="math inline">\mathrm P(\mathbf x_0,t_0)</span> and b) the transition probabilities for every step in the trajectory given by <span class="math inline">\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1})</span>. The initial condition <span class="math inline">\mathrm P(\mathbf x_0,t_0)</span> depends on the specific problem and will often be taken to be the steady-state distribution. For the transition probabilities however, we can find a simple expression in terms of the master equation.</p>
<p>The transition probability <span class="math inline">\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1})</span> describes the probability for a small segment of the trajectory from <span class="math inline">t_{i-1}</span> to <span class="math inline">t_i</span> where first, the system is at state <span class="math inline">\mathbf x_{i-1}</span> for a duration <span class="math inline">t_i - t_{i-1}</span> and then makes a jump <span class="math inline">\mathbf x_{i-1}\rightarrow \mathbf x_{i}</span>. We are now going to derive how to express the probabilities of the constant part as well as the probability of the jump using only terms from the master equation. At the start of the segment, the probability for there to be no jump until at least <span class="math inline">t  t_{i-1}</span> is called the <em>survival probability</em> <span class="math inline">S_{t_{i-1}}(\mathbf x_{i-1}, t)</span>. By noticing that the change in the survival probability is given by the <em>loss</em> term of the master equation <span><span class="math display">
S(\mathbf x_{i-1}, t_{i-1}, t+\delta t) = S(\mathbf x_{i-1}, t_{i-1}, t) \left[ 1-\delta t
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t_{i-1}}(\mathbf x^\prime, \mathbf x_{i-1}) + \mathcal O(\delta t^2) \right]
\qquad(14)</span></span> we can motivate the differential equation <span><span class="math display">
\frac{\partial S(\mathbf x_{i-1}, t_{i-1}, t)}{\partial t} = - S(\mathbf x_{i-1}, t_{i-1}, t) \sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t_{i-1}}(\mathbf x^\prime, \mathbf x_{i-1})
\qquad(15)</span></span> with the only physically sensible solution being <span id="eq:survival_probability"><span class="math display">
S(\mathbf x_{i-1}, t_{i-1}, t) = \exp\left( -\int\limits^{t}_{t_{i-1}}\mathrm dt^\prime 
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t^\prime}(\mathbf x^\prime, \mathbf x_{i-1})
\right)\,.
\qquad(16)</span></span> The jump <span class="math inline">\mathbf x_{i-1}\rightarrow \mathbf x_{i}</span> at time <span class="math inline">t_i</span> itself has a probability <span id="eq:jump_probability"><span class="math display">\mathrm P(\mathbf x_{i-1}\rightarrow \mathbf x_{i}, t_i) = \frac{w_t(\mathbf x_{i}, \mathbf x_{i-1})}{\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t}(\mathbf x^\prime, \mathbf x_{i-1})}\,.\qquad(17)</span></span> Combining the jump probability with the survival probability we can thus express the transition probability by the multiplication <span><span class="math display">
\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1}) = \left. \frac{\partial S(\mathbf x_{i-1}, t_{i-1}, t)}{\partial t} \right|_{t=t_i} \mathrm P(\mathbf x_{i-1}\rightarrow \mathbf x_{i}, t_i)
\qquad(18)</span></span> and by inserting the results from eqns. <a href="#eq:survival_probability">16</a>, <a href="#eq:jump_probability">17</a> we arrive at the expression <span><span class="math display">
\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1}) = w_t(\mathbf x_{i}, \mathbf x_{i-1})
\exp\left( -\int\limits^{t_i}_{t_{i-1}}\mathrm dt^\prime 
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t^\prime}(\mathbf x^\prime, \mathbf x_{i-1})
\right)
\,.
\qquad(19)</span></span></p>
</section>
</section>
<section id="generating-stochastic-trajectories-for-jump-processes" class="level2" data-number="1.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Generating Stochastic Trajectories for Jump Processes</h2>
<section id="direct-gillespie-algorithm" class="level3" data-number="1.3.1">
<h3 data-number="2.3.1"><span class="header-section-number">2.3.1</span> Direct Gillespie Algorithm</h3>
<p>The direct Gillespie algorithm is an efficient and exact procedure to generate stochastic trajectories for jump processes. Starting from an initial state <span class="math inline">\mathbf x_0</span> at time <span class="math inline">t_0</span> it generates a trajectory <span class="math inline">\mathbf x = (\mathbf x_0, t_0;\ldots;\mathbf x_N, t_N)</span> that is a realization of the corresponding stochastic process. Since the trajectories of a jump process are constant segments, separated by discontinuous jumps, at every point in time the system either remains unchanged or it performs an instantaneous jump. Therefore the algorithm works in two phases that are repeated over and over until a trajectory of the desired length is generated.</p>
<p>In the first phase we compute the stochastic waiting time <span class="math inline">\tau</span> until the next <em>event</em> occurs. Given that we are currently at time <span class="math inline">t_{i-1}</span> and state <span class="math inline">\mathbf x_{i-1}</span>, the probability for the waiting time being <span class="math inline">\tau\geq t-t_{i-1}</span> is given by the survival probability from eq. <a href="#eq:survival_probability">16</a>. By the inverse function method we can generate a correctly distributed waiting time <span class="math inline">\tau</span> using a random variate <span class="math inline">u</span> that is uniformly distributed between 0 and 1 and then solving the equation <span id="eq:inverse_function_method"><span class="math display">
u = 1-\exp\left( -\int\limits^{t_{i-1} + \tau}_{t_{i-1}}\mathrm dt^\prime 
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t^\prime}(\mathbf x^\prime, \mathbf x_{i-1}) \right)
\qquad(20)</span></span> for <span class="math inline">\tau</span>. We then set <span class="math inline">t_i = t_{i-1} + \tau</span>. For general transition rates <span class="math inline">w_{t^\prime}</span> eq. <a href="#eq:inverse_function_method">20</a> will have no analytical solution. If however the transition rates are time-independent for a given state we find the simple solution <span><span class="math display">
\tau = -\frac{\ln(1-u)}{\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t_{i-1}}(\mathbf x^\prime, \mathbf x_{i-1})}\,.
\qquad(21)</span></span></p>
<p>After the first phase we already know the time of the next event. Therefore all that remains to do is to decide which reaction should happen. The probability for the individual jumps is given by eq. <a href="#eq:jump_probability">17</a> and thus we can make a weighted choice between all reactions according to their probabilities. Using this choice we have found the next state <span class="math inline">\mathbf x_i</span> and we have finished one step in the stochastic simulation.</p>

</section>
</section>
<section id="monte-carlo-computation-of-the-mutual-information" class="level2" data-number="1.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span> Monte-Carlo computation of the Mutual Information</h2>
<p>Equipped with analytical formulae for the computation of trajectory probabilities and with methods for efficient stochastic simulation, we can start to develop estimates for the mutual information. The basis for our method is eq. <a href="#eq:mi_form2">4</a>; specifically we separate the mutual information into two parts that are computed independently, the <em>marginal entropy</em> <span class="math inline">\mathrm H(\mathcal X)</span> and the <em>conditional entropy</em> <span class="math inline">\mathrm H(\mathcal X|\mathcal S)</span>. Since signals and responses are time-varying, both entropies involve integrals over spaces of trajectories which are high-dimensional. At high dimensionality direct numerical integration is not viable and instead, we use Monte-Carlo approaches based on random sampling of trajectories as explained in the previous section. We can Monte-Carlo estimates for both entropies, however the marginal entropy requires the evaluation of one additional integral for the computation of the marginal probability density <span class="math inline">\mathrm P(\mathbf x)</span>.</p>

<section id="monte-carlo-estimate-for-the-marginal-entropy" class="level3" data-number="1.4.1">
<h3 data-number="2.4.1"><span class="header-section-number">2.4.1</span> Monte-Carlo Estimate for the Marginal Entropy</h3>
<p>We compute the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> using Monte-Carlo (MC) sampling to evaluate the necessary integrals. First we generate a number of samples <span class="math inline">(\mathbf x_i)_{i=1,\ldots,N_x}</span> that are distributed according to the distribution of <span class="math inline">\mathcal X</span>. We use these to estimate the entropy <span id="eq:mc_entropy"><span class="math display">
\mathrm H(\mathcal X) = -\int\mathrm d\mathbf x\ \mathrm P(\mathbf x)\ln \mathrm P(\mathbf x) \approx \frac{\sum\limits_{i=1}^{N_x} \ln\mathrm P(\mathbf x_i)}{N_x} \,.
\qquad(22)</span></span></p>
<p>It is important to realize that we do not actually need to know the distribution of <span class="math inline">\mathcal X</span> to do create appropriate Monte-Carlo samples. Since the stochastic model for a biochemical network provides us with the distributions <span class="math inline">\mathrm P(\mathbf s)</span> and <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> we can generate samples from <span class="math inline">\mathrm P(\mathbf x)</span> by first generating a sample <span class="math inline">\mathbf s_j</span> from <span class="math inline">\mathrm P(\mathbf s)</span> and then use <span class="math inline">P(\mathbf x|\mathbf s_j)</span> to generate a sample <span class="math inline">\mathbf x_i</span>.</p>
<p>Nonetheless we see from eq. <a href="#eq:mc_entropy">22</a> that we <em>do</em> have to evaluate <span class="math inline">\mathrm P(\mathbf x_i)</span> for every generated sample. However, from the dynamics of the biochemical network, the distribution of the responses <span class="math inline">\mathrm P(\mathbf x)</span> is not known a priori. Therefore we choose to evaluate <span class="math inline">\mathrm P(\mathbf x_i)</span> by doing a Monte-Carlo integration using signal samples <span class="math inline">(\mathbf s_j)_{j=1,\ldots,N_s}</span> that are distributed according to <span class="math inline">\mathrm P(\mathcal S)</span>: <span id="eq:mc_marginal"><span class="math display">
\mathrm P(\mathbf x_i) = \int\mathrm d\mathbf s\ \mathrm P(\mathbf s)\ \mathrm P(\mathbf x_i|\mathbf s) \approx \frac{\sum\limits_{j=1}^{N_s} \mathrm P(\mathbf x_i | \mathbf s_j)}{N_s} \,.
\qquad(23)</span></span> While for a low-dimensional signal space it is feasible to instead compute the marginalization integral using direct evaluation <span class="citation" data-cites="2019:Cepeda-Humerez">[<a href="#ref-2019:Cepeda-Humerez" role="doc-biblioref">20</a>]</span> we choose to use MC evaluation to also be able to handle high-dimensional signal spaces. This is crucial since time-varying signals are described by high-dimensional trajectories.</p>
<p>We can summarize the estimation procedure for the marginal entropy using the equation <span id="eq:mc_entropy_notation"><span class="math display">
\mathrm H(\mathcal X) = -\left\langle \ln \left\langle \mathrm P(\mathbf x | \mathbf s) \right\rangle_{\mathrm P(\mathbf s)} \right\rangle_{\mathrm P(\mathbf x)}
\qquad(24)</span></span> where we use the notation <span class="math inline">\langle f(x)\rangle_{g(x)}</span> for the expected value of <span class="math inline">f(x)</span> when <span class="math inline">x</span> is distributed according to the probability density given by <span class="math inline">g(x)</span>. Thus when thinking in mathematical terms we have the shorthand <span class="math inline">\langle f(x)\rangle_{g(x)} \equiv\int \mathrm dx\ g(x) f(x)</span>. We can also easily translate this notation into a Monte-Carlo estimate, i.e. <span class="math inline">\langle f(x)\rangle_{g(x)} = \lim\limits_{N\rightarrow\infty}\frac{\sum_{i=1}^N f(x_i)}{N}</span> where <span class="math inline">x_1, x_2,\ldots</span> are independent samples of the probability distribution given by <span class="math inline">g(x)</span>.</p>
</section>
<section id="estimating-the-conditional-entropy" class="level3" data-number="1.4.2">
<h3 data-number="2.4.2"><span class="header-section-number">2.4.2</span> Estimating the Conditional Entropy</h3>
<p>We can also estimate the <em>conditional entropy</em> using MC averages over trajectories. We express the conditional entropy using the notation introduced above <span><span class="math display">
\mathrm H(\mathcal X|\mathcal S) = -\iint \mathrm d\mathbf s\mathrm d\mathbf x\ \mathrm P(\mathbf s)\mathrm P(\mathbf x | \mathbf s) \ln\mathrm P(\mathbf x|\mathbf s) = -\left\langle\langle\ln\mathrm P(\mathbf x | \mathbf s)\rangle_{\mathrm P(\mathbf x | \mathbf s)} \right\rangle_{\mathrm P(\mathbf s)}
\qquad(25)</span></span> to show that we require nested Monte Carlo integrations to evaluate the integral. We first generate signal samples <span class="math inline">\mathbf s_1, \ldots, \mathbf s_{N_s}</span> from the density <span class="math inline">\mathrm P(\mathbf s)</span>. Let <span class="math inline">\mathbf x_i^1,\ldots,\mathbf x_i^{N_x}</span> be response samples generated from <span class="math inline">\mathrm P(\mathbf x | \mathbf s_i)</span>. The Monte Carlo estimate for the conditional entropy then reads <span id="eq:conditional_entropy_estimate"><span class="math display">
\mathrm H(\mathcal X|\mathcal S) \approx - \frac1{N_s N_x} \sum\limits_{i=1}^{N_s} \sum\limits_{j=1}^{N_x} \ln\mathrm P(\mathbf x_i^j | \mathbf s_i)\,.
\qquad(26)</span></span></p>
</section>
</section>
<section id="references" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="1">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-2002:Elowitz">
<p>[1] M.B. Elowitz, A.J. Levine, E.D. Siggia, P.S. Swain, Stochastic Gene Expression in a Single Cell, Science. 297 (2002) 1183–1186. <a href="https://doi.org/10.1126/science.1070919">https://doi.org/10.1126/science.1070919</a>.</p>
</div>
<div id="ref-2008:Faisal">
<p>[2] A.A. Faisal, L.P.J. Selen, D.M. Wolpert, Noise in the nervous system, Nature Reviews Neuroscience. 9 (2008) 292–303. <a href="https://doi.org/10.1038/nrn2258">https://doi.org/10.1038/nrn2258</a>.</p>
</div>
<div id="ref-1990:Parsons">
<p>[3] P.A. Parsons, Fluctuating Asymmetry: An Epigenetic Measure of Stress, Biological Reviews. 65 (1990) 131–145. <a href="https://doi.org/10.1111/j.1469-185x.1990.tb01186.x">https://doi.org/10.1111/j.1469-185x.1990.tb01186.x</a>.</p>
</div>
<div id="ref-2011:Hallatschek">
<p>[4] O. Hallatschek, Noise Driven Evolutionary Waves, PLoS Computational Biology. 7 (2011) e1002005. <a href="https://doi.org/10.1371/journal.pcbi.1002005">https://doi.org/10.1371/journal.pcbi.1002005</a>.</p>
</div>
<div id="ref-2014:Tsimring">
<p>[5] L.S. Tsimring, Noise in biology, Reports on Progress in Physics. 77 (2014) 026601. <a href="https://doi.org/10.1088/0034-4885/77/2/026601">https://doi.org/10.1088/0034-4885/77/2/026601</a>.</p>
</div>
<div id="ref-2020:Leifer">
<p>[6] I. Leifer, F. Morone, S.D.S. Reis, J.S. Andrade, M. Sigman, H.A. Makse, Circuits with broken fibration symmetries perform core logic computations in biological networks, PLOS Computational Biology. 16 (2020) e1007776. <a href="https://doi.org/10.1371/journal.pcbi.1007776">https://doi.org/10.1371/journal.pcbi.1007776</a>.</p>
</div>
<div id="ref-1948:Shannon">
<p>[7] C.E. Shannon, A Mathematical Theory of Communication, Bell System Technical Journal. 27 (1948) 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.</p>
</div>
<div id="ref-2009:Tkačik">
<p>[8] G. Tkačik, A.M. Walczak, W. Bialek, Optimizing information flow in small genetic networks, Physical Review E. 80 (2009) 031920. <a href="https://doi.org/10.1103/physreve.80.031920">https://doi.org/10.1103/physreve.80.031920</a>.</p>
</div>
<div id="ref-2020:Uda">
<p>[9] S. Uda, Application of information theory in systems biology., Biophysical Reviews. 12 (2020) 377–384. <a href="https://doi.org/10.1007/s12551-020-00665-w">https://doi.org/10.1007/s12551-020-00665-w</a>.</p>
</div>
<div id="ref-2006:Cover">
<p>[10] T.M. Cover, J.A. Thomas, Elements of Information Theory, 2nd ed., John Wiley  Sons, 2006.</p>
</div>
<div id="ref-1908:Langevin">
<p>[11] P. Langevin, Sur la théorie du mouvement brownien, C. R. Acad. Sci. (1908) 530–533.</p>
</div>
<div id="ref-2010:Kunita">
<p>[12] H. Kunita, Itô’s stochastic calculus: Its surprising power for applications, Stochastic Processes and Their Applications. 120 (2010) 622–652. <a href="https://doi.org/10.1016/j.spa.2010.01.013">https://doi.org/10.1016/j.spa.2010.01.013</a>.</p>
</div>
<div id="ref-1997:Bunkin">
<p>[13] F.V. Bunkin, B.B. Kadomtsev, Y.L. Klimontovich, N.I. Koroteev, P.S. Landa, V.P. Maslov, Y.M. Romanovskii, In memory of Ruslan Leont’evich Stratonovich, Physics-Uspekhi. 40 (1997) 751–752. <a href="https://doi.org/10.1070/pu1997v040n07abeh000259">https://doi.org/10.1070/pu1997v040n07abeh000259</a>.</p>
</div>
<div id="ref-2009:Gardiner">
<p>[14] C. Gardiner, Stochastic Methods, 4th ed., Springer-Verlag, Berlin Heidelberg, 2009.</p>
</div>
<div id="ref-2010:Tostevin">
<p>[15] F. Tostevin, P.R. ten Wolde, Mutual information in time-varying biochemical systems, Physical Review E. 81 (2010) 061917. <a href="https://doi.org/10.1103/physreve.81.061917">https://doi.org/10.1103/physreve.81.061917</a>.</p>
</div>
<div id="ref-1976:Gillespie">
<p>[16] D.T. Gillespie, A general method for numerically simulating the stochastic time evolution of coupled chemical reactions, Journal of Computational Physics. 22 (1976) 403–434. <a href="https://doi.org/10.1016/0021-9991(76)90041-3">https://doi.org/10.1016/0021-9991(76)90041-3</a>.</p>
</div>
<div id="ref-2001:Gillespie">
<p>[17] D.T. Gillespie, Approximate accelerated stochastic simulation of chemically reacting systems, The Journal of Chemical Physics. 115 (2001) 1716–1733. <a href="https://doi.org/10.1063/1.1378322">https://doi.org/10.1063/1.1378322</a>.</p>
</div>
<div id="ref-2008:Boulware">
<p>[18] M.J. Boulware, J.S. Marchant, Timing in Cellular Ca2+ Signaling, Current Biology. 18 (2008) R769–R776. <a href="https://doi.org/10.1016/j.cub.2008.07.018">https://doi.org/10.1016/j.cub.2008.07.018</a>.</p>
</div>
<div id="ref-2020:Richards">
<p>[19] D.M. Richards, J.J. Walker, J. Tabak, Ion channel noise shapes the electrical activity of endocrine cells, PLOS Computational Biology. 16 (2020) e1007769. <a href="https://doi.org/10.1371/journal.pcbi.1007769">https://doi.org/10.1371/journal.pcbi.1007769</a>.</p>
</div>
<div id="ref-2019:Cepeda-Humerez">
<p>[20] S.A. Cepeda-Humerez, J. Ruess, G. Tkačik, Estimating information in time-varying signals., PLoS Computational Biology. 15 (2019) e1007290. <a href="https://doi.org/10.1371/journal.pcbi.1007290">https://doi.org/10.1371/journal.pcbi.1007290</a>.</p>
</div>
<div id="ref-2017:Weber">
<p>[21] M.F. Weber, E. Frey, Master equations and the theory of stochastic path integrals, Reports on Progress in Physics. 80 (2017) 046601. <a href="https://doi.org/10.1088/1361-6633/aa5ae2">https://doi.org/10.1088/1361-6633/aa5ae2</a>.</p>
</div>
</div>
</section>
</section>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="monte-carlo-in-trajectory-space.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="monte-carlo-in-trajectory-space.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        

        

        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>