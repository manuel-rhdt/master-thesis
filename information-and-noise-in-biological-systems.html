<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js Light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mutual Information between Trajectories</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
        <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var regex = /\\qquad\W*\(([0-9]+)\)/;
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    var tex_str = texText.data.replace(regex, "\\tag{$1}");
                    katex.render(tex_str, mathElements[i], {
                    displayMode: mathElements[i].classList.contains('display'),
                    throwOnError: false,
                    fleqn: false
                });
            }}});
        </script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "" : "Light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('Light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded"><a href="index.html" class=""><span class="header-section-number">1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html" class="active"><span class="header-section-number">2</span> Information and Noise in Biological Systems
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-theory-in-the-context-of-cellular-signalling-networks" class=""><span class="header-section-number">2.1</span> Information Theory in the context of cellular signalling networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#mutual-information-as-an-efficiency-measure-in-cell-signalling" class=""><span class="header-section-number">2.1.1</span> Mutual Information as an Efficiency Measure in Cell Signalling
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-transmission-for-time-varying-signals" class=""><span class="header-section-number">2.1.2</span> Information Transmission for Time-Varying Signals
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#stochastic-modeling-of-biochemical-networks" class=""><span class="header-section-number">2.2</span> Stochastic Modeling of Biochemical Networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#chemical-master-equation" class=""><span class="header-section-number">2.2.1</span> Chemical Master Equation
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#jump-processes" class=""><span class="header-section-number">2.2.2</span> Jump Processes
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#generating-stochastic-trajectories" class=""><span class="header-section-number">2.3</span> Generating Stochastic Trajectories
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#direct-gillespie-algorithm" class=""><span class="header-section-number">2.3.1</span> Direct Gillespie Algorithm
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#simulating-a-biochemical-network-driven-by-an-external-signal" class=""><span class="header-section-number">2.3.2</span> Simulating a Biochemical Network Driven by an External Signal
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#monte-carlo-computation-of-the-mutual-information" class=""><span class="header-section-number">2.4</span> Monte-Carlo computation of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#computing-the-likelihood" class=""><span class="header-section-number">2.4.1</span> Computing the likelihood
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#the-probability-density-for-the-starting-point-of-a-trajectory" class=""><span class="header-section-number">2.4.2</span> The probability density for the starting point of a trajectory
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#estimating-the-marginal-probability-of-response-trajectories" class=""><span class="header-section-number">2.4.3</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html" class=""><span class="header-section-number">3</span> Monte-Carlo in Trajectory space
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-in-trajectory-space.html#simulating-a-biochemical-network-driven-by-an-external-signal" class=""><span class="header-section-number">3.1</span> Simulating a Biochemical Network Driven by an External Signal
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#monte-carlo-computation-of-the-mutual-information" class=""><span class="header-section-number">3.2</span> Monte-Carlo computation of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-in-trajectory-space.html#computing-the-likelihood" class=""><span class="header-section-number">3.2.1</span> Computing the likelihood
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#the-probability-density-for-the-starting-point-of-a-trajectory" class=""><span class="header-section-number">3.2.2</span> The probability density for the starting point of a trajectory
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#estimating-the-marginal-probability-of-response-trajectories" class=""><span class="header-section-number">3.2.3</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-in-trajectory-space.html#references" class=""><span class="header-section-number">3.3</span> References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html" class=""><span class="header-section-number">4</span> Mutual Information for Trajectories in a Gaussian Framework
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#introduction" class=""><span class="header-section-number">4.1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">4.2</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#choice-of-covariance-matrices" class=""><span class="header-section-number">4.2.1</span> Choice of Covariance Matrices
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#direct-importance-sampling" class=""><span class="header-section-number">4.2.2</span> Direct Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#umbrella-sampling" class=""><span class="header-section-number">4.2.3</span> Umbrella Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">4.3</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#discussion" class=""><span class="header-section-number">4.4</span> Discussion
                    </a>
                    </li><li class="expanded"><a href="mutual-information-for-trajectories-in-a-gaussian-framework.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html" class=""><span class="header-section-number">5</span> Estimation Strategies Inspired by Statistical Physics
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#borrowing-terminology-from-statistical-physics" class=""><span class="header-section-number">5.1</span> Borrowing Terminology from Statistical Physics
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#thermodynamic-integration" class=""><span class="header-section-number">5.2</span> Thermodynamic Integration
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#summary-of-ti" class=""><span class="header-section-number">5.2.1</span> Summary of TI
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#markov-chain-monte-carlo" class=""><span class="header-section-number">5.2.2</span> Markov Chain Monte Carlo
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#estimating-the-density-of-states" class=""><span class="header-section-number">5.3</span> Estimating the Density of States
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#wang-and-landau-algorithm" class=""><span class="header-section-number">5.4</span> Wang and Landau Algorithm
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#applying-wang-landau-to-the-computation-of-the-marginal-density" class=""><span class="header-section-number">5.4.1</span> Applying Wang-Landau to the Computation of the Marginal Density
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#modified-dos" class=""><span class="header-section-number">5.4.1.1</span> Modified DOS
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#modified-wang-landau-algorithm" class=""><span class="header-section-number">5.4.1.2</span> Modified Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#comparison-to-usual-wang-landau-algorithm" class=""><span class="header-section-number">5.4.1.3</span> Comparison to usual Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#connection-to-standard-monte-carlo-sampling" class=""><span class="header-section-number">5.4.1.4</span> Connection to Standard Monte-Carlo Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#example-results-for-a-wang-landau-simulation" class=""><span class="header-section-number">5.4.2</span> Example Results for a Wang-Landau Simulation
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#conclusion" class=""><span class="header-section-number">5.5</span> Conclusion
                    </a>
                    </li><li class="expanded"><a href="estimation-strategies-inspired-by-statistical-physics.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                    </div>

                    <h1 class="menu-title">Mutual Information between Trajectories</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/manuel-rhdt/master-thesis" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <section id="information-and-noise-in-biological-systems" class="level1" data-number="1">
<h1 data-number="2"><span class="header-section-number">2</span> Information and Noise in Biological Systems</h1>
<p><em>Noise</em> is inherent across diverse biological systems and remains relevant at all biological scales. From <em>stochastic gene expression</em> and random <em>action potential spikes</em> in neuronal networks at the cellular scale to the <em>development of multicellular organisms</em> all the way to the <em>variations in population level</em> of competing species in whole ecosystems, we find examples of processes which can only be described precisely by taking into account noise as an intrinsic feature <span class="citation" data-cites="2002:Elowitz 2008:Faisal 1990:Parsons 2011:Hallatschek 2014:Tsimring">[<a href="#ref-2002:Elowitz" role="doc-biblioref">1</a>–<a href="#ref-2014:Tsimring" role="doc-biblioref">5</a>]</span>. In this thesis we focus on the smaller end of this scale, namely on the stochastic description of <em>biochemical networks</em>. These comprise among others <em>gene expression</em>, <em>gene regulatory networks</em>, and <em>cell signalling</em> networks, all of which exhibit noise due to small copy numbers of participating components. Additionally, since biological processes often happen out of equilibrium even macroscopic quantities can exhibit large fluctuations. The main source of noise at the cellular level may be fluctuations in <em>gene expression</em> which propagate to higher levels of biological organization <span class="citation" data-cites="2014:Tsimring">[<a href="#ref-2014:Tsimring" role="doc-biblioref">5</a>]</span>. The abundance of noise in all these systems invites the questions of how cells can reliably make correct decisions, even in complex and changing environments and how cells are able to <em>encode</em> the information about their environment using biochemical networks.</p>
<p>To successfully function, cells must generally correctly react to environmental changes. This requires processing the environmental cues they receive through their receptors and thereby filtering out the useful information from the noisy signal. The processing of information through biochemical networks can be quite elaborate with individual network components being able to perform analogous functions as silicon computational devices such as transistors <strong>???</strong>. It is thus tempting to think that optimization of information processing drives the evolution of cellular signaling networks. To understand information processing from the cell’s point of view we employ the very general framework of <em>Information Theory</em> which was originally developed to address questions of the reliability of telecommunications <span class="citation" data-cites="1948:Shannon">[<a href="#ref-1948:Shannon" role="doc-biblioref">6</a>]</span>. Information theory has been successfully used in biology to study cellular communication, embryonic development and other biological systems <span class="citation" data-cites="2009:Tkačik 2020:Uda">[<a href="#ref-2009:Tkačik" role="doc-biblioref">7</a>,<a href="#ref-2020:Uda" role="doc-biblioref">8</a>]</span> [citation needed]. Notably, there are many parallels between biological signal processing and <em>noisy channels</em> which are used for example to describe information transmission across a telephone line.</p>
<p>A crucial feature of <em>Information Theory</em> is that its results are broadly applicable, irrespective of the nature of the communication channel or the medium used to transmit a signal. The communication channel merely describes any kind of abstract device that processes an <em>input</em> in a probabilistic way to yield a corresponding <em>output</em>. It turns out that the study of information transmission through such a channel is closely related to the study of noise since the amount of noise introduced by a communication channel sets an upper bound to the amount of data that can be transmitted through it, known as the <em>channel capacity</em> <span class="citation" data-cites="2006:Cover">[<a href="#ref-2006:Cover" role="doc-biblioref">9</a>]</span>. Consequently, the output can be described as a deterministic, lossless transformation of the input <em>plus</em> some random noise from the channel which leads to a loss of information. Note that in biological systems the signal itself is typically a fluctuating quantity such that the noise in the output is a combination of the channel noise and the signal noise. Since in cell signalling both, input and output are time-varying quantities we require a description of our system that allows for deterministic <em>and</em> stochastic time evolution.</p>
<p><em>Differential equations</em> are generally extremely useful to describe any kind of system that evolves deterministically. Therefore it is natural to try to extend the framework of <em>ordinary differential equations</em> (ODEs) to also include the ability to describe the effects of noise. Historically, this approach to modeling stochastic dynamics has first been formulated heuristically by Langevin to describe <em>Brownian motion</em> <span class="citation" data-cites="1908:Langevin">[<a href="#ref-1908:Langevin" role="doc-biblioref">10</a>]</span>. Later the theory of <em>stochastic differential equations</em> (SDEs) was put on solid mathematical footing by Itô and Stratonovich through the development of <em>stochastic calculus</em> which has been successfully used for applications in physics, biology, economics and others <span class="citation" data-cites="2010:Kunita 1997:Bunkin">[<a href="#ref-2010:Kunita" role="doc-biblioref">11</a>,<a href="#ref-1997:Bunkin" role="doc-biblioref">12</a>]</span> [citation needed]. The solutions to SDEs are not ordinary functions like for ODEs but <em>stochastic processes</em> that describe the probabilities for the system to be in any state for every instant in time. Consequently, a stochastic process contains the probabilities for any possible individual sequence of states in time, i.e. the probabilities for individual <em>trajectories</em>. Since SDEs contain a complete account of noise in the system, information theoretic concepts like the <em>entropy</em> and the <em>mutual information</em>—which we are going to use to understand information transmission in cell signalling—can be applied to stochastic processes. While SDEs can be formulated to describe the evolution of biochemical networks in a discrete state space it is generally more useful to use a less general but simpler <em>chemical master equation</em> for these kinds of problems <span class="citation" data-cites="2009:Gardiner">[<a href="#ref-2009:Gardiner" role="doc-biblioref">13</a>]</span>.</p>
<p>The chemical master equation is a description of a subset of stochastic processes by deriving the <em>time-evolution</em> of the probability distribution over the discrete state space. I.e. instead of describing the stochastic change to an individual state at a given time it focuses on the <em>deterministic</em> evolution of the whole probability distribution over all states. Conveniently, for a given set of chemical reactions that form a reaction network, we can easily find the corresponding chemical master equation that describes the stochastic dynamics of this network given some assumptions of homogeneity. The stochastic process that emerges of this formulation describes the probabilities for the individual counts of all species and how these probabilities change with time. The ease with which the chemical master equation allows the construction of a stochastic process for any kind of biochemical netork makes it very attractive to try to use <em>master equations</em> as the basis for information theoretic computations. If we can <em>solve</em> the master equation we in principle have access to all stochastic (and therefore information theoretic) properties of the corresponding biochemical network. E.g. in <span class="citation" data-cites="2010:Tostevin">[<a href="#ref-2010:Tostevin" role="doc-biblioref">14</a>]</span> it is shown how by analytically solving some very simple biochemical networks (using some approximations) it is possible to compute the <em>mutual information</em> between time-varying signals and corresponding responses of these networks.</p>
<p>In most cases however, chemical master equations cannot be solved analytically and thus require averaging over an ensemble of <em>stochastic trajectories</em>. For instance, in Shannon’s information theory the amount of communicated information is not a function of individual signal-response pairs but an averaged quantity that depends on the probability of seeing <em>any</em> random signal-response-pair. Hence the time-efficient generation of stochastic realizations for a given master equation is a central requirement for the exact computation of information processing in chemical networks. A very well known algorithm for the <em>exact</em> generation of trajectories from a given initial condition is the <em>stochastic simulation algorithm</em> (SSA) also known by the name of it’s inventor as the <em>Gillespie algorithm</em> <span class="citation" data-cites="1976:Gillespie">[<a href="#ref-1976:Gillespie" role="doc-biblioref">15</a>]</span>. The most widely used variant is the <em>direct Gillespie method</em> which works by alternatingly a) computing the time during which no reactions happen and b) choosing which of the available reactions to perform next. As a result we generate a list of times where some reaction happens and a corresponding list of reactions that specifies the exact trajectory that was generated. This algorithm works quite well in practice and is also used for the work presented in this thesis. It is still worth mentioning that for systems that evolve at many different time scales simultaneously, the direct Gillespie method can be computationally inefficient since by its design it always operates at the smallest time scale. Therefore there have been developed further trajectory-generation algorithms that can generate <em>approximately</em> correct trajectories by accumulating various reactions into a single time step such as the <span class="math inline">\tau</span>-leap method <span class="citation" data-cites="2001:Gillespie">[<a href="#ref-2001:Gillespie" role="doc-biblioref">16</a>]</span>.</p>
<section id="information-theory-in-the-context-of-cellular-signalling-networks" class="level2" data-number="1.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Information Theory in the context of cellular signalling networks</h2>
<section id="mutual-information-as-an-efficiency-measure-in-cell-signalling" class="level3" data-number="1.1.1">
<h3 data-number="2.1.1"><span class="header-section-number">2.1.1</span> Mutual Information as an Efficiency Measure in Cell Signalling</h3>
<figure>
<img src="figures/information_cartoon.svg" id="fig:information_cartoon" alt="" /></img><figcaption>Figure 1: Abstracting cell signalling as an information channel. The channel’s input is an environmental signal that the cell needs to respond to. The signal processing happens through a biochemical network which “computes” a response which is the output of the information channel. The mutual information between signals and responses quantifies the cell’s ability to discern between different signals and choose appropriate responses.</figcaption>
</figure>
<p>In a general sense, cells sense chemical <em>signals</em> from their environment e.g. through receptors on their membrane. These signals provide the cell with important information e.g. about current environmental conditions, their position inside a structure or the location of food. To translate the signal into a useful response (such as expressing a certain gene or changing movement direction) cells have evolved biochemical signalling networks that recognize and process the signals. We use a general description of the cell that is depicted in fig. <a href="#fig:information_cartoon">1</a> where the signal acts as the input of the information channel. The processing of the signal that yields a response is assumed to be a known biochemical network and the response is one species of the biochemical network that acts as the “result” of the computation and represents the reaction of the cell to the signal.</p>
<p>For any given signal there are many stochastically possible responses. Conversely, for any given response there is a range of signals that could have produced it. Both of these statements of uncertainty can be quantified using a single quantity: the <em>mutual information</em>. In information theoretic terms we quantify the <em>uncertainty</em> of a random variable <span class="math inline">\mathcal S</span> by the <em>entropy</em> <span id="eq:signal_entropy"><span class="math display">
\mathrm H(\mathcal S)=-\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s)\,\ln\mathrm P(\mathbf s)
\qquad(1)</span></span> where <span class="math inline">\sigma(\mathcal S)</span> is the set of possible realizations of <span class="math inline">\mathcal S</span> and we use <span class="math inline">\mathrm P(\mathbf s)</span> to denote the probility (density) of <span class="math inline">\mathbf s</span> with respect to the distribution of <span class="math inline">\mathcal S</span>. If <span class="math inline">\mathcal S</span> describes the signal then a large entropy signifies that there is a large range of possible signals that could be expected by the cell. Now given the response <span class="math inline">\mathbf x</span> to a signal <span class="math inline">\mathbf s</span>, we expect <span class="math inline">\mathbf x</span> to contain information about <span class="math inline">\mathbf s</span> such that the uncertainty about the signal is reduced. The conditional entropy <span class="math inline">\mathrm H(\mathcal S|\mathcal X)</span> captures the <em>average</em> remaining uncertainty of a signal after observing the response, hence it reads <span id="eq:conditional_entropy"><span class="math display">
\mathrm H(\mathcal S|\mathcal X)=-
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x)
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s|\mathbf x)\ln\mathrm P(\mathbf s|\mathbf x) \,.
\qquad(2)</span></span> <span class="math inline">\mathrm P(\mathbf s|\mathbf x)</span> is the conditional distribution of the signals for a given response <span class="math inline">\mathbf x</span> which encodes the transmission characteristics of the communication channel. Combining eqns. <a href="#eq:signal_entropy">1</a>, <a href="#eq:conditional_entropy">2</a> we can express the <em>average</em> amount of information gained on the signal by observing the response <span id="eq:mi_form1"><span class="math display">
\mathrm I(\mathcal S,\mathcal X) = \mathrm H(\mathcal S) - \mathrm H(\mathcal S|\mathcal X) = 
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x)
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s|\mathbf x)
\ln \frac{\mathrm P(\mathbf s|\mathbf x)}{\mathrm P(\mathbf s)}
\qquad(3)</span></span> which is precisely the <em>mutual information between <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span></em>. Eq. <a href="#eq:mi_form1">3</a> shows that the mutual information (MI) is not only dependend on the characteristics of the communication channel but also on the statistics of the input signal. Notably the <span class="math inline">\mathrm I(\mathcal S,\mathcal X)</span> is symmetric under exchange of <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> such that we can express it as <span><span class="math display">
\mathrm I(\mathcal S,\mathcal X) = \mathrm H(\mathcal X) - \mathrm H(\mathcal X|\mathcal S) = 
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s)
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x|\mathbf s)
\ln \frac{\mathrm P(\mathbf x|\mathbf s)}{\mathrm P(\mathbf x)}\,,
\qquad(4)</span></span> resulting in a more useful formula for the Monte-Carlo estimation of the MI.</p>
<p>The mutual information has been used previously to understand TODO elaborate</p>
</section>
<section id="information-transmission-for-time-varying-signals" class="level3" data-number="1.1.2">
<h3 data-number="2.1.2"><span class="header-section-number">2.1.2</span> Information Transmission for Time-Varying Signals</h3>
<p>Biochemical networks may store information about the signal in the time-dependency of the response, for example in cellular Ca<sup>2+</sup> signaling information seems to be encoded in the timing and duration of Calcium bursts <span class="citation" data-cites="2010:Tostevin 2008:Boulware 2020:Richards">[<span class="citeproc-not-found" data-reference-id="2008:Boulware"><strong>???</strong></span>,<a href="#ref-2010:Tostevin" role="doc-biblioref">14</a>,<a href="#ref-2020:Richards" role="doc-biblioref">17</a>]</span>. For the case where the signal can be regarded as slowly changing with respect to the response Cepeda-Humerez, et. al. propose a Monte-Carlo technique for the estimation of the MI that includes information that is stored in the full temporal dynamics of the response <span class="citation" data-cites="2019:Cepeda-Humerez">[<a href="#ref-2019:Cepeda-Humerez" role="doc-biblioref">18</a>]</span>. As described in the article, that method is limited to situations where there can only be finitely many discrete signals.</p>
<p>Often however, biochemical networks not only respond to instantaneous signal levels but also to changes in the signal over time. [reference needed]. Therefore, we build on the technique in <span class="citation" data-cites="2019:Cepeda-Humerez">[<a href="#ref-2019:Cepeda-Humerez" role="doc-biblioref">18</a>]</span> by extending it to allow for time-varying signals as well. In this way we aim to find a novel way to compute the MI for time-varying signals <em>and</em> responses for general biochemical networks.</p>
<p>The study of time-varying quantities motivates the use of the <em>information rate</em> which is the asymptotic rate at which the MI between signal and response increases <span class="citation" data-cites="2010:Tostevin">[<a href="#ref-2010:Tostevin" role="doc-biblioref">14</a>]</span> <span><span class="math display">
\mathrm I_R = \lim\limits_{T\rightarrow\infty} \frac{\mathrm I(\mathcal S_T,\mathcal X_T)}{T}
\qquad(5)</span></span> where <span class="math inline">\mathcal S_T</span> and <span class="math inline">\mathcal X_T</span> are random variables over <em>trajectories</em> of length <span class="math inline">T</span>. Since it describes the information gained by the cell in a unit time interval it may be an important quantity for the cell to optimize for.</p>

</section>
</section>
<section id="stochastic-modeling-of-biochemical-networks" class="level2" data-number="1.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Stochastic Modeling of Biochemical Networks</h2>
<section id="chemical-master-equation" class="level3" data-number="1.2.1">
<h3 data-number="2.2.1"><span class="header-section-number">2.2.1</span> Chemical Master Equation</h3>
<p>As a model for the biochemical processing that takes place inside a cell we suppose that all interactions can be described by a chemical networks composed of different molecular species and reactions between them. Such networks can be described by a <em>chemical master equation</em> which makes it possible to compute all the probabilities associated with the time-evolution of such a system.</p>
<p>For illustrative purposes, let’s consider a highly simplified model of gene expression consisting of two components and four reactions</p>
<p><span><span class="math display"> \begin{gathered}
\emptyset \xrightarrow{\kappa} S \xrightarrow{\lambda} \emptyset\\
S \xrightarrow{\rho} S + X\\
X \xrightarrow{\mu}\emptyset
\end{gathered} \qquad(6)</span></span></p>
<p>The constants <span class="math inline">\kappa, \lambda, \rho, \mu</span> determine the rates at which the individual reactions occur. For example, assuming a well stirred system in thermal equilibrium, it can be shown that the probabilities for the individual reactions happening at least once in the time interval <span class="math inline">[t, t+\mathrm\delta t]</span> are</p>
<p><span id="eq:transition_probabilities"><span class="math display">
\begin{aligned}
p^{(\kappa)}_{[t, t+\mathrm\delta t]} = \kappa\delta t + \mathcal{O}(\delta t^2)\\
p^{(\lambda)}_{[t, t+\mathrm\delta t]}(s) = s\lambda\delta t + \mathcal{O}(\delta t^2)\\
p^{(\rho)}_{[t, t+\mathrm\delta t]}(s) = s\rho\delta t + \mathcal{O}(\delta t^2)\\
p^{(\mu)}_{[t, t+\mathrm\delta t]}(x) = x\mu\delta t + \mathcal{O}(\delta t^2)
\end{aligned}
\qquad(7)</span></span></p>
<p>where <span class="math inline">s</span> and <span class="math inline">x</span> denote the particle numbers of the respective species at time <span class="math inline">t</span>. Consequently, the probability for <em>any</em> of the reactions to occur at least once in the time interval <span class="math inline">[t, t+\mathrm\delta t]</span> is</p>
<p><span id="eq:exit_probability"><span class="math display">p_{[t, t+\mathrm\delta t]}(s, x) = (\kappa + s\lambda + s\rho + x\mu)\ \mathrm \delta t + \mathcal{O}(\delta t^2)\qquad(8)</span></span></p>
<p>Using these expressions we can write down the so-called <em>chemical master equation</em> for this network. Let <span class="math inline">\mathrm P_{s,x}(t)</span> be the probability that the system is in state <span class="math inline">(s, x)</span> at time <span class="math inline">t</span>. Assuming that at most one reaction happens in the small time interval <span class="math inline">[t, t+\delta t]</span> we can use the transition probabilities from eqns. <a href="#eq:transition_probabilities">7</a>, <a href="#eq:exit_probability">8</a> to write</p>
<p><span><span class="math display">
\begin{aligned}
\mathrm P_{s,x}(t + \delta t) = \phantom{+}p^{(\kappa)}_{[t, t+\mathrm\delta t]}\ \mathrm P_{s-1,x}(t)\\ +
p^{(\lambda)}_{[t, t+\mathrm\delta t]}(s + 1)\ \mathrm P_{s+1,x}(t)\\ +
p^{(\rho)}_{[t, t+\mathrm\delta t]}(s)\ \mathrm P_{s,x-1}(t)\\ +
p^{(\mu)}_{[t, t+\mathrm\delta t]}(x + 1)\ \mathrm P_{s, x + 1}(t)\\ + 
\left[1 - p_{[t, t+\mathrm\delta t]}(s, x)\right]\ \mathrm P_{s,x}(t)
\end{aligned}
\qquad(9)</span></span></p>
<p>and by taking the limit <span class="math inline">\delta t\rightarrow 0</span> we arrive at the chemical master equation</p>
<p><span id="eq:chemical_master_equation"><span class="math display">
\begin{aligned}
\frac{\partial \mathrm P_{s,x}(t)}{\partial t} = \lim\limits_{\delta t\rightarrow 0} \frac{\mathrm P_{s,x}(t + \delta t) -  \mathrm P_{s,x}(t)}{\delta t}\\
= \kappa\ \mathrm P_{s-1,x}(t) +
(s+1)\lambda\ \mathrm P_{s+1,x}(t) +
s\rho\ \mathrm P_{s,x-1}(t) +
(x+1)\mu\ \mathrm P_{s, x + 1}(t)\\ \phantom{=} - 
(\kappa + s\lambda + s\rho + x\mu)\ \mathrm P_{s,x}(t)
\end{aligned}
\qquad(10)</span></span></p>
<p>In an analogous way a chemical master equation can be derived for any biochemical network <span class="citation" data-cites="2009:Gardiner">[<a href="#ref-2009:Gardiner" role="doc-biblioref">13</a>]</span> and thus forms the basis for our further computations. Eq. <a href="#eq:chemical_master_equation">10</a> describes a special kind of <em>stochastic process</em>, a so-called <em>jump process</em>.</p>
<p>In our example we might interpret <span class="math inline">S</span> as some signal whose quantity varies stochastically. For every signal molecule there is a constant probability to be sensed by the cell which triggers the creation of an <span class="math inline">X</span>. Additionally, <span class="math inline">X</span> molecules decays by themselves over time. We call the trajectory of <span class="math inline">S</span> the “signal” and the trajectory of <span class="math inline">X</span> the “response”. This nomenclature will we used throughout the thesis.</p>
</section>
<section id="jump-processes" class="level3" data-number="1.2.2">
<h3 data-number="2.2.2"><span class="header-section-number">2.2.2</span> Jump Processes</h3>
<p>Since particle counts can’t ever become negative, eq. <a href="#eq:chemical_master_equation">10</a> describes a Markov process in continuous time with the state space <span class="math inline">\{(s, x) | s\in\mathbb{N}_0, x\in\mathbb{N}_0\}</span>. In general, every continuous-time Markov process with a discrete state space obeys a master equation. Such processes are also commonly called <em>jump processes</em> since they generate discontinuous sample paths <span class="citation" data-cites="2017:Weber">[<a href="#ref-2017:Weber" role="doc-biblioref">19</a>]</span>.</p>
<p>A jump process <span class="math inline">\mathcal{X}</span> with state space <span class="math inline">\mathcal{U}</span> and an initial state <span class="math inline">\mathbf{x}_0\in\mathcal{U}</span> at time <span class="math inline">t_0</span> generates trajectories that can be described by a sequence of pairs <span class="math inline">(\mathbf{x}_i, t_i)_{i=1,2,\ldots}</span> where at every <em>transition time</em> <span class="math inline">t_i</span> there occurs a jump in state space <span class="math inline">\mathbf{x}_{i-1}\rightarrow \mathbf{x}_{i}</span>. The trajectories generated by a jump process are infinitely long and we denote the set of all possible trajectories by <span class="math inline">\mathcal K(\mathcal X)</span>. For a given duration <span class="math inline">\tau0</span> we denote the set of all unique trajectory segments of duration <span class="math inline">\tau</span> by <span class="math inline">\mathcal K_\tau[\mathcal X]</span>. Using the master equation for this process we can express the probability density for any trajectory segment in <span class="math inline">\mathcal K_\tau[\mathcal X]</span>.</p>
<p>The master equation for the process allows to express the probability distribution in trajectory space…</p>
</section>
</section>
<section id="generating-stochastic-trajectories" class="level2" data-number="1.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Generating Stochastic Trajectories</h2>
<section id="direct-gillespie-algorithm" class="level3" data-number="1.3.1">
<h3 data-number="2.3.1"><span class="header-section-number">2.3.1</span> Direct Gillespie Algorithm</h3>
</section>
<section id="simulating-a-biochemical-network-driven-by-an-external-signal" class="level3" data-number="1.3.2">
<h3 data-number="2.3.2"><span class="header-section-number">2.3.2</span> Simulating a Biochemical Network Driven by an External Signal</h3>
</section>
</section>
<section id="monte-carlo-computation-of-the-mutual-information" class="level2" data-number="1.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span> Monte-Carlo computation of the Mutual Information</h2>
<p>While so-called Monte-Carlo methods comprise a wide variety of approaches to stochastically evaluate integrals or sums the common idea is easily stated. We have a state space <span class="math inline">U</span> and a probability distribution <span class="math inline">p_U</span> over that state space. The problem is to evaluate</p>
<p><span><span class="math display">
\langle f(u) \rangle \equiv \int\limits_{u \in U} \mathrm du\; f(u) p_U(u)
\qquad(11)</span></span></p>
<p>where <span class="math inline">f: U\rightarrow\mathbb R</span> is some smooth function. If <span class="math inline">U</span> is high-dimensional it is very time-consuming to estimate it by direct numerical integration.</p>
<section id="computing-the-likelihood" class="level3" data-number="1.4.1">
<h3 data-number="2.4.1"><span class="header-section-number">2.4.1</span> Computing the likelihood</h3>
<p>The Probability density of a markovian trajectory can be expressed as</p>
<p><span><span class="math display">
\mathrm P(X) = \mathrm P(x_0,t_0;x_1,t_1;\ldots;x_{N-1},t_{N-1}) = \mathrm P(x_0,t_0 ) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}) \,.
\qquad(12)</span></span></p>
<p>Therefore the problem of calculating the likelihood for a particular trajectory amounts to solving two independent problems:</p>
<ol type="1">
<li>estimating the probability density of the starting point <span class="math inline">\mathrm P (x_0, t_0)</span> of a response</li>
<li>calculating the transition probabilities <span class="math inline">\mathrm P(x_n,t_n|x_{n-1},t_{n-1})</span></li>
</ol>
<p>For a given chemical reaction network we can write down the chemical master equation. The chemical master equation contains all the information needed to compute the individual terms <span class="math inline">\mathrm P(x_n,t_n|x_{n-1},t_{n-1})</span> for the entire system.</p>
<p>To calculate the mutual information between <span class="math inline">\mathcal{S}</span> and <span class="math inline">\mathcal X</span> we have to consider the entire reaction network containing the components both in <span class="math inline">S</span> and in <span class="math inline">X</span>. The precise reaction dynamics of the response part of the chemical network crucially depend on the observed signal trajectory. Therefore the chemical master equation for the whole reaction network allows us to compute the likelihood of a response trajectory for a particular signal trajectory:</p>
<p><span><span class="math display">
\mathrm P(\mathcal X = X|\mathcal S = S) = \mathrm P(x_0,t_0;x_1,t_1;\ldots;x_{N-1},t_{N-1} | S) = \mathrm P(x_0,t_0 | S) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S) \,.
\qquad(13)</span></span></p>
<p>For increasingly long trajectories this quantity will in many physically relevant cases either grow or decay exponentially (<em>TODO: explain why</em>). Thus sufficiently long trajectories, the numerical values of the likelihood will not be directly representable by conventional floating-point numbers.</p>
<p>This problem can be avoided if we compute the <em>log-likelihood</em> <span class="math inline">\ell(X|S) \equiv \ln\mathrm P(X|S)</span> instead. We can easily rephrase the equation for the likelihood:</p>
<p><span><span class="math display">
\ell(X|S) = \ln\left[ \mathrm P(x_0,t_0 | S) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S) \right] = \ln \mathrm P(x_0,t_0 | S) +\sum\limits^{N-1}_{n=1} \ln \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S)\,.
\qquad(14)</span></span></p>
</section>
<section id="the-probability-density-for-the-starting-point-of-a-trajectory" class="level3" data-number="1.4.2">
<h3 data-number="2.4.2"><span class="header-section-number">2.4.2</span> The probability density for the starting point of a trajectory</h3>
<p>We first look at the term <span class="math inline">P_0 = \mathrm P(x_0,t_0 | S)</span>. Since <span class="math inline">S</span> is a trajectory in time we can directly conclude from causality that</p>
<p><span><span class="math display">
\mathrm P(x_0, t_0 | S) = \mathrm P(x_0, t_0 | S_{t \leq t_0})
\qquad(15)</span></span></p>
<p>where <span class="math inline">S_{t \leq t_0}</span> is the temporal piece of the signal up to <span class="math inline">t_0</span>. We further suppose that the signal itself is markovian and therefore has no memory of its past. With this simplification we get</p>
<p><span><span class="math display">
\mathrm P(x_0, t_0 | S) = \mathrm P(x_0, t_0 | S_{t = t_0}) = \frac{\mathrm P((x_0, t_0), (s_0, t_0))}{\mathrm P(s_0, t_0)} \,.
\qquad(16)</span></span></p>
<p>We estimate <span class="math inline">P_0</span> using gaussian kernel density estimation to approximate both, the joint distribution of <span class="math inline">X_0, S_0</span> and the marginal distribution of <span class="math inline">S_0</span>.</p>
<p>Knowing the probabilities of the initial condition of both response and signal we can directly estimate the mutual information of <span class="math inline">\mathcal{X}_{t=t_0}</span> and <span class="math inline">\mathcal{S}_{t=t_0}</span>:</p>
<p><span><span class="math display">
\mathrm I(\mathcal{X}_{t=t_0}, \mathcal{S}_{t=t_0}) = \int ds_0\int dx_0\; \mathrm{P}(x_0, s_0)\; \ln \frac{\mathrm{P} (x_0, s_0)}{\mathrm{P} (x_0) \mathrm{P} (s_0)}
\qquad(17)</span></span></p>
</section>
<section id="estimating-the-marginal-probability-of-response-trajectories" class="level3" data-number="1.4.3">
<h3 data-number="2.4.3"><span class="header-section-number">2.4.3</span> Estimating the marginal probability of response trajectories</h3>
<p>To calculate the mutual information between trajectories we need to have a good estimate for <span class="math inline">\ln\left\langle \mathrm P(X | S) \right\rangle_\mathcal{S}</span>. We calculate this average by sampling of trajectories <span class="math inline">(S^{(i)})_{i=1\ldots N_S}</span> from the probability distribution of <span class="math inline">\mathcal{S}</span>:</p>
<p><span><span class="math display">
\ln\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S} \approx \ln \frac{\sum^{N_S}_{i=1} \mathrm P(X|S^{(i)})}{N_S} = \ln \sum^{N_S}_{i=1} \mathrm P(X|S^{(i)}) - \ln N_S
\qquad(18)</span></span></p>
<p>Thus we find that it is enough to be able to compute the likelihood between trajectories to estimate the marginal distribution of trajectories.</p>
<p>In practice (due to limited precision of floating-point arithmetic) it is only possible to evaluate the log-likelihood <span class="math inline">\ell(X|S) \equiv \ln\mathrm P(X|S)</span>. This means that the calculation of the averaged likelihood involves the quantity</p>
<p><span><span class="math display">
\ln \sum^{N_S}_{i=1} \mathrm P(X|S^{(i)}) = \ln \sum\limits^{N_S}_{i=1} \exp \ell(X|S^{(i)}) \equiv \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right)
\qquad(19)</span></span></p>
<p>where <span class="math inline">\mathrm{LSE} : \mathbb{R}^n \rightarrow \mathbb{R}</span> is called log-sum-exp . An interesting property of <span class="math inline">\mathrm{LSE}</span> is that it’s a smooth approximation to the <span class="math inline">\max</span> function. This means that for finite sample sizes the monte-carlo estimate of the averaged likelihood will always be too small!</p>
<p>We approximate the mutual information between trajectories as</p>
<p><span><span class="math display">
\mathrm{I}(\mathcal{X}; \mathcal{S}) = \left\langle \ln \frac{\mathrm{P} ( X |  S)}{\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S}} \right\rangle_{\mathcal{X},\mathcal{S}} = \left\langle \ell ( X |  S) - \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right) + \ln N_S\right\rangle_{\mathcal{X},\mathcal{S}}
\qquad(20)</span></span></p>
<p>which means that for finite amount of signal samples we will <em>systematically over-estimate</em> the mutual information. <strong>TODO: Is that really true? What about <span class="math inline">\ln N_S</span>?</strong> Even worse: the longer the trajectories the bigger the error becomes since the dimensionality of the space of possible signals is growing.</p>
<p>Another way to phrase this insight is that to get a good approximation for the logarithmic average likelihood, our set of signals that we use for monte-carlo sampling should contain many signals that produce a high likelihood. <strong>Therefore it probably is necessary to come up with a scheme to specifically sample signal trajectories for which the likelihood of a particular trajectory is high</strong>. On the other hand the results do not seem to get significantly better when averaging over more trajectories.</p>
</section>
</section>
<section id="references" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="1">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-2002:Elowitz">
<p>[1] M.B. Elowitz, A.J. Levine, E.D. Siggia, P.S. Swain, Stochastic Gene Expression in a Single Cell, Science. 297 (2002) 1183–1186. <a href="https://doi.org/10.1126/science.1070919">https://doi.org/10.1126/science.1070919</a>.</p>
</div>
<div id="ref-2008:Faisal">
<p>[2] A.A. Faisal, L.P.J. Selen, D.M. Wolpert, Noise in the nervous system, Nature Reviews Neuroscience. 9 (2008) 292–303. <a href="https://doi.org/10.1038/nrn2258">https://doi.org/10.1038/nrn2258</a>.</p>
</div>
<div id="ref-1990:Parsons">
<p>[3] P.A. Parsons, Fluctuating Asymmetry: An Epigenetic Measure of Stress, Biological Reviews. 65 (1990) 131–145. <a href="https://doi.org/10.1111/j.1469-185x.1990.tb01186.x">https://doi.org/10.1111/j.1469-185x.1990.tb01186.x</a>.</p>
</div>
<div id="ref-2011:Hallatschek">
<p>[4] O. Hallatschek, Noise Driven Evolutionary Waves, PLoS Computational Biology. 7 (2011) e1002005. <a href="https://doi.org/10.1371/journal.pcbi.1002005">https://doi.org/10.1371/journal.pcbi.1002005</a>.</p>
</div>
<div id="ref-2014:Tsimring">
<p>[5] L.S. Tsimring, Noise in biology, Reports on Progress in Physics. 77 (2014) 026601. <a href="https://doi.org/10.1088/0034-4885/77/2/026601">https://doi.org/10.1088/0034-4885/77/2/026601</a>.</p>
</div>
<div id="ref-1948:Shannon">
<p>[6] C.E. Shannon, A Mathematical Theory of Communication, Bell System Technical Journal. 27 (1948) 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.</p>
</div>
<div id="ref-2009:Tkačik">
<p>[7] G. Tkačik, A.M. Walczak, W. Bialek, Optimizing information flow in small genetic networks, Physical Review E. 80 (2009) 031920. <a href="https://doi.org/10.1103/physreve.80.031920">https://doi.org/10.1103/physreve.80.031920</a>.</p>
</div>
<div id="ref-2020:Uda">
<p>[8] S. Uda, Application of information theory in systems biology., Biophysical Reviews. 12 (2020) 377–384. <a href="https://doi.org/10.1007/s12551-020-00665-w">https://doi.org/10.1007/s12551-020-00665-w</a>.</p>
</div>
<div id="ref-2006:Cover">
<p>[9] T.M. Cover, J.A. Thomas, Elements of Information Theory, 2nd ed., John Wiley  Sons, 2006.</p>
</div>
<div id="ref-1908:Langevin">
<p>[10] P. Langevin, Sur la théorie du mouvement brownien, C. R. Acad. Sci. (1908) 530–533.</p>
</div>
<div id="ref-2010:Kunita">
<p>[11] H. Kunita, Itô’s stochastic calculus: Its surprising power for applications, Stochastic Processes and Their Applications. 120 (2010) 622–652. <a href="https://doi.org/10.1016/j.spa.2010.01.013">https://doi.org/10.1016/j.spa.2010.01.013</a>.</p>
</div>
<div id="ref-1997:Bunkin">
<p>[12] F.V. Bunkin, B.B. Kadomtsev, Y.L. Klimontovich, N.I. Koroteev, P.S. Landa, V.P. Maslov, Y.M. Romanovskii, In memory of Ruslan Leont’evich Stratonovich, Physics-Uspekhi. 40 (1997) 751–752. <a href="https://doi.org/10.1070/pu1997v040n07abeh000259">https://doi.org/10.1070/pu1997v040n07abeh000259</a>.</p>
</div>
<div id="ref-2009:Gardiner">
<p>[13] C. Gardiner, Stochastic Methods, 4th ed., Springer-Verlag, Berlin Heidelberg, 2009.</p>
</div>
<div id="ref-2010:Tostevin">
<p>[14] F. Tostevin, P.R. ten Wolde, Mutual information in time-varying biochemical systems, Physical Review E. 81 (2010) 061917. <a href="https://doi.org/10.1103/physreve.81.061917">https://doi.org/10.1103/physreve.81.061917</a>.</p>
</div>
<div id="ref-1976:Gillespie">
<p>[15] D.T. Gillespie, A general method for numerically simulating the stochastic time evolution of coupled chemical reactions, Journal of Computational Physics. 22 (1976) 403–434. <a href="https://doi.org/10.1016/0021-9991(76)90041-3">https://doi.org/10.1016/0021-9991(76)90041-3</a>.</p>
</div>
<div id="ref-2001:Gillespie">
<p>[16] D.T. Gillespie, Approximate accelerated stochastic simulation of chemically reacting systems, The Journal of Chemical Physics. 115 (2001) 1716–1733. <a href="https://doi.org/10.1063/1.1378322">https://doi.org/10.1063/1.1378322</a>.</p>
</div>
<div id="ref-2020:Richards">
<p>[17] D.M. Richards, J.J. Walker, J. Tabak, Ion channel noise shapes the electrical activity of endocrine cells, PLOS Computational Biology. 16 (2020) e1007769. <a href="https://doi.org/10.1371/journal.pcbi.1007769">https://doi.org/10.1371/journal.pcbi.1007769</a>.</p>
</div>
<div id="ref-2019:Cepeda-Humerez">
<p>[18] S.A. Cepeda-Humerez, J. Ruess, G. Tkačik, Estimating information in time-varying signals., PLoS Computational Biology. 15 (2019) e1007290. <a href="https://doi.org/10.1371/journal.pcbi.1007290">https://doi.org/10.1371/journal.pcbi.1007290</a>.</p>
</div>
<div id="ref-2017:Weber">
<p>[19] M.F. Weber, E. Frey, Master equations and the theory of stochastic path integrals, Reports on Progress in Physics. 80 (2017) 046601. <a href="https://doi.org/10.1088/1361-6633/aa5ae2">https://doi.org/10.1088/1361-6633/aa5ae2</a>.</p>
</div>
</div>
</section>
</section>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="monte-carlo-in-trajectory-space.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="monte-carlo-in-trajectory-space.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        

        

        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>