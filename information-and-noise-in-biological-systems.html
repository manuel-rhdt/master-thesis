<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js Light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mutual Information between Trajectories</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.min.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
        <script>document.addEventListener("DOMContentLoaded", function () {
            var mathElements = document.getElementsByClassName("math");
            var regex = /\\qquad\W*\(([0-9]+)\)/;
            for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                    var tex_str = texText.data.replace(regex, "\\tag{$1}");
                    katex.render(tex_str, mathElements[i], {
                    displayMode: mathElements[i].classList.contains('display'),
                    throwOnError: false,
                    fleqn: false
                });
            }}});
        </script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "" : "Light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('Light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded"><a href="index.html" class=""><span class="header-section-number">1</span> Introduction
                    </a>
                    </li><li class="expanded"><a href="acknowledgements.html" class="">Acknowledgements
                    </a>
                    </li><li class="expanded"><a href="introduction.html" class=""><span class="header-section-number">2</span> Introduction
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="introduction.html#goal-of-the-thesis" class=""><span class="header-section-number">2.1</span> Goal of the Thesis
                    </a>
                    </li><li class="expanded"><a href="introduction.html#related-work" class=""><span class="header-section-number">2.2</span> Related Work
                    </a>
                    </li><li class="expanded"><a href="introduction.html#structure" class=""><span class="header-section-number">2.3</span> Structure
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html" class="active"><span class="header-section-number">3</span> Information and Noise in Biological Systems
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-theory-in-the-context-of-cellular-signaling-networks" class=""><span class="header-section-number">3.1</span> Information Theory in the context of cellular signaling networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#mutual-information-as-an-efficiency-measure-in-cell-signaling" class=""><span class="header-section-number">3.1.1</span> Mutual Information as an Efficiency Measure in Cell signaling
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#information-transmission-for-time-varying-signals" class=""><span class="header-section-number">3.1.2</span> Information Transmission for Time-Varying Signals
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#stochastic-modeling-of-biochemical-networks" class=""><span class="header-section-number">3.2</span> Stochastic Modeling of Biochemical Networks
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="information-and-noise-in-biological-systems.html#markov-processes" class=""><span class="header-section-number">3.2.1</span> Markov Processes
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#chemical-master-equation" class=""><span class="header-section-number">3.2.2</span> Chemical Master Equation
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#jump-processes" class=""><span class="header-section-number">3.2.3</span> Jump Processes
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#probability-densities-of-trajectories" class=""><span class="header-section-number">3.2.4</span> Probability Densities of Trajectories
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#generating-stochastic-trajectories-for-jump-processes" class=""><span class="header-section-number">3.2.5</span> Generating Stochastic Trajectories for Jump Processes
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#a-simple-model-of-gene-expression" class=""><span class="header-section-number">3.3</span> A Simple Model of Gene Expression
                    </a>
                    </li><li class="expanded"><a href="information-and-noise-in-biological-systems.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html" class=""><span class="header-section-number">3</span> Monte-Carlo Estimate of the Mutual Information
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#monte-carlo-estimate-for-the-marginal-entropy" class=""><span class="header-section-number">3.1</span> Monte-Carlo Estimate for the Marginal Entropy
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-conditional-entropy" class=""><span class="header-section-number">3.2</span> Estimating the Conditional Entropy
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#monte-carlo-simulations-for-trajectories" class=""><span class="header-section-number">3.3</span> Monte-Carlo Simulations for Trajectories
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#general-stochastic-dynamics-of-signals" class=""><span class="header-section-number">3.3.1</span> General Stochastic Dynamics of Signals
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generating-responses-for-time-varying-signals" class=""><span class="header-section-number">3.3.2</span> Generating Responses for Time-Varying Signals
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generation-of-response-trajectories-according-to-mathrm-pmathbf-xmathbf-s" class=""><span class="header-section-number">3.3.2.1</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#generation-of-response-trajectories-according-to-mathrm-pmathbf-x" class=""><span class="header-section-number">3.3.2.2</span> Generation of response trajectories according to <span class="math inline">\mathrm P(\mathbf x)</span>
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#compuation-of-mathrm-pmathbf-xmathbf-s" class=""><span class="header-section-number">3.3.2.3</span> Compuation of <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span>
                    </a>
                    </li>
                    </ol>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#practical-concerns" class=""><span class="header-section-number">3.4</span> Practical Concerns
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#probability-distribution-of-the-initial-state" class=""><span class="header-section-number">3.4.1</span> Probability Distribution of the Initial State
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#computation-in-logarithmic-space" class=""><span class="header-section-number">3.4.2</span> Computation in Logarithmic Space
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-marginal-probability-of-response-trajectories" class=""><span class="header-section-number">3.4.3</span> Estimating the marginal probability of response trajectories
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#computation-of-the-mutual-information-for-a-simple-model-of-gene-expression" class=""><span class="header-section-number">3.5</span> Computation of the Mutual Information for a Simple Model of Gene Expression
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#simulations-with-different-parameters" class=""><span class="header-section-number">3.5.1</span> Simulations with different parameters
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#consistent-bias-in-comparisons-with-analytic-approximations" class=""><span class="header-section-number">3.5.2</span> Consistent Bias in Comparisons with Analytic Approximations
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#gaussian-approximation" class=""><span class="header-section-number">3.6</span> Gaussian Approximation
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#covariance-matrices" class=""><span class="header-section-number">3.6.1</span> Covariance Matrices
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#direct-importance-sampling" class=""><span class="header-section-number">3.6.2</span> Direct Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#umbrella-sampling" class=""><span class="header-section-number">3.6.3</span> Umbrella Sampling
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#estimating-the-conditional-entropy-1" class=""><span class="header-section-number">3.6.4</span> Estimating the Conditional Entropy
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#discussion" class=""><span class="header-section-number">3.7</span> Discussion
                    </a>
                    </li><li class="expanded"><a href="monte-carlo-estimate-of-the-mutual-information.html#references" class="">References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html" class=""><span class="header-section-number">4</span> Estimating Marginal Probabilities for Trajectories
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#previous-work-on-the-computation-of-normalization-constants" class=""><span class="header-section-number">4.1</span> Previous Work on the Computation of Normalization Constants
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#importance-sampling" class=""><span class="header-section-number">4.1.1</span> Importance Sampling
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#estimating-the-marginal-density-from-posterior-samples" class=""><span class="header-section-number">4.1.2</span> Estimating the Marginal Density from Posterior Samples
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#borrowing-terminology-from-statistical-physics" class=""><span class="header-section-number">4.2</span> Borrowing Terminology from Statistical Physics
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#thermodynamic-integration" class=""><span class="header-section-number">4.3</span> Thermodynamic Integration
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#summary-of-the-technique" class=""><span class="header-section-number">4.3.1</span> Summary of the Technique
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#markov-chain-monte-carlo" class=""><span class="header-section-number">4.3.2</span> Markov Chain Monte Carlo
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#estimating-the-density-of-states" class=""><span class="header-section-number">4.4</span> Estimating the Density of States
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#wang-and-landau-algorithm" class=""><span class="header-section-number">4.5</span> Wang and Landau Algorithm
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#applying-wang-landau-to-the-computation-of-the-marginal-density" class=""><span class="header-section-number">4.5.1</span> Applying Wang-Landau to the Computation of the Marginal Density
                    </a>
                    </li><li class="expanded">
                    <ol class="section">
                        <li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#modified-dos" class=""><span class="header-section-number">4.5.1.1</span> Modified DOS
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#modified-wang-landau-algorithm" class=""><span class="header-section-number">4.5.1.2</span> Modified Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#comparison-to-usual-wang-landau-algorithm" class=""><span class="header-section-number">4.5.1.3</span> Comparison to usual Wang-Landau algorithm
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#connection-to-standard-monte-carlo-sampling" class=""><span class="header-section-number">4.5.1.4</span> Connection to Standard Monte-Carlo Sampling
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#example-results-for-a-wang-landau-simulation" class=""><span class="header-section-number">4.5.2</span> Example Results for a Wang-Landau Simulation
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#generating-proposal-trajectories-from-stochastic-dynamics" class=""><span class="header-section-number">4.6</span> Generating Proposal Trajectories from Stochastic Dynamics
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#conclusion" class=""><span class="header-section-number">4.7</span> Conclusion
                    </a>
                    </li><li class="expanded"><a href="estimating-marginal-probabilities-for-trajectories.html#references" class=""><span class="header-section-number">4.8</span> References
                    </a>
                    </li>
                    </ol>
                    </li><li class="expanded"><a href="summary.html" class=""><span class="header-section-number">5</span> Summary
                    </a>
                    </li><li class="expanded"><a href="references-1.html" class="">References
                    </a>
                    </li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                    </div>

                    <h1 class="menu-title">Mutual Information between Trajectories</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        <a href="https://github.com/manuel-rhdt/master-thesis" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <section id="information-and-noise-in-biological-systems" class="level1" data-number="2">
<h1 data-number="3"><span class="header-section-number">3</span> Information and Noise in Biological Systems</h1>
<p><em>Noise</em> is inherent across diverse biological systems and remains relevant at all biological scales. From <em>stochastic gene expression</em> and random <em>action potential spikes</em> in neuronal networks at the cellular scale to the <em>development of multicellular organisms</em> all the way to the <em>variations in population level</em> of competing species in whole ecosystems, we find examples of processes which can only be described precisely by taking into account noise as an intrinsic feature <span class="citation" data-cites="2002.Elowitz 2008.Faisal 1990.Parsons 2011.Hallatschek 2014.Tsimring">[<a href="#ref-2002.Elowitz" role="doc-biblioref">1</a>–<a href="#ref-2014.Tsimring" role="doc-biblioref">5</a>]</span>. In this thesis we focus on the smaller end of this scale, namely on the stochastic description of <em>biochemical networks</em>. These comprise among others <em>gene expression</em>, <em>gene regulatory networks</em>, and <em>cell signaling</em> networks, all of which exhibit noise due to small copy numbers of participating components. Additionally, since biological processes often happen out of equilibrium even macroscopic quantities can exhibit large fluctuations. The main source of noise at the cellular level may be fluctuations in <em>gene expression</em> which propagate to higher levels of biological organization <span class="citation" data-cites="2014.Tsimring">[<a href="#ref-2014.Tsimring" role="doc-biblioref">5</a>]</span>. The abundance of noise in all these systems invites the questions of how cells can reliably make correct decisions, even in complex and changing environments and how cells are able to <em>encode</em> the information about their environment using biochemical networks.</p>
<p>To successfully function, cells must generally correctly react to environmental changes. This requires processing the environmental cues they receive through their receptors and thereby filtering out the useful information from the noisy signal. The processing of information through biochemical networks can be quite elaborate with individual network components being able to perform analogous functions as silicon computational devices such as transistors <span class="citation" data-cites="2020.Leifer">[<a href="#ref-2020.Leifer" role="doc-biblioref">6</a>]</span>. It is thus tempting to think that optimization of information processing drives the evolution of cellular signaling networks. To understand information processing from the cell’s point of view we employ the very general framework of <em>information theory</em> which was originally developed to address questions of the reliability of telecommunications <span class="citation" data-cites="1948.Shannon">[<a href="#ref-1948.Shannon" role="doc-biblioref">7</a>]</span>. Information theory has been successfully used in biology to study cellular communication, embryonic development and other biological systems <span class="citation" data-cites="2009.Tkačik 2020.Uda">[<a href="#ref-2009.Tkačik" role="doc-biblioref">8</a>,<a href="#ref-2020.Uda" role="doc-biblioref">9</a>]</span> [citation needed]. Notably, there are many parallels between biological signal processing and <em>noisy channels</em> which are used for example to describe information transmission across a telephone line.</p>
<p>A crucial feature of <em>information theory</em> is that its results are broadly applicable, irrespective of the nature of the communication channel or the medium used to transmit a signal. The communication channel merely describes any kind of abstract device that processes an <em>input</em> in a probabilistic way to yield a corresponding <em>output</em>. It turns out that the study of information transmission through such a channel is closely related to the study of noise since the amount of noise introduced by a communication channel sets an upper bound to the amount of data that can be transmitted through it, known as the <em>channel capacity</em> <span class="citation" data-cites="2006.Cover">[<a href="#ref-2006.Cover" role="doc-biblioref">10</a>]</span>. Consequently, the output can be described as a deterministic, lossless transformation of the input <em>plus</em> some random noise from the channel which leads to a loss of information. Note that in biological systems the signal itself is typically a fluctuating quantity such that the noise in the output is a combination of the channel noise and the signal noise. Since in cell signaling both, input and output are time-varying quantities we require a description of our system that allows for deterministic <em>and</em> stochastic time evolution.</p>
<p><em>Differential equations</em> are generally extremely useful to describe any kind of system that evolves deterministically. Therefore it is natural to try to extend the framework of <em>ordinary differential equations</em> (ODEs) to also include the ability to describe the effects of noise. Historically, this approach to modeling stochastic dynamics has first been formulated heuristically by Langevin to describe <em>Brownian motion</em> <span class="citation" data-cites="1908.Langevin">[<a href="#ref-1908.Langevin" role="doc-biblioref">11</a>]</span>. Later the theory of <em>stochastic differential equations</em> (SDEs) was put on solid mathematical footing by Itô and Stratonovich through the development of <em>stochastic calculus</em> which has been successfully used for applications in physics, biology, economics and others <span class="citation" data-cites="2010.Kunita 1997.Bunkin">[<a href="#ref-2010.Kunita" role="doc-biblioref">12</a>,<a href="#ref-1997.Bunkin" role="doc-biblioref">13</a>]</span> [citation needed]. The solutions to SDEs are not ordinary functions like for ODEs but <em>stochastic processes</em> that describe the probabilities for the system to be in any state for every instant in time. Consequently, a stochastic process contains the probabilities for any possible individual sequence of states in time, i.e. the probabilities for individual <em>trajectories</em>. Since SDEs contain a complete account of noise in the system, information theoretic concepts like the <em>entropy</em> and the <em>mutual information</em>—which we are going to use to understand information transmission in cell signaling—can be applied to stochastic processes. While SDEs can be formulated to describe the evolution of biochemical networks in a discrete state space it is generally more useful to use a less general but simpler <em>chemical master equation</em> for these kinds of problems <span class="citation" data-cites="2009.Gardiner">[<a href="#ref-2009.Gardiner" role="doc-biblioref">14</a>]</span>.</p>
<p>The chemical master equation is a description of a subset of stochastic processes by deriving the <em>time-evolution</em> of the probability distribution over the discrete state space. I.e. instead of describing the stochastic change to an individual state at a given time it focuses on the <em>deterministic</em> evolution of the whole probability distribution over all states. Conveniently, for a given set of chemical reactions that form a reaction network, we can easily find the corresponding chemical master equation that describes the stochastic dynamics of this network given some assumptions of homogeneity. The stochastic process that emerges of this formulation describes the probabilities for the individual counts of all species and how these probabilities change with time. The ease with which the chemical master equation allows the construction of a stochastic process for any kind of biochemical netork makes the idea very attractive to try to use <em>master equations</em> as the basis for information theoretic computations. If we can <em>solve</em> the master equation we in principle have access to all stochastic (and therefore information theoretic) properties of the corresponding biochemical network. E.g. in <span class="citation" data-cites="2010.Tostevin">[<a href="#ref-2010.Tostevin" role="doc-biblioref">15</a>]</span> it is shown how by analytically solving some very simple biochemical networks (using some approximations) it is possible to compute many quantities related to information transmission for time-varying signals and corresponding responses of these networks.</p>
<p>In most cases however, chemical master equations cannot be solved analytically and instead require computing averages for ensembles of <em>stochastic trajectories</em>. For instance, in Shannon’s information theory the amount of communicated information is not a function of individual signal-response pairs but an averaged quantity that depends on the probability of seeing <em>any</em> random signal-response-pair. Hence computationally efficient generation of stochastic realizations for a given master equation is a central requirement for the exact computation of information processing in chemical networks. A very well known algorithm for the <em>exact</em> generation of trajectories from a given initial condition is the <em>stochastic simulation algorithm</em> (SSA) also known by the name of its inventor as the <em>Gillespie algorithm</em> <span class="citation" data-cites="1976.Gillespie">[<a href="#ref-1976.Gillespie" role="doc-biblioref">16</a>]</span>. The most widely used variant is the <em>direct Gillespie method</em> which works by alternatingly a) computing the time during which no reactions happen and b) choosing which of the available reactions to perform next. As a result we generate a list of times where some reaction happens and a corresponding list of reactions that specifies the exact trajectory that was generated. This algorithm works quite well in practice and is also used for the work presented in this thesis. It is still worth mentioning that for systems that evolve at many different time scales simultaneously, the direct Gillespie method can be computationally inefficient since by its design it always operates at the smallest time scale. Therefore there have been developed further trajectory-generation algorithms that can generate <em>approximately</em> correct trajectories by accumulating various reactions into a single time step such as the <span class="math inline">\tau</span>-leap method <span class="citation" data-cites="2001.Gillespie">[<a href="#ref-2001.Gillespie" role="doc-biblioref">17</a>]</span>.</p>
<p>The paragraphs above provide an introductory overview of techniques and issues related to information processing in biological contexts. In this thesis we aim to present computational advances that allow the efficient estimation of the amount of information that a cell can use from its environment. In the following sections of this chapter we explain in more detail how we can model biochemical signaling networks in cells and how to understand information transmisssion in that context. The following chapters…</p>
<section id="information-theory-in-the-context-of-cellular-signaling-networks" class="level2" data-number="2.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Information Theory in the context of cellular signaling networks</h2>
<p>In this section we motivate the use of a quantities known as the <em>mutual information</em> and the <em>information rate</em> as relevant properties to understand optimality criteria for biochemical signaling networks.</p>
<section id="mutual-information-as-an-efficiency-measure-in-cell-signaling" class="level3" data-number="2.1.1">
<h3 data-number="3.1.1"><span class="header-section-number">3.1.1</span> Mutual Information as an Efficiency Measure in Cell signaling</h3>
<figure>
<img src="figures/information_cartoon.svg" id="fig:information_cartoon" alt="" /></img><figcaption>Figure 1: Abstracting cell signaling as an information channel. The channel’s input is an environmental signal that the cell needs to respond to. The signal processing happens through a biochemical network which “computes” a response which is the output of the information channel. The mutual information between signals and responses quantifies the cell’s ability to discern between different signals and choose appropriate responses.</figcaption>
</figure>
<p>In a general sense, cells sense chemical <em>signals</em> from their environment e.g. through receptors on their membrane. These signals provide the cell with important information e.g. about current environmental conditions, their position inside a structure or the location of food. To translate the signal into a useful response (such as expressing a certain gene or changing movement direction) cells have evolved biochemical signaling networks that recognize and process the signals. We use a general description of the cell that is depicted in fig. <a href="#fig:information_cartoon">1</a> where the signal acts as the input of the information channel. The processing of the signal that yields a response is assumed to be a known biochemical network and the response is one species of the biochemical network that acts as the “result” of the computation and represents the reaction of the cell to the signal.</p>
<p>For any given signal there are many stochastically possible responses. Conversely, for any given response there is a range of signals that could have produced it. Both of these statements of uncertainty can be quantified using a single concept: the <em>mutual information</em>. In information theoretic terms we quantify the <em>uncertainty</em> of a random variable <span class="math inline">\mathcal S</span> by the <em>entropy</em> <span id="eq:signal_entropy"><span class="math display">
\mathrm H(\mathcal S)=-\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s)\,\ln\mathrm P(\mathbf s)
\qquad(1)</span></span> where <span class="math inline">\sigma(\mathcal S)</span> is the set of possible realizations of <span class="math inline">\mathcal S</span> and we use <span class="math inline">\mathrm P(\mathbf s)</span> to denote the probability (density) of <span class="math inline">\mathbf s</span> with respect to the distribution of <span class="math inline">\mathcal S</span>. If <span class="math inline">\mathcal S</span> describes the signal then a large entropy signifies that there is a large range of possible signals that could be expected by the cell. Now given the response <span class="math inline">\mathbf x</span> to a signal <span class="math inline">\mathbf s</span>, we expect <span class="math inline">\mathbf x</span> to contain information about <span class="math inline">\mathbf s</span> such that the uncertainty about the signal is reduced. The conditional entropy <span class="math inline">\mathrm H(\mathcal S|\mathcal X)</span> denotes the <em>average</em> remaining uncertainty of a signal after observing the response, hence it reads <span id="eq:conditional_entropy"><span class="math display">
\mathrm H(\mathcal S|\mathcal X)=-
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x)
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s|\mathbf x)\ln\mathrm P(\mathbf s|\mathbf x) \,.
\qquad(2)</span></span> <span class="math inline">\mathrm P(\mathbf s|\mathbf x)</span> is the conditional distribution of the signals for a given response <span class="math inline">\mathbf x</span> which encodes the transmission characteristics of the communication channel. So we have eq. <a href="#eq:signal_entropy">1</a> which describes the distribution of signals and eq. <a href="#eq:conditional_entropy">2</a> which describes how information is processed. Combining eqns. <a href="#eq:signal_entropy">1</a>, <a href="#eq:conditional_entropy">2</a> we can express the <em>average</em> amount of information gained on the signal by observing the response <span id="eq:mi_form1"><span class="math display">
\mathrm I(\mathcal S,\mathcal X) = \mathrm H(\mathcal S) - \mathrm H(\mathcal S|\mathcal X) = 
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x)
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s|\mathbf x)
\ln \frac{\mathrm P(\mathbf s|\mathbf x)}{\mathrm P(\mathbf s)}
\qquad(3)</span></span> which is precisely the <em>mutual information between <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span></em>. It depends on both, characteristics of the communication channel and the statistics of the input signal. Notably the <span class="math inline">\mathrm I(\mathcal S,\mathcal X)</span> is symmetric under exchange of <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> such that we can express it as <span id="eq:mi_form2"><span class="math display">
\mathrm I(\mathcal S,\mathcal X) = \mathrm H(\mathcal X) - \mathrm H(\mathcal X|\mathcal S) = 
\int\limits_{\sigma(\mathcal S)} 
\mathrm d\mathbf s\ \mathrm P(\mathbf s)
\int\limits_{\sigma(\mathcal X)} 
\mathrm d\mathbf x\ \mathrm P(\mathbf x|\mathbf s)
\ln \frac{\mathrm P(\mathbf x|\mathbf s)}{\mathrm P(\mathbf x)}\,,
\qquad(4)</span></span> resulting in a more useful formula for the Monte-Carlo estimation of the MI. Therefore, on average, a response reduces the uncertainty about the signal by exactly the same amount that a signal reduces the uncertainty about the possible responses.</p>

</section>
<section id="information-transmission-for-time-varying-signals" class="level3" data-number="2.1.2">
<h3 data-number="3.1.2"><span class="header-section-number">3.1.2</span> Information Transmission for Time-Varying Signals</h3>
<p>Biochemical networks may store information about the signal in the time-dependency of the response, for example in cellular Ca<sup>2+</sup> signaling information seems to be encoded in the timing and duration of Calcium bursts <span class="citation" data-cites="2010.Tostevin 2008.Boulware 2020.Richards">[<a href="#ref-2010.Tostevin" role="doc-biblioref">15</a>,<a href="#ref-2008.Boulware" role="doc-biblioref">18</a>,<a href="#ref-2020.Richards" role="doc-biblioref">19</a>]</span>. For the case where the signal can be regarded as slowly changing with respect to the response, Cepeda-Humerez, et. al. propose a Monte-Carlo technique for the estimation of the MI that includes information that is stored in the full temporal dynamics of the response <span class="citation" data-cites="2019.Cepeda-Humerez">[<a href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">20</a>]</span>. As described in their work, that method is limited to situations that are well-described by a finite number of discrete signals.</p>
<p>Often however, biochemical networks not only respond to instantaneous signal levels but also to changes in the signal over time. [reference needed]. Therefore, we build on the technique in <span class="citation" data-cites="2019.Cepeda-Humerez">[<a href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">20</a>]</span> by extending it to allow for time-varying, stochastic signals as well. In this way we aim to find a novel way to compute the MI for time-varying signals <em>and</em> responses for general biochemical networks.</p>
<p>The study of time-varying quantities motivates the use of the <em>information rate</em> which is the asymptotic rate at which the MI between signal and response increases <span class="citation" data-cites="2010.Tostevin">[<a href="#ref-2010.Tostevin" role="doc-biblioref">15</a>]</span> <span id="eq:information_rate"><span class="math display">
\mathrm I_R = \lim\limits_{T\rightarrow\infty} \frac{\mathrm I(\mathcal S_T,\mathcal X_T)}{T}
\qquad(5)</span></span> where <span class="math inline">\mathcal S_T</span> and <span class="math inline">\mathcal X_T</span> are random variables over <em>trajectories</em> of length <span class="math inline">T</span>. By <em>trajectories</em> we denote an entire time-trace of the signal or response, instead of singular values at specific times. Since eq. <a href="#eq:information_rate">5</a> describes the information gained by the cell in a unit time interval it may be an important quantity for the cell to optimize for.</p>
<p>Hence we find that the fundamental building block for the computation of the information rate at the cellular level is to estimate the mutual information between trajectories of finite length. In the remainder of this thesis, we describe methods to compute the mutual information between trajectories <span class="math inline">\mathcal S_T,\mathcal X_T</span> based on a stochastic model of the biochemical signaling network.</p>

</section>
</section>
<section id="stochastic-modeling-of-biochemical-networks" class="level2" data-number="2.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Stochastic Modeling of Biochemical Networks</h2>
<ul>
<li>There exist model-free methods to quantify the entropies associated with signals and responses</li>
<li>These can only yield an upper bound for the entropy</li>
<li>In practice, biochemical networks may operate far from that bound(?)</li>
</ul>
<section id="markov-processes" class="level3" data-number="2.2.1">
<h3 data-number="3.2.1"><span class="header-section-number">3.2.1</span> Markov Processes</h3>
<ul>
<li>memoryless</li>
</ul>
</section>
<section id="chemical-master-equation" class="level3" data-number="2.2.2">
<h3 data-number="3.2.2"><span class="header-section-number">3.2.2</span> Chemical Master Equation</h3>
<p>As a model for the biochemical processing that takes place inside a cell we suppose that all interactions can be described by a chemical networks composed of different molecular species and reactions between them. Such networks can be described by a <em>chemical master equation</em> which makes it possible to compute all the probabilities associated with the time-evolution of such a system.</p>
<p>For illustrative purposes, let’s consider a highly simplified model of gene expression consisting of two components and four reactions <span id="eq:reaction_network1"><span class="math display"> \begin{gathered}
\emptyset \xrightarrow{\kappa} S \xrightarrow{\lambda} \emptyset\\
S \xrightarrow{\rho} S + X\\
X \xrightarrow{\mu}\emptyset\,.
\end{gathered} \qquad(6)</span></span> Here <span class="math inline">S</span> and <span class="math inline">X</span> are arbitrary chemical species whose particle counts we want to describe stochastically. The constants <span class="math inline">\kappa, \lambda, \rho, \mu</span> determine the rates at which the individual reactions occur. Hence, for every signal particle there is a constant rate <span class="math inline">\rho</span> to be sensed by the cell which triggers the creation of an <span class="math inline">X</span>. Additionally, <span class="math inline">X</span> particles decays by themselves over time with a per-particle rate of <span class="math inline">\mu</span>. Assuming a well stirred system in thermal equilibrium, it can be shown that the probabilities for the individual reactions happening at least once in the time interval <span class="math inline">[t, t+\mathrm\delta t]</span> are <span id="eq:transition_probabilities"><span class="math display">
\begin{aligned}
p^{(\kappa)}_{[t, t+\mathrm\delta t]} = \kappa\delta t + \mathcal{O}(\delta t^2)\\
p^{(\lambda)}_{[t, t+\mathrm\delta t]}(s) = s\lambda\delta t + \mathcal{O}(\delta t^2)\\
p^{(\rho)}_{[t, t+\mathrm\delta t]}(s) = s\rho\delta t + \mathcal{O}(\delta t^2)\\
p^{(\mu)}_{[t, t+\mathrm\delta t]}(x) = x\mu\delta t + \mathcal{O}(\delta t^2)
\end{aligned}
\qquad(7)</span></span> where <span class="math inline">s</span> and <span class="math inline">x</span> denote the particle numbers of the respective species at time <span class="math inline">t</span>. Consequently, the probability for <em>any</em> of the reactions to occur at least once in the time interval <span class="math inline">[t, t+\mathrm\delta t]</span> is <span id="eq:exit_probability"><span class="math display">p_{[t, t+\mathrm\delta t]}(s, x) = (\kappa + s\lambda + s\rho + x\mu)\ \mathrm \delta t + \mathcal{O}(\delta t^2)\qquad(8)</span></span></p>
<p>Using these expressions we can write down the so-called <em>chemical master equation</em> for this network. Let <span class="math inline">\mathrm P_{s,x}(t)</span> be the probability that the system is in state <span class="math inline">(s, x)</span> at time <span class="math inline">t</span>. Assuming that at most one reaction happens in the small time interval <span class="math inline">[t, t+\delta t]</span> we can use the transition probabilities from eqns. <a href="#eq:transition_probabilities">7</a>, <a href="#eq:exit_probability">8</a> to write <span><span class="math display">
\begin{aligned}
\mathrm P_{s,x}(t + \delta t) = \phantom{+}p^{(\kappa)}_{[t, t+\mathrm\delta t]}\ \mathrm P_{s-1,x}(t)\\ +
p^{(\lambda)}_{[t, t+\mathrm\delta t]}(s + 1)\ \mathrm P_{s+1,x}(t)\\ +
p^{(\rho)}_{[t, t+\mathrm\delta t]}(s)\ \mathrm P_{s,x-1}(t)\\ +
p^{(\mu)}_{[t, t+\mathrm\delta t]}(x + 1)\ \mathrm P_{s, x + 1}(t)\\ + 
\left[1 - p_{[t, t+\mathrm\delta t]}(s, x)\right]\ \mathrm P_{s,x}(t)
\end{aligned}
\qquad(9)</span></span> and by taking the limit <span class="math inline">\delta t\rightarrow 0</span> we arrive at the chemical master equation <span id="eq:chemical_master_equation"><span class="math display">
\begin{aligned}
\frac{\partial \mathrm P_{s,x}(t)}{\partial t} = \lim\limits_{\delta t\rightarrow 0} \frac{\mathrm P_{s,x}(t + \delta t) -  \mathrm P_{s,x}(t)}{\delta t}\\
= \kappa\ \mathrm P_{s-1,x}(t) +
(s+1)\lambda\ \mathrm P_{s+1,x}(t) +
s\rho\ \mathrm P_{s,x-1}(t) +
(x+1)\mu\ \mathrm P_{s, x + 1}(t)\\ \phantom{=} - 
(\kappa + s\lambda + s\rho + x\mu)\ \mathrm P_{s,x}(t)
\end{aligned}
\qquad(10)</span></span> In an analogous way a chemical master equation can be derived for any biochemical network <span class="citation" data-cites="2009.Gardiner">[<a href="#ref-2009.Gardiner" role="doc-biblioref">14</a>]</span> and thus forms the basis for our further computations. Stochastic processes that can be described by a master equation are generally called <em>jump processes</em> and provide a useful abstraction for the processes that we want to consider.</p>
</section>
<section id="jump-processes" class="level3" data-number="2.2.3">
<h3 data-number="3.2.3"><span class="header-section-number">3.2.3</span> Jump Processes</h3>
<p>Since particle counts can’t ever become negative, eq. <a href="#eq:chemical_master_equation">10</a> describes a Markov process in continuous time with the state space <span class="math inline">\{(s, x) | s\in\mathbb{N}_0, x\in\mathbb{N}_0\}</span>. In general, every continuous-time Markov process with a discrete state space obeys a master equation. Such processes are also commonly called <em>jump processes</em> since they generate discontinuous sample paths <span class="citation" data-cites="2017.Weber">[<a href="#ref-2017.Weber" role="doc-biblioref">21</a>]</span>.</p>
<p>A jump process with state space <span class="math inline">\mathcal{U}</span> and an initial state <span class="math inline">\mathbf{x}_0\in\mathcal{U}</span> at time <span class="math inline">t_0</span> generates trajectories that can be described by a sequence of pairs <span class="math inline">(\mathbf{x}_i, t_i)_{i=1,2,\ldots}</span> where at every <em>transition time</em> <span class="math inline">t_i</span> there occurs a jump in state space <span class="math inline">\mathbf{x}_{i-1}\rightarrow \mathbf{x}_{i}</span>. As illustrated in eq. <a href="#eq:chemical_master_equation">10</a> the master equation encodes the transition rates for all possible state changes in the system. Similarly, for an arbitrary jump process we can formulate the master equation <span id="eq:general_master_eq"><span class="math display">
\frac{\partial \mathrm P(\mathbf x, t|\mathbf x_0, t_0)}{\partial t} = \sum\limits_{\mathbf x^\prime\in\mathcal U} \left[
w_t(\mathbf x, \mathbf x^\prime)\ \mathrm P(\mathbf x^\prime, t|\mathbf x_0, t_0)
- w_t(\mathbf x^\prime, \mathbf x)\ \mathrm P(\mathbf x, t|\mathbf x_0, t_0)
\right]
\qquad(11)</span></span> where <span class="math inline">w_t(\mathbf x^\prime, \mathbf x)</span> specifies the rate for the transition <span class="math inline">\mathbf x \rightarrow \mathbf x^\prime</span> at time <span class="math inline">t</span> and <span class="math inline">w_t(\mathbf x, \mathbf x) = 0</span>. The first term of the sum in eq. <a href="#eq:general_master_eq">11</a> is known as the <em>gain</em> since it describes how probability flows from other states into the current one while the second term is called <em>loss</em> as it expresses how much probability flows away from the current state. This form of the master equation allows us to find a relatively simple expression for the probability of individual trajectories of jump processes.</p>
</section>
<section id="probability-densities-of-trajectories" class="level3" data-number="2.2.4">
<h3 data-number="3.2.4"><span class="header-section-number">3.2.4</span> Probability Densities of Trajectories</h3>
<p>From eqns. <a href="#eq:mi_form1">3</a>, <a href="#eq:mi_form2">4</a> we can see immediately that the computation of the mutual information between two random variables relies on the calculation of (conditional) probability densities for these variables. Our interest in this thesis lies in the computation of the mutual information for time-varying signals and responses. Hence we need to evaluate the mutual information between random variables whose individual samples are <em>entire trajectories</em>. To do this we require the notion of probability for a given trajectory, based on a given biochemical model for a cell. In the following derivation we show how the master equation as formulated in eq. <a href="#eq:general_master_eq">11</a> makes it possible to compute the trajectory probability exactly for the corresponding reaction network. In the next chapter we will then discuss how to use these results in practice to compute the conditional probability of a response <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> for a biochemical network and a known signal.</p>
<p>By the probability of a trajectory with <span class="math inline">N</span> jumps we denote joint probability density <span class="math inline">\mathrm P(\mathbf x_0, t_0;\ldots;\mathbf x_N,t_N)</span>. We include the initial state <span class="math inline">\mathbf x_0, t_0</span> in the probability since the initial condition itself is usually given by a probability distribution. Using conditional probabilities we can write <span id="eq:trajectory_probability"><span class="math display">
\begin{aligned}
\mathrm P(\mathbf x_0, t_0;\ldots;\mathbf x_N,t_N) =\; \mathrm P(\mathbf x_0,t_0)\,\mathrm P(\mathbf x_1,t_1|\mathbf x_0,t_0)\,\mathrm P(\mathbf x_2,t_2|\mathbf x_1,t_1;\mathbf x_0,t_0)\cdots\\
\mathrm P(\mathbf x_N,t_N|\mathbf x_{N-1},t_{N-1};\ldots;\mathbf x_0,t_0)\,.
\end{aligned}
\qquad(12)</span></span> We can make use of the fact that jump processes are Markov processes to simplify eq. <a href="#eq:trajectory_probability">12</a> such that every conditional probability only explicitly depends on the immediately preceding state <span id="eq:trajectory_probability_product"><span class="math display">
\begin{aligned}
\mathrm P(\mathbf x_0, t_0;\ldots;\mathbf x_N,t_N) = \mathrm P(\mathbf x_0,t_0)\,\mathrm P(\mathbf x_1,t_1|\mathbf x_0,t_0)\,\mathrm P(\mathbf x_2,t_2|\mathbf x_1,t_1)\cdots \mathrm P(\mathbf x_N,t_N|\mathbf x_{N-1},t_{N-1})\\
= \mathrm P(\mathbf x_0,t_0)\,\prod\limits^{N}_{i=1} \mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1})
\,.
\end{aligned}
\qquad(13)</span></span> Hence the expression for the probability of a trajectory contains two distinct kinds of probability distributions, a) the probability of the initial condition <span class="math inline">\mathrm P(\mathbf x_0,t_0)</span> and b) the transition probabilities for every step in the trajectory given by <span class="math inline">\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1})</span>. The initial condition <span class="math inline">\mathrm P(\mathbf x_0,t_0)</span> depends on the specific problem and will often be taken to be the steady-state distribution. For the transition probabilities however, we can find a simple expression in terms of the master equation.</p>
<p>The transition probability <span class="math inline">\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1})</span> describes the probability for a small segment of the trajectory from <span class="math inline">t_{i-1}</span> to <span class="math inline">t_i</span> where first, the system is at state <span class="math inline">\mathbf x_{i-1}</span> for a duration <span class="math inline">t_i - t_{i-1}</span> and then makes a jump <span class="math inline">\mathbf x_{i-1}\rightarrow \mathbf x_{i}</span>. We are now going to derive how to express the probabilities of the constant part as well as the probability of the jump using only terms from the master equation given in eq. <a href="#eq:general_master_eq">11</a>. At the start of the segment, the probability for there to be no jump until at least <span class="math inline">t  t_{i-1}</span> is called the <em>survival probability</em> <span class="math inline">S(\mathbf x_{i-1}, t_{i-1}, t)</span>. By noticing that the change in the survival probability is given by the <em>loss</em> term of the master equation <span><span class="math display">
S(\mathbf x_{i-1}, t_{i-1}, t+\delta t) = S(\mathbf x_{i-1}, t_{i-1}, t) \left[ 1-\delta t
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t_{i-1}}(\mathbf x^\prime, \mathbf x_{i-1}) + \mathcal O(\delta t^2) \right]
\qquad(14)</span></span> we can motivate the differential equation <span><span class="math display">
\frac{\partial S(\mathbf x_{i-1}, t_{i-1}, t)}{\partial t} = - S(\mathbf x_{i-1}, t_{i-1}, t) \sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t_{i-1}}(\mathbf x^\prime, \mathbf x_{i-1})
\qquad(15)</span></span> with the solution <span id="eq:survival_probability"><span class="math display">
S(\mathbf x_{i-1}, t_{i-1}, t) = \exp\left( -\int\limits^{t}_{t_{i-1}}\mathrm dt^\prime 
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t^\prime}(\mathbf x^\prime, \mathbf x_{i-1})
\right)\,.
\qquad(16)</span></span> The expression <span class="math inline">S(\mathbf x_{i-1}, t_{i-1}, t)</span> is the probability for no reaction happening from time <span class="math inline">t_{i-1}</span> until <em>at least</em> <span class="math inline">t</span>. The survival probability therefore is the cumulative probability distribution for the <em>waiting time</em> <span class="math inline">\tau_{i-1} = t - t_{i-1}</span>, i.e. using standard notation we can write <span class="math inline">\mathrm P(\tau_{i-1} \geq \delta) = S(\mathbf x_{i-1}, t_{i-1}, t_{i-1}+\delta)</span>. Since the probability for the waiting time to be <em>exactly</em> <span class="math inline">\tau_{i-1}=t_i-t_{i-1}</span> is <em>zero</em> we instead compute the probability density <span id="eq:survival_probability_density"><span class="math display">\mathrm P(\tau_{i-1} = t_i-t_{i-1}) =
\left. 
\frac{\partial\mathrm P(\tau_{i-1}  \delta)}{\partial\delta} 
\right|_{\delta = t_i-t_{i-1}} = 
\left. 
\frac{\partial [1 - S(\mathbf x_{i-1}, t_{i-1}, t_{i-1} + \delta)]}{\partial\delta}
\right|_{\delta = t_i-t_{i-1}}\,.
\qquad(17)</span></span></p>
<p>The jump <span class="math inline">\mathbf x_{i-1}\rightarrow \mathbf x_{i}</span> at time <span class="math inline">t_i</span> itself happens with probability <span id="eq:jump_probability"><span class="math display">\mathrm P(\mathbf x_{i-1}\rightarrow \mathbf x_{i}, t_i) = \frac{w_t(\mathbf x_{i}, \mathbf x_{i-1})}{\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t}(\mathbf x^\prime, \mathbf x_{i-1})}\,.\qquad(18)</span></span> Combining the jump probability with the survival probability we can thus express the transition probability density by the multiplication <span><span class="math display">
\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1}) = 
\mathrm P(\tau_{i-1} = t_i-t_{i-1})\;
\mathrm P(\mathbf x_{i-1}\rightarrow \mathbf x_{i}, t_i)
\qquad(19)</span></span> and by inserting the results from eqns. <a href="#eq:survival_probability">16</a>-<a href="#eq:jump_probability">18</a> we arrive at the expression <span id="eq:transition_probability"><span class="math display">
\mathrm P(\mathbf x_i,t_i|\mathbf x_{i-1},t_{i-1}) = w_t(\mathbf x_{i}, \mathbf x_{i-1})
\exp\left( -\int\limits^{t_i}_{t_{i-1}}\mathrm dt^\prime 
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t^\prime}(\mathbf x^\prime, \mathbf x_{i-1})
\right)
\,.
\qquad(20)</span></span></p>
<p>Therefore eq. <a href="#eq:transition_probability">20</a> plugged into eq. <a href="#eq:trajectory_probability_product">13</a> allows the exact computation of probability densities for arbitrary stochastic trajectories. The results were derived in enough generality such that time-dependent transition rates are explicitly allowed. We will see that the analytical expression for the probability of a stochastic trajectory is central for the estimation of the mutual information throughout this thesis. Eqns. <a href="#eq:trajectory_probability_product">13</a>, <a href="#eq:transition_probability">20</a> will be used to compute the conditional probability <span class="math inline">\mathrm P(\mathbf x|\mathbf s)</span> for stochastically generated signal and response trajectories <span class="math inline">\mathbf s</span> and <span class="math inline">\mathbf x</span>. Furthermore, the survival probability and the jump probability introduced in this section form the basis of the <em>Stochastic Simulation Algorithm</em> that is introduced in the following subsection.</p>
</section>
<section id="generating-stochastic-trajectories-for-jump-processes" class="level3" data-number="2.2.5">
<h3 data-number="3.2.5"><span class="header-section-number">3.2.5</span> Generating Stochastic Trajectories for Jump Processes</h3>
<p>One main ingredient for a computational recipe to calculate the mutual information is the formula for the probability of a trajectory, as derived in the previous section. Of equal importance is the correct generation of stochastic realizations for the biochemical model we intend to study. So far we showed how to model any biochemical network as a <em>jump process</em> through the formulation of the chemical master equation. In the previous section we were able to derive an equation for the probability of a trajectory using only terms from the master equation. Similarly, we can formulate the <em>stochastic simulation algorithm</em> which efficiently generates trajectories for any jump process with a known master equation. In the following we explain the variant named <em>direct Gillespie alogithm</em> after its inventor <span class="citation" data-cites="1976.Gillespie">[<a href="#ref-1976.Gillespie" role="doc-biblioref">16</a>]</span>.</p>
<p>The direct Gillespie algorithm is an efficient and exact procedure to generate stochastic trajectories for jump processes. Starting from an initial state <span class="math inline">\mathbf x_0</span> at time <span class="math inline">t_0</span> it generates a trajectory <span class="math inline">\mathbf x = (\mathbf x_0, t_0;\ldots;\mathbf x_N, t_N)</span> that is a realization of the corresponding stochastic process. Since the trajectories of a jump process are constant segments, separated by discontinuous jumps, at every point in time the system either remains unchanged or it performs an instantaneous jump. Therefore the algorithm works in two phases that are repeated over and over until a trajectory of the desired length is generated.</p>
<p>In the first phase we compute the stochastic waiting time <span class="math inline">\tau</span> until the next <em>event</em> occurs. Given that we are currently at time <span class="math inline">t_{i-1}</span> and state <span class="math inline">\mathbf x_{i-1}</span>, the probability for the waiting time being <span class="math inline">\tau\geq t-t_{i-1}</span> is given by the survival probability from eq. <a href="#eq:survival_probability">16</a>. By the inverse function method we can generate a correctly distributed waiting time <span class="math inline">\tau</span> using a random variate <span class="math inline">u</span> that is uniformly distributed between 0 and 1 and then solving the equation <span id="eq:inverse_function_method"><span class="math display">
u = 1-\exp\left( -\int\limits^{t_{i-1} + \tau}_{t_{i-1}}\mathrm dt^\prime 
\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t^\prime}(\mathbf x^\prime, \mathbf x_{i-1}) \right)
\qquad(21)</span></span> for <span class="math inline">\tau</span>. We then set <span class="math inline">t_i = t_{i-1} + \tau</span>. For general transition rates <span class="math inline">w_{t^\prime}</span> eq. <a href="#eq:inverse_function_method">21</a> will have no analytical solution and it may be necessary to use approximate numerical integration techniques to solve for <span class="math inline">\tau</span>. If however the transition rates are time-independent for a given state we find the simple solution <span><span class="math display">
\tau = -\frac{\ln(1-u)}{\sum\limits_{\mathbf x^\prime\in\mathcal U} w_{t_{i-1}}(\mathbf x^\prime, \mathbf x_{i-1})}\,.
\qquad(22)</span></span></p>
<p>After the first phase we already know the time of the next event. Therefore all that remains to do is to decide which reaction should happen. The probability for the individual jumps is given by eq. <a href="#eq:jump_probability">18</a> and thus we can make a weighted choice between all reactions according to their probabilities. Using this choice we have found the next state <span class="math inline">\mathbf x_i</span> and we have finished one step in the stochastic simulation.</p>
<p>Using both of</p>

</section>
</section>
<section id="a-simple-model-of-gene-expression" class="level2" data-number="2.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> A Simple Model of Gene Expression</h2>
<p>In the course of this thesis we explore computational methods for the estimation of the mutual information in cellular reaction networks. To test and analyse our algorithms we perform computations for a very simple biochemical network for gene expression, consisting in merely four reactions and two species. It represents one of the simplest possible problems that a Monte-Carlo approach should be able to solve. Consequently, an estimation procedure that fails even on this simple model will almost certainly not provide satisfactory results for more realistic and complex networks. So on one hand we use a very simple biochemical network as a minimal system that an algorithm must be able to solve. On the other hand this model for gene expression has already been studied from an information theoretical point of view. Its simple structure allowed Tostevin, et. al. <span class="citation" data-cites="2010.Tostevin">[<a href="#ref-2010.Tostevin" role="doc-biblioref">15</a>]</span> to derive precise analytical approximations for the mutual information that we can use to test the results of our computational estimates against. In the following we describe the biochemical network and derive some useful analytical results.</p>
<p>We begin by specifying the 4 reactions that make up a simple model for gene expression <span id="eq:simple_reaction_network"><span class="math display">
\begin{gathered}
\emptyset \xrightarrow{\kappa} S \xrightarrow{\lambda} \emptyset\\
S \xrightarrow{\rho} S + X\\
X \xrightarrow{\mu}\emptyset
\end{gathered}
\qquad(23)</span></span> and note that it is the same network that was already used in eq. <a href="#eq:reaction_network1">6</a>. It includes two species, <span class="math inline">S</span> and <span class="math inline">X</span> that represent the signal and the response, respectively. The signal dynamics are fully described by the first two reactions, i.e. a birth-death process where signals are “born” stochastically with a constant rate <span class="math inline">\kappa</span> and every signal particle has a constant decay rate of <span class="math inline">\lambda</span>. The response dynamics are given by the other two reactions in eq. <a href="#eq:simple_reaction_network">23</a>. Each signal particle can stochastically produce a response with rate <span class="math inline">\rho</span> and every response itself decays with rate <span class="math inline">\mu</span>. The stochastic process that corresponds to the full reaction network under well-mixed conditions is described by a chemical master equation that we anticipatingly already derived for this network in eq. <a href="#eq:chemical_master_equation">10</a>. While the stochastic formulation describes the full reaction dynamics (assuming the system is well-mixed) we can understand many of its properties from a deterministic description.</p>
<p>The average number of signal particles <span class="math inline">s(t)</span> is described by the deterministic rate equation <span id="eq:det_s"><span class="math display">
\partial_t s(t) = \kappa - \lambda\ s(t)\,.
\qquad(24)</span></span> This ODE has a fixed point that represents the steady-state average of the signal <span class="math inline">\bar s = \kappa / \lambda</span>. In a similar way we can deterministically describe the average number of response particles <span class="math inline">x(t)</span> using the ODE <span id="eq:det_x"><span class="math display">
\partial_t x(t) = \rho\ s(t) - \mu\ x(t)\,.
\qquad(25)</span></span> Analogous to the signal we have a steady state average for the response <span class="math inline">\bar x = \rho\bar s / \mu</span>. In this case, the deterministic equations are not only useful to derive steady-state averages but can also be used to understand <em>correlations</em> between the components of the system. If <span class="math inline">S_t</span> is the stochastic count of signal particles at time <span class="math inline">t</span> we call <span class="math inline">C_{ss}(t, t^\prime) = \langle S_t S_{t^\prime}\rangle</span> the <em>autocorrelation function</em> of <span class="math inline">\mathcal S</span>. Similarly we can also define correlation functions between the signal and the response, such as <span class="math inline">C_{sx}(t, t^\prime) = \langle S_t X_{t^\prime}\rangle</span>, where by <span class="math inline">X_t</span> we denote the stochastic number of X molecules at time <span class="math inline">t</span>. Thus, in total we have four different correlation functions <span class="math inline">C_{ss}, C_{sx}, C_{xs},</span> and <span class="math inline">C_{xx}</span>. If we assume the system is in steady state, the correlation functions do only depend on the time-difference <span class="math inline">t^\prime - t</span> such that we can write <span class="math inline">C_{\alpha\beta}(t, t^\prime) = C_{\alpha\beta}(t^\prime - t)</span> <span class="citation" data-cites="2009.Gardiner">[<a href="#ref-2009.Gardiner" role="doc-biblioref">14</a>]</span>. For simple biochemical networks like eq. <a href="#eq:simple_reaction_network">23</a> it is relatively straightforward to analytically derive the correlation functions for the steady state.</p>
<p>Since the deterministic eqns. <a href="#eq:det_s">24</a>, <a href="#eq:det_x">25</a> are both <em>first-order</em> ODEs we say that the biochemical system has <em>linear equations of motions</em> such that for some matrix <span class="math inline">A(t)</span> and vector <span class="math inline">\xi(t)</span> we can write <span><span class="math display">
\frac{\partial \mathbf z(t)}{\partial t} = A(t)\ \mathbf z(t) + \xi(t)
\qquad(26)</span></span> with <span class="math inline">\mathbf z(t) = (s(t),x(t))^T</span>. We say that the corresponding Markov process describes a <em>linear system</em>. In the following we make use of the fact that for linear systems we can apply the <em>regression theorem</em> <span class="citation" data-cites="2009.Gardiner">[<a href="#ref-2009.Gardiner" role="doc-biblioref">14</a>]</span> <span><span class="math display">\partial_tC_{ij}(t) = -\sum_k A_{ik}(t)C_{kj}(t)\qquad(27)</span></span> where <span class="math inline">C_{ij}</span> are the correlation functions. Specifically, for our biochemical network we find the following set of coupled differential equations for the correlation functions <span><span class="math display">
\begin{aligned}
\partial_t C_{ss}(t) = -\lambda\ C_{ss}(t) \\
\partial_t C_{sx}(t) = \rho\ C_{ss}(t)-\mu\ C_{sx}(t) \\
\partial_t C_{xs}(t) = -\lambda\ C_{xs}(t) \\
\partial_t C_{xx}(t) = \rho\ C_{xs}(t)-\mu\ C_{xx}(t)
\end{aligned}
\qquad(28)</span></span> with the solutions for <span class="math inline">t\geq0</span> (assuming steady-state initial conditions) <span id="eq:correlation_functions"><span class="math display">
\begin{aligned}
C_{ss}(t) = \frac\kappa\lambda e^{-\lambda t} \\
C_{sx}(t) = \frac{\rho\kappa}{\lambda(\lambda - \mu)} 
\left[ \left( 1 + \frac{\lambda - \mu}{\lambda + \mu} \right) e^{-\mu t} - e^{-\lambda t} \right] \\
C_{xs}(t) = \frac{\rho\kappa}{\lambda(\lambda + \mu)} e^{-\lambda t}  \\
C_{xx}(t) = \frac{\rho^2 \kappa}{\lambda(\lambda^2 - \mu^2)} \left( e^{-\mu t} - e^{-\lambda t} \right) + \left( 1 + \frac{\rho}{\lambda + \mu} \right) \frac{\rho \kappa}{\lambda \mu} e^{-\mu t}\,.
\end{aligned}
\qquad(29)</span></span> Since by definition <span class="math inline">C_{\alpha\beta}(-t) = C_{\beta\alpha}(t)</span>, from eq. <a href="#eq:correlation_functions">29</a> we know the complete correlation functions for the system in steady state. The correlation functions are plotted in fig. <a href="#fig:correlation">2</a> to visualize the system behavior.</p>
<figure>
<img src="figures/correlation.svg" id="fig:correlation" alt="" /></img><figcaption>Figure 2: Correlation functions for the steady state <span class="math inline">C_{\alpha\beta}</span> from eq. <a href="#eq:correlation_functions">29</a>, plotted as functions of time. They describe how the deviations from the mean correlate over time. The solid lines represent the autocorrelation functions whereas the dashed lines display correlations between the signal and response. The slight peak at <span class="math inline">t^\prime-t\approx70</span> of <span class="math inline">C_{sx}(t^\prime-t)=\langle S_t X_{t^\prime} \rangle</span> shows that the chemical interaction between <span class="math inline">S</span> and <span class="math inline">X</span> leads to positive correlations between them. For the reaction rates the values from tbl. <a href="#tbl:k">1</a> were used.</figcaption>
</figure>
<p>The steady-state averages and the correlation functions represent the first and second moments of the trajectories of <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span>. By discarding all higher-order moments and discretizing time we build an approximate stochastic model for the chemical reaction network that allows further analytic results. Since we will approximate the trajectories using samples form multivariate normal distributions, this method is often called the <em>Gaussian approximation</em>. For a given discretization <span class="math inline">t_1t_2\cdotst_d</span> we describe the signal and response trajectories as a vector of values at discrete sample times, e.g. <span class="math inline">\mathbf s = \left(s(t_1),\ldots,s(t_d)\right)^T</span>. In the following we define a multivariate probability density <span class="math inline">\mathrm P(\mathbf{s}, \mathbf{x})</span> for the signal and response trajectories such that the resulting approximate system has identical correlation functions to the full biochemical network.</p>
<p>Hence we consider the case where the joint probability distribution <span class="math inline">\mathrm P(\mathbf{s}, \mathbf{x})</span> is given by a multivariate normal distribution <span id="eq:joint_multivariate"><span class="math display">
\mathrm P(\mathbf{s}, \mathbf{x}) = \frac{1}{\sqrt{\left( 2\pi  \right)^{2d} \det Z}} \;\exp\left[-\frac12\ (\mathbf s^T\; \mathbf x^T)\ Z^{-1}\ \binom{\mathbf s}{\mathbf x}\right]
\qquad(30)</span></span> where <span class="math inline">\mathbf s, \mathbf x \in \mathbb R^d</span> are the signal and response vectors respectively and the symmetric positive-definite covariance matrix <span class="math inline">Z\in\mathbb R^{2d\times 2d}</span> has the block form <span id="eq:corr_z"><span class="math display">
Z =  \begin{pmatrix}
C_{ss}  C_{xs} \\
C_{sx}  C_{xx}
\end{pmatrix}
\qquad(31)</span></span> with matrices <span class="math inline">C_{\alpha\beta}\in\mathbb R^{d\times d}</span>. The correlation functions in eq. <a href="#eq:correlation_functions">29</a> then give us the elements of the matrix blocks <span><span class="math display">
(C_{\alpha\beta})_{ij} = C_{\alpha\beta}(t_j - t_i)\,.
\qquad(32)</span></span> For the joint distribution in eq. <a href="#eq:joint_multivariate">30</a> there exists a simple analytical expression to compute the mutual information between <span class="math inline">\mathcal S</span> and <span class="math inline">\mathcal X</span> <span class="citation" data-cites="2010.Tostevin 1948.Shannon">[<a href="#ref-1948.Shannon" role="doc-biblioref">7</a>,<a href="#ref-2010.Tostevin" role="doc-biblioref">15</a>]</span> <span id="eq:analytical_mi"><span class="math display">
\mathrm I(\mathcal S, \mathcal X) = \frac 12 \ln\left( \frac{\det C_{ss} \det C_{xx}}{\det Z} \right)
\qquad(33)</span></span> which will be our benchmark to compare the proposed Monte-Carlo estimation procedure against. In a similar way we can also acquire analytical equations for both the marginal entropy <span class="math inline">\mathrm H(\mathcal X)</span> and the conditional entropy <span class="math inline">\mathrm H(\mathcal X | \mathcal S)</span>.</p>
<p>While we can use eq. <a href="#eq:analytical_mi">33</a> to directly evaluate the mutual information for trajectories using discretized time, Tostevin, et. al. <span class="citation" data-cites="2010.Tostevin">[<a href="#ref-2010.Tostevin" role="doc-biblioref">15</a>]</span> derive—specifically for this reaction network—a formula to analytically compute the mutual information rate (in nats per unit time) in the limit of infinitely fine discretization <span id="eq:analytical_rate"><span class="math display">
I_R = \frac{\lambda}{2}\left( \sqrt{1 + \frac{\rho}{\lambda}} -1 \right)\,.
\qquad(34)</span></span></p>
<p>In this subsection we analyzed a simple, yet non-trivial example of a biochemical network for which we intend to compute the mutual information using Monte-Carlo techniques. Since we were able to derive the correlation functions for this network, we can make use of the Gaussian approximation to estimate the mutual information using eq. <a href="#eq:analytical_mi">33</a>. That approximation will turn out to be very useful to understand the accuracy of Monte-Carlo estimates. To make the different results in this thesis comparable we used consistent parameter values for the reaction constants that are shown in tbl. <a href="#tbl:k">1</a>.</p>
<div id="tbl:k">
<table>
<caption>Table 1: Values of the reaction constants along with the steady-state averages, correlation times and approximate information rate from eq. <a href="#eq:analytical_rate">34</a>. The values for the reaction constants were used for all computations unless stated otherwise.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\kappa</span></th>
<th style="text-align: center;"><span class="math inline">\lambda</span></th>
<th style="text-align: center;"><span class="math inline">\rho</span></th>
<th style="text-align: center;"><span class="math inline">\mu</span></th>
<th style="text-align: center;"><span class="math inline">\bar s</span></th>
<th style="text-align: center;"><span class="math inline">\bar x</span></th>
<th style="text-align: center;"><span class="math inline">\tau_s</span></th>
<th style="text-align: center;"><span class="math inline">\tau_x</span></th>
<th style="text-align: center;"><span class="math inline">I_R</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">0.00183</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="references" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="1">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-2002.Elowitz">
<p>[1] M.B. Elowitz, A.J. Levine, E.D. Siggia, P.S. Swain, Stochastic Gene Expression in a Single Cell, Science. 297 (2002) 1183–1186. <a href="https://doi.org/10.1126/science.1070919">https://doi.org/10.1126/science.1070919</a>.</p>
</div>
<div id="ref-2008.Faisal">
<p>[2] A.A. Faisal, L.P.J. Selen, D.M. Wolpert, Noise in the nervous system, Nature Reviews Neuroscience. 9 (2008) 292–303. <a href="https://doi.org/10.1038/nrn2258">https://doi.org/10.1038/nrn2258</a>.</p>
</div>
<div id="ref-1990.Parsons">
<p>[3] P.A. Parsons, Fluctuating Asymmetry: An Epigenetic Measure of Stress, Biological Reviews. 65 (1990) 131–145. <a href="https://doi.org/10.1111/j.1469-185x.1990.tb01186.x">https://doi.org/10.1111/j.1469-185x.1990.tb01186.x</a>.</p>
</div>
<div id="ref-2011.Hallatschek">
<p>[4] O. Hallatschek, Noise Driven Evolutionary Waves, PLoS Computational Biology. 7 (2011) e1002005. <a href="https://doi.org/10.1371/journal.pcbi.1002005">https://doi.org/10.1371/journal.pcbi.1002005</a>.</p>
</div>
<div id="ref-2014.Tsimring">
<p>[5] L.S. Tsimring, Noise in biology, Reports on Progress in Physics. 77 (2014) 026601. <a href="https://doi.org/10.1088/0034-4885/77/2/026601">https://doi.org/10.1088/0034-4885/77/2/026601</a>.</p>
</div>
<div id="ref-2020.Leifer">
<p>[6] I. Leifer, F. Morone, S.D.S. Reis, J.S. Andrade, M. Sigman, H.A. Makse, Circuits with broken fibration symmetries perform core logic computations in biological networks, PLOS Computational Biology. 16 (2020) e1007776. <a href="https://doi.org/10.1371/journal.pcbi.1007776">https://doi.org/10.1371/journal.pcbi.1007776</a>.</p>
</div>
<div id="ref-1948.Shannon">
<p>[7] C.E. Shannon, A Mathematical Theory of Communication, Bell System Technical Journal. 27 (1948) 379–423. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.</p>
</div>
<div id="ref-2009.Tkačik">
<p>[8] G. Tkačik, A.M. Walczak, W. Bialek, Optimizing information flow in small genetic networks, Physical Review E. 80 (2009) 031920. <a href="https://doi.org/10.1103/physreve.80.031920">https://doi.org/10.1103/physreve.80.031920</a>.</p>
</div>
<div id="ref-2020.Uda">
<p>[9] S. Uda, Application of information theory in systems biology., Biophysical Reviews. 12 (2020) 377–384. <a href="https://doi.org/10.1007/s12551-020-00665-w">https://doi.org/10.1007/s12551-020-00665-w</a>.</p>
</div>
<div id="ref-2006.Cover">
<p>[10] T.M. Cover, J.A. Thomas, Elements of Information Theory, 2nd ed., John Wiley  Sons, 2006.</p>
</div>
<div id="ref-1908.Langevin">
<p>[11] P. Langevin, Sur la théorie du mouvement brownien, C. R. Acad. Sci. (1908) 530–533.</p>
</div>
<div id="ref-2010.Kunita">
<p>[12] H. Kunita, Itô’s stochastic calculus: Its surprising power for applications, Stochastic Processes and Their Applications. 120 (2010) 622–652. <a href="https://doi.org/10.1016/j.spa.2010.01.013">https://doi.org/10.1016/j.spa.2010.01.013</a>.</p>
</div>
<div id="ref-1997.Bunkin">
<p>[13] F.V. Bunkin, B.B. Kadomtsev, Y.L. Klimontovich, N.I. Koroteev, P.S. Landa, V.P. Maslov, Y.M. Romanovskii, In memory of Ruslan Leont’evich Stratonovich, Physics-Uspekhi. 40 (1997) 751–752. <a href="https://doi.org/10.1070/pu1997v040n07abeh000259">https://doi.org/10.1070/pu1997v040n07abeh000259</a>.</p>
</div>
<div id="ref-2009.Gardiner">
<p>[14] C. Gardiner, Stochastic Methods, 4th ed., Springer-Verlag, Berlin Heidelberg, 2009.</p>
</div>
<div id="ref-2010.Tostevin">
<p>[15] F. Tostevin, P.R. ten Wolde, Mutual information in time-varying biochemical systems, Physical Review E. 81 (2010) 061917. <a href="https://doi.org/10.1103/physreve.81.061917">https://doi.org/10.1103/physreve.81.061917</a>.</p>
</div>
<div id="ref-1976.Gillespie">
<p>[16] D.T. Gillespie, A general method for numerically simulating the stochastic time evolution of coupled chemical reactions, Journal of Computational Physics. 22 (1976) 403–434. <a href="https://doi.org/10.1016/0021-9991(76)90041-3">https://doi.org/10.1016/0021-9991(76)90041-3</a>.</p>
</div>
<div id="ref-2001.Gillespie">
<p>[17] D.T. Gillespie, Approximate accelerated stochastic simulation of chemically reacting systems, The Journal of Chemical Physics. 115 (2001) 1716–1733. <a href="https://doi.org/10.1063/1.1378322">https://doi.org/10.1063/1.1378322</a>.</p>
</div>
<div id="ref-2008.Boulware">
<p>[18] M.J. Boulware, J.S. Marchant, Timing in Cellular Ca2+ Signaling, Current Biology. 18 (2008) R769–R776. <a href="https://doi.org/10.1016/j.cub.2008.07.018">https://doi.org/10.1016/j.cub.2008.07.018</a>.</p>
</div>
<div id="ref-2020.Richards">
<p>[19] D.M. Richards, J.J. Walker, J. Tabak, Ion channel noise shapes the electrical activity of endocrine cells, PLOS Computational Biology. 16 (2020) e1007769. <a href="https://doi.org/10.1371/journal.pcbi.1007769">https://doi.org/10.1371/journal.pcbi.1007769</a>.</p>
</div>
<div id="ref-2019.Cepeda-Humerez">
<p>[20] S.A. Cepeda-Humerez, J. Ruess, G. Tkačik, Estimating information in time-varying signals., PLoS Computational Biology. 15 (2019) e1007290. <a href="https://doi.org/10.1371/journal.pcbi.1007290">https://doi.org/10.1371/journal.pcbi.1007290</a>.</p>
</div>
<div id="ref-2017.Weber">
<p>[21] M.F. Weber, E. Frey, Master equations and the theory of stochastic path integrals, Reports on Progress in Physics. 80 (2017) 046601. <a href="https://doi.org/10.1088/1361-6633/aa5ae2">https://doi.org/10.1088/1361-6633/aa5ae2</a>.</p>
</div>
</div>
</section>
</section>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="introduction.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="monte-carlo-estimate-of-the-mutual-information.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="introduction.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="monte-carlo-estimate-of-the-mutual-information.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        

        

        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>