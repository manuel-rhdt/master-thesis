@book{2006:Cover, 
author = {Cover, Thomas M. and Thomas, Joy A.}, 
title = {{Elements of Information Theory}}, 
isbn = {978-0-471-24195-9}, 
publisher = {John Wiley \& Sons}, 
edition = {2}, 
year = {2006}
}
@article{2008:Boulware, 
author = {Boulware, Michael J and Marchant, Jonathan S}, 
title = {{Timing in Cellular Ca2+ Signaling}}, 
issn = {0960-9822}, 
doi = {10.1016/j.cub.2008.07.018}, 
pmid = {18786382}, 
pmcid = {PMC3236564}, 
abstract = {{Calcium (Ca2+) signals are generated across a broad time range. Kinetic considerations impact how information is processed to encode and decode Ca2+ signals, the choreography of responses that ensure specific and efficient signaling and the overall temporal amplification such that ephemeral Ca2+ signals have lasting physiological value. The reciprocal importance of timing for Ca2+ signaling, and Ca2+ signaling for timing is exemplified by the altered kinetic profiles of Ca2+ signals in certain diseases and the likely role of basal Ca2+ fluctuations in the perception of time itself.}}, 
pages = {R769--R776}, 
number = {17}, 
volume = {18}, 
journal = {Current Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Current%20Biology-2008-Boulware-Marchant.pdf}, 
year = {2008}
}
@article{2020:Leifer, 
author = {Leifer, Ian and Morone, Flaviano and Reis, Saulo D. S. and Andrade, José S. and Sigman, Mariano and Makse, Hernán A.}, 
title = {{Circuits with broken fibration symmetries perform core logic computations in biological networks}}, 
issn = {1553-7358}, 
doi = {10.1371/journal.pcbi.1007776}, 
url = {http://dx.doi.org/10.1371/journal.pcbi.1007776}, 
pages = {e1007776}, 
number = {6}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Leifer-Makse.pdf}, 
year = {2020}
}
@article{1997:Bunkin, 
author = {Bunkin, F V and Kadomtsev, Boris B and Klimontovich, Yu L and Koroteev, Nikolai I and Landa, Polina S and Maslov, V P and Romanovskii, Yurii M}, 
title = {{In memory of Ruslan Leont'evich Stratonovich}}, 
issn = {1063-7869}, 
doi = {10.1070/pu1997v040n07abeh000259}, 
pages = {751--752}, 
number = {7}, 
volume = {40}, 
journal = {Physics-Uspekhi}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physics-Uspekhi-1997-Bunkin-Romanovskii.pdf}, 
year = {1997}
}
@article{2010:Kunita, 
author = {Kunita, Hiroshi}, 
title = {{Itô’s stochastic calculus: Its surprising power for applications}}, 
issn = {0304-4149}, 
doi = {10.1016/j.spa.2010.01.013}, 
abstract = {{We trace Itô’s early work in the 1940s, concerning stochastic integrals, stochastic differential equations (SDEs) and Itô’s formula. Then we study its developments in the 1960s, combining it with martingale theory. Finally, we review a surprising application of Itô’s formula in mathematical finance in the 1970s. Throughout the paper, we treat Itô’s jump SDEs driven by Brownian motions and Poisson random measures, as well as the well-known continuous SDEs driven by Brownian motions.}}, 
pages = {622--652}, 
number = {5}, 
volume = {120}, 
journal = {Stochastic Processes and their Applications}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Stochastic%20Processes%20and%20their%20Applications-2010-Kunita-Kunita.pdf}, 
year = {2010}
}
@article{1908:Langevin, 
author = {Langevin, Paul}, 
title = {{Sur la théorie du mouvement brownien}}, 
pages = {530--533}, 
number = {146}, 
journal = {C. R. Acad. Sci.}, 
year = {1908}
}
@article{2009:Tkačik, 
author = {Tkačik, Gašper and Walczak, Aleksandra M and Bialek, William}, 
title = {{Optimizing information flow in small genetic networks}}, 
issn = {1539-3755}, 
doi = {10.1103/physreve.80.031920}, 
pmid = {19905159}, 
eprint = {0903.4491}, 
abstract = {{In order to survive, reproduce, and (in multicellular organisms) differentiate, cells must control the concentrations of the myriad different proteins that are encoded in the genome. The precision of this control is limited by the inevitable randomness of individual molecular events. Here we explore how cells can maximize their control power in the presence of these physical limits; formally, we solve the theoretical problem of maximizing the information transferred from inputs to outputs when the number of available molecules is held fixed. We start with the simplest version of the problem, in which a single transcription factor protein controls the readout of one or more genes by binding to DNA. We further simplify by assuming that this regulatory network operates in steady state, that the noise is small relative to the available dynamic range, and that the target genes do not interact. Even in this simple limit, we find a surprisingly rich set of optimal solutions. Importantly, for each locally optimal regulatory network, all parameters are determined once the physical constraints on the number of available molecules are specified. Although we are solving an oversimplified version of the problem facing real cells, we see parallels between the structure of these optimal solutions and the behavior of actual genetic regulatory networks. Subsequent papers will discuss more complete versions of the problem.}}, 
pages = {031920}, 
number = {3}, 
volume = {80}, 
journal = {Physical Review E}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2009-Tkačik-Bialek.pdf}, 
year = {2009}
}
@article{2011:Hallatschek, 
author = {Hallatschek, Oskar}, 
title = {{Noise Driven Evolutionary Waves}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1002005}, 
pmid = {21423714}, 
pmcid = {PMC3053316}, 
abstract = {{Adaptation in spatially extended populations entails the propagation of evolutionary novelties across habitat ranges. Driven by natural selection, beneficial mutations sweep through the population in a “wave of advance”. The standard model for these traveling waves, due to R. Fisher and A. Kolmogorov, plays an important role in many scientific areas besides evolution, such as ecology, epidemiology, chemical kinetics, and recently even in particle physics. Here, we extend the Fisher–Kolmogorov model to account for mutations that confer an increase in the density of the population, for instance as a result of an improved metabolic efficiency. We show that these mutations invade by the action of random genetic drift, even if the mutations are slightly deleterious. The ensuing class of noise-driven waves are characterized by a wave speed that decreases with increasing population sizes, contrary to conventional Fisher–Kolmogorov waves. When a trade-off exists between density and growth rate, an evolutionary optimal population density can be predicted. Our simulations and analytical results show that genetic drift in conjunction with spatial structure promotes the economical use of limited resources. The simplicity of our model, which lacks any complex interactions between individuals, suggests that noise-induced pattern formation may arise in many complex biological systems including evolution.}}, 
pages = {e1002005}, 
number = {3}, 
volume = {7}, 
journal = {PLoS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLoS%20Computational%20Biology-2011-Hallatschek-Hallatschek.pdf}, 
year = {2011}
}
@article{1990:Parsons, 
author = {Parsons, P A}, 
title = {{Fluctuating Asymmetry: An Epigenetic Measure of Stress}}, 
issn = {1464-7931}, 
doi = {10.1111/j.1469-185x.1990.tb01186.x}, 
pmid = {2190634}, 
abstract = {{(1) Fluctuating asymmetry (FA) is a useful trait for monitoring stress in the laboratory and in natural environments.
(2) Both genomic and environmental changes can increase FA which represents a deterioration in developmental homeostasis apparent in adult morphology. Genetic perturbations include intense directional selection and certain specific genes. Environmental perturbations include temperature extremes in particular, protein deprivation, audiogenic stress, and exposure to pollutants.
(3) There is a negative association between FA and heterozygosity in a range of taxa especially fish, a result consistent with FA being a measure of fitness.
(4) Scattered reports on non-experimental populations are consistent with experiments under controlled laboratory conditions. FA tends to increase as habitats become ecologically marginal; this includes exposure to environmental toxicants.
(5) In our own species, FA of an increasing range of traits has been related to both environmental and genomic stress.
(6) Domestication increases FA of the strength of homologous long bones of vertebrate species due to a relaxation of natural selection.
(7) FA levels are paralleled by the incidence of skeletal abnormalities in stressful environments.
(8) Increased FA is a reflection of poorer developmental homeostasis at the molecular, chromosomal and epigenetic levels.}}, 
pages = {131--145}, 
number = {2}, 
volume = {65}, 
journal = {Biological Reviews}, 
year = {1990}
}
@article{2008:Faisal, 
author = {Faisal, A Aldo and Selen, Luc P J and Wolpert, Daniel M}, 
title = {{Noise in the nervous system}}, 
issn = {1471-003X}, 
doi = {10.1038/nrn2258}, 
pmid = {18319728}, 
pmcid = {PMC2631351}, 
abstract = {{Noise--random disturbances of signals--poses a fundamental problem for information processing and affects all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-to-trial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.}}, 
pages = {292--303}, 
number = {4}, 
volume = {9}, 
journal = {Nature Reviews Neuroscience}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Nature%20Reviews%20Neuroscience-2008-Faisal-Wolpert.pdf}, 
year = {2008}
}
@article{1995:Mainen, 
author = {Mainen, Z and Sejnowski, T}, 
title = {{Reliability of spike timing in neocortical neurons}}, 
issn = {0036-8075}, 
doi = {10.1126/science.7770778}, 
pmid = {7770778}, 
pages = {1503--1506}, 
number = {5216}, 
volume = {268}, 
journal = {Science}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Science-1995-Mainen-Sejnowski.pdf}, 
year = {1995}
}
@article{2018:Timme, 
author = {Timme, Nicholas M and Lapish, Christopher}, 
title = {{A Tutorial for Information Theory in Neuroscience}}, 
issn = {2373-2822}, 
doi = {10.1523/eneuro.0052-18.2018}, 
pmid = {30211307}, 
abstract = {{Understanding how neural systems integrate, encode, and compute information is central to understanding brain function. Frequently, data from neuroscience experiments are multivariate, the interactions between the variables are nonlinear, and the landscape of hypothesized or possible interactions between variables is extremely broad. Information theory is well suited to address these types of data, as it possesses multivariate analysis tools, it can be applied to many different types of data, it can capture nonlinear interactions, and it does not require assumptions about the structure of the underlying data (i.e., it is model independent). In this article, we walk through the mathematics of information theory along with common logistical problems associated with data type, data binning, data quantity requirements, bias, and significance testing. Next, we analyze models inspired by canonical neuroscience experiments to improve understanding and demonstrate the strengths of information theory analyses. To facilitate the use of information theory analyses, and an understanding of how these analyses are implemented, we also provide a free MATLAB software package that can be applied to a wide range of data from neuroscience experiments, as well as from other fields of study.}}, 
pages = {ENEURO.0052--18.2018}, 
number = {3}, 
volume = {5}, 
journal = {eneuro}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/eneuro-2018-Timme-Lapish.pdf}, 
year = {2018}
}
@article{2002:Elowitz, 
author = {Elowitz, Michael B. and Levine, Arnold J. and Siggia, Eric D. and Swain, Peter S.}, 
title = {{Stochastic Gene Expression in a Single Cell}}, 
issn = {0036-8075}, 
doi = {10.1126/science.1070919}, 
pmid = {12183631}, 
abstract = {{Clonal populations of cells exhibit substantial phenotypic variation. Such heterogeneity can be essential for many biological processes and is conjectured to arise from stochasticity, or noise, in gene expression. We constructed strains of Escherichia coli that enable detection of noise and discrimination between the two mechanisms by which it is generated. Both stochasticity inherent in the biochemical process of gene expression (intrinsic noise) and fluctuations in other cellular components (extrinsic noise) contribute substantially to overall variation. Transcription rate, regulatory dynamics, and genetic factors control the amplitude of noise. These results establish a quantitative foundation for modeling noise in genetic networks and reveal how low intracellular copy numbers of molecules can fundamentally limit the precision of gene regulation.}}, 
pages = {1183--1186}, 
number = {5584}, 
volume = {297}, 
journal = {Science}, 
year = {2002}
}
@article{2009:Simpson, 
author = {Simpson, Michael L and Cox, Chris D and Allen, Michael S and McCollum, James M and Dar, Roy D and Karig, David K and Cooke, John F}, 
title = {{Noise in biological circuits}}, 
issn = {1939-5116}, 
doi = {10.1002/wnan.22}, 
pmid = {20049792}, 
abstract = {{Noise biology focuses on the sources, processing, and biological consequences of the inherent stochastic fluctuations in molecular transitions or interactions that control cellular behavior. These fluctuations are especially pronounced in small systems where the magnitudes of the fluctuations approach or exceed the mean value of the molecular population. Noise biology is an essential component of nanomedicine where the communication of information is across a boundary that separates small synthetic and biological systems that are bound by their size to reside in environments of large fluctuations. Here we review the fundamentals of the computational, analytical, and experimental approaches to noise biology. We review results that show that the competition between the benefits of low noise and those of low population has resulted in the evolution of genetic system architectures that produce an uneven distribution of stochasticity across the molecular components of cells and, in some cases, use noise to drive biological function. We review the exact and approximate approaches to gene circuit noise analysis and simulation, and review many of the key experimental results obtained using flow cytometry and time‐lapse fluorescent microscopy. In addition, we consider the probative value of noise with a discussion of using measured noise properties to elucidate the structure and function of the underlying gene circuit. We conclude with a discussion of the frontiers of and significant future challenges for noise biology. Copyright © 2009 John Wiley \& Sons, Inc. This article is categorized under: Nanotechnology Approaches to Biology > Cells at the Nanoscale}}, 
pages = {214--225}, 
number = {2}, 
volume = {1}, 
journal = {Wiley Interdisciplinary Reviews: Nanomedicine and Nanobiotechnology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Wiley%20Interdisciplinary%20Reviews-%20Nanomedicine%20and%20Nanobiotechnology-2009-Simpson-Cooke.pdf}, 
year = {2009}
}
@article{2010:Nemenman, 
author = {Nemenman, Ilya}, 
title = {{Information theory and adaptation}}, 
eprint = {1011.5466}, 
abstract = {{In this Chapter, we ask questions (1) What is the right way to measure the quality of information processing in a biological system? and (2) What can real-life organisms do in order to improve their performance in information-processing tasks? We then review the body of work that investigates these questions experimentally, computationally, and theoretically in biological domains as diverse as cell biology, population biology, and computational neuroscience}}, 
journal = {arXiv}, 
year = {2010}
}
@article{2014:Levchenko, 
author = {Levchenko, Andre and Nemenman, Ilya}, 
title = {{Cellular noise and information transmission}}, 
issn = {0958-1669}, 
doi = {10.1016/j.copbio.2014.05.002}, 
pmid = {24922112}, 
abstract = {{The technological revolution in biological research, and in particular the use of molecular fluorescent labels, has allowed investigation of heterogeneity of cellular responses to stimuli on the single cell level. Computational, theoretical, and synthetic biology advances have allowed predicting and manipulating this heterogeneity with an exquisite precision previously reserved only for physical sciences. Functionally, this cell-to-cell variability can compromise cellular responses to environmental signals, and it can also enlarge the repertoire of possible cellular responses and hence increase the adaptive nature of cellular behaviors. And yet quantification of the functional importance of this response heterogeneity remained elusive. Recently the mathematical language of information theory has been proposed to address this problem. This opinion reviews the recent advances and discusses the broader implications of using information-theoretic tools to characterize heterogeneity of cellular behaviors.}}, 
pages = {156--164}, 
volume = {28}, 
journal = {Current Opinion in Biotechnology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Current%20Opinion%20in%20Biotechnology-2014-Levchenko-Nemenman.pdf}, 
year = {2014}
}
@article{2001:Gillespie, 
author = {Gillespie, Daniel T}, 
title = {{Approximate accelerated stochastic simulation of chemically reacting systems}}, 
issn = {0021-9606}, 
doi = {10.1063/1.1378322}, 
pages = {1716--1733}, 
number = {4}, 
volume = {115}, 
journal = {The Journal of Chemical Physics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20Chemical%20Physics-2001-Gillespie-Gillespie.pdf}, 
year = {2001}
}
@article{1976:Gillespie, 
author = {Gillespie, Daniel T}, 
title = {{A general method for numerically simulating the stochastic time evolution of coupled chemical reactions}}, 
issn = {0021-9991}, 
doi = {10.1016/0021-9991(76)90041-3}, 
abstract = {{An exact method is presented for numerically calculating, within the framework of the stochastic formulation of chemical kinetics, the time evolution of any spatially homogeneous mixture of molecular species which interreact through a specified set of coupled chemical reaction channels. The method is a compact, computer-oriented, Monte Carlo simulation procedure. It should be particularly useful for modeling the transient behavior of well-mixed gas-phase systems in which many molecular species participate in many highly coupled chemical reactions. For “ordinary” chemical systems in which fluctuations and correlations play no significant role, the method stands as an alternative to the traditional procedure of numerically solving the deterministic reaction rate equations. For nonlinear systems near chemical instabilities, where fluctuations and correlations may invalidate the deterministic equations, the method constitutes an efficient way of numerically examining the predictions of the stochastic master equation. Although fully equivalent to the spatially homogeneous master equation, the numerical simulation algorithm presented here is more directly based on a newly defined entity called “the reaction probability density function.” The purpose of this article is to describe the mechanics of the simulation algorithm, and to establish in a rigorous, a priori manner its physical and mathematical validity; numerical applications to specific chemical systems will be presented in subsequent publications.}}, 
pages = {403--434}, 
number = {4}, 
volume = {22}, 
journal = {Journal of Computational Physics}, 
year = {1976}
}
@article{2020:Richards, 
author = {Richards, David M and Walker, Jamie J and Tabak, Joel}, 
title = {{Ion channel noise shapes the electrical activity of endocrine cells}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007769}, 
pmid = {32251433}, 
abstract = {{Endocrine cells in the pituitary gland typically display either spiking or bursting electrical activity, which is related to the level of hormone secretion. Recent work, which combines mathematical modelling with dynamic clamp experiments, suggests the difference is due to the presence or absence of a few large-conductance potassium channels. Since endocrine cells only contain a handful of these channels, it is likely that stochastic effects play an important role in the pattern of electrical activity. Here, for the first time, we explicitly determine the effect of such noise by studying a mathematical model that includes the realistic noisy opening and closing of ion channels. This allows us to investigate how noise affects the electrical activity, examine the origin of spiking and bursting, and determine which channel types are responsible for the greatest noise. Further, for the first time, we address the role of cell size in endocrine cell electrical activity, finding that larger cells typically display more bursting, while the smallest cells almost always only exhibit spiking behaviour.}}, 
pages = {e1007769}, 
number = {4}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Richards-Tabak.pdf}, 
year = {2020}
}
@article{2020:Einav, 
author = {Einav, Tal and Bloom, Jesse D}, 
title = {{When two are better than one: Modeling the mechanisms of antibody mixtures}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007830}, 
pmid = {32365091}, 
abstract = {{It is difficult to predict how antibodies will behave when mixed together, even after each has been independently characterized. Here, we present a statistical mechanical model for the activity of antibody mixtures that accounts for whether pairs of antibodies bind to distinct or overlapping epitopes. This model requires measuring n individual antibodies and their n ( n - 1 ) 2 pairwise interactions to predict the 2n potential combinations. We apply this model to epidermal growth factor receptor (EGFR) antibodies and find that the activity of antibody mixtures can be predicted without positing synergy at the molecular level. In addition, we demonstrate how the model can be used in reverse, where straightforward experiments measuring the activity of antibody mixtures can be used to infer the molecular interactions between antibodies. Lastly, we generalize this model to analyze engineered multidomain antibodies, where components of different antibodies are tethered together to form novel amalgams, and characterize how well it predicts recently designed influenza antibodies.}}, 
pages = {e1007830}, 
number = {5}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Einav-Bloom.pdf}, 
year = {2020}
}
@article{2020:Harootonian, 
author = {Harootonian, Sevan K and Wilson, Robert C and Hejtmánek, Lukáš and Ziskin, Eli M and Ekstrom, Arne D}, 
title = {{Path integration in large-scale space and with novel geometries: Comparing vector addition and encoding-error models}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007489}, 
pmid = {32379824}, 
abstract = {{Path integration is thought to rely on vestibular and proprioceptive cues yet most studies in humans involve primarily visual input, providing limited insight into their respective contributions. We developed a paradigm involving walking in an omnidirectional treadmill in which participants were guided on two sides of a triangle and then found their back way to origin. In Experiment 1, we tested a range of different triangle types while keeping the distance of the unguided side constant to determine the influence of spatial geometry. Participants overshot the angle they needed to turn and undershot the distance they needed to walk, with no consistent effect of triangle type. In Experiment 2, we manipulated distance while keeping angle constant to determine how path integration operated over both shorter and longer distances. Participants underestimated the distance they needed to walk to the origin, with error increasing as a function of the walked distance. To attempt to account for our findings, we developed configural-based computational models involving vector addition, the second of which included terms for the influence of past trials on the current one. We compared against a previously developed configural model of human path integration, the Encoding-Error model. We found that the vector addition models captured the tendency of participants to under-encode guided sides of the triangles and an influence of past trials on current trials. Together, our findings expand our understanding of body-based contributions to human path integration, further suggesting the value of vector addition models in understanding these important components of human navigation.}}, 
pages = {e1007489}, 
number = {5}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Harootonian-Ekstrom.pdf}, 
year = {2020}
}
@article{2014:Tsimring, 
author = {Tsimring, Lev S}, 
title = {{Noise in biology}}, 
issn = {0034-4885}, 
doi = {10.1088/0034-4885/77/2/026601}, 
pmid = {24444693}, 
pmcid = {PMC4033672}, 
abstract = {{Noise permeates biology on all levels, from the most basic molecular, sub-cellular processes to the dynamics of tissues, organs, organisms and populations. The functional roles of noise in biological processes can vary greatly. Along with standard, entropy-increasing effects of producing random mutations, diversifying phenotypes in isogenic populations, limiting information capacity of signaling relays, it occasionally plays more surprising constructive roles by accelerating the pace of evolution, providing selective advantage in dynamic environments, enhancing intracellular transport of biomolecules and increasing information capacity of signaling pathways. This short review covers the recent progress in understanding mechanisms and effects of fluctuations in biological systems of different scales and the basic approaches to their mathematical modeling.}}, 
pages = {026601}, 
number = {2}, 
volume = {77}, 
journal = {Reports on Progress in Physics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Reports%20on%20Progress%20in%20Physics-2014-Tsimring-Tsimring.pdf}, 
year = {2014}
}
@article{2011:Chan, 
author = {Chan, Joshua C C and Kroese, Dirk P}, 
title = {{Improved cross-entropy method for estimation}}, 
issn = {0960-3174}, 
doi = {10.1007/s11222-011-9275-7}, 
abstract = {{The cross-entropy (CE) method is an adaptive importance sampling procedure that has been successfully applied to a diverse range of complicated simulation problems. However, recent research has shown that in some high-dimensional settings, the likelihood ratio degeneracy problem becomes severe and the importance sampling estimator obtained from the CE algorithm becomes unreliable. We consider a variation of the CE method whose performance does not deteriorate as the dimension of the problem increases. We then illustrate the algorithm via a high-dimensional estimation problem in risk management.}}, 
pages = {1031--1040}, 
number = {5}, 
volume = {22}, 
journal = {Statistics and Computing}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Statistics%20and%20Computing-2011-Chan-Kroese.pdf}, 
year = {2011}
}
@article{1995:Ceperley, 
author = {Ceperley, D M}, 
title = {{Path integrals in the theory of condensed helium}}, 
issn = {0034-6861}, 
doi = {10.1103/revmodphys.67.279}, 
abstract = {{One of Feynman's early applications of path integrals was to superfluid He4. He showed that the thermodynamic properties of Bose systems are exactly equivalent to those of a peculiar type of interacting classical "ring polymer." Using this mapping, one can generalize Monte Carlo simulation techniques commonly used for classical systems to simulate boson systems. In this review, the author introduces this picture of a boson superfluid and shows how superfluidity and Bose condensation manifest themselves. He shows the excellent agreement between simulations and experimental measurements on liquid and solid helium for such quantities as pair correlations, the superfluid density, the energy, and the momentum distribution. Major aspects of computational techniques developed for a boson superfluid are discussed: the construction of more accurate approximate density matrices to reduce the number of points on the path integral, sampling techniques to move through the space of exchanges and paths quickly, and the construction of estimators for various properties such as the energy, the momentum distribution, the superfluid density, and the exchange frequency in a quantum crystal. Finally the path-integral Monte Carlo method is compared to other quantum Monte Carlo methods.}}, 
pages = {279--355}, 
number = {2}, 
volume = {67}, 
journal = {Reviews of Modern Physics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Reviews%20of%20Modern%20Physics-1995-Ceperley-Ceperley.pdf}, 
year = {1995}
}
@article{1970:Hastings, 
author = {Hastings, W K}, 
title = {{Monte Carlo sampling methods using Markov chains and their applications}}, 
issn = {0006-3444}, 
doi = {10.1093/biomet/57.1.97}, 
pages = {97--109}, 
number = {1}, 
volume = {57}, 
journal = {Biometrika}, 
year = {1970}
}
@article{2006:Skilling, 
author = {Skilling, John}, 
title = {{Nested sampling for general Bayesian computation}}, 
issn = {1936-0975}, 
doi = {10.1214/06-ba127}, 
abstract = {{Nested sampling estimates directly how the likelihood function relates to prior mass. The evidence (alternatively the marginal likelihood, marginal density of the data, or the prior predictive) is immediately obtained by summation. It is the prime result of the computation, and is accompanied by an estimate of numerical uncertainty. Samples from the posterior distribution are an optional by-product, obtainable for any temperature. The method relies on sampling within a hard constraint on likelihood value, as opposed to the softened likelihood of annealing methods. Progress depends only on the shape of the "nested" contours of likelihood, and not on the likelihood values. This invariance (over monotonic re-labelling) allows the method to deal with a class of phase-change problems which effectively defeat thermal annealing.}}, 
pages = {833--859}, 
number = {4}, 
volume = {1}, 
journal = {Bayesian Analysis}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Bayesian%20Analysis-2006-Skilling-Skilling.pdf}, 
year = {2006}
}
@article{2019:Rotskoff, 
author = {Rotskoff, Grant M and Vanden-Eijnden, Eric}, 
title = {{Dynamical Computation of the Density of States and Bayes Factors Using Nonequilibrium Importance Sampling}}, 
issn = {0031-9007}, 
doi = {10.1103/physrevlett.122.150602}, 
pmid = {31050526}, 
eprint = {1809.11132}, 
abstract = {{Nonequilibrium sampling is potentially much more versatile than its equilibrium counterpart, but it comes with challenges because the invariant distribution is not typically known when the dynamics breaks detailed balance. Here, we derive a generic importance sampling technique that leverages the statistical power of configurations transported by nonequilibrium trajectories and can be used to compute averages with respect to arbitrary target distributions. As a dissipative reweighting scheme, the method can be viewed in relation to the annealed importance sampling (AIS) method and the related Jarzynski equality. Unlike AIS, our approach gives an unbiased estimator, with a provably lower variance than directly estimating the average of an observable. We also establish a direct relation between a dynamical quantity, the dissipation, and the volume of phase space, from which we can compute quantities such as the density of states and Bayes factors. We illustrate the properties of estimators relying on this sampling technique in the context of density of state calculations, showing that it scales favorable with dimensionality—in particular, we show that it can be used to compute the phase diagram of the mean-field Ising model from a single nonequilibrium trajectory. We also demonstrate the robustness and efficiency of the approach with an application to a Bayesian model comparison problem of the type encountered in astrophysics and machine learning.}}, 
pages = {150602}, 
number = {15}, 
volume = {122}, 
journal = {Physical Review Letters}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20Letters-2019-Rotskoff-Vanden-Eijnden.pdf}, 
year = {2019}
}
@article{1995:Chib, 
author = {Chib, Siddhartha}, 
title = {{Marginal Likelihood from the Gibbs Output}}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1995.10476635}, 
abstract = {{In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.}}, 
pages = {1313--1321}, 
number = {432}, 
volume = {90}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-1995-Chib-Chib.pdf}, 
year = {1995}
}
@book{2009:Gardiner, 
author = {Gardiner, Crispin}, 
title = {{Stochastic Methods}}, 
isbn = {978-3-540-70712-7}, 
series = {Springer Series in Synergetics}, 
number = {13}, 
publisher = {Springer-Verlag}, 
address = {Berlin Heidelberg}, 
edition = {4}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Springer%20Series%20in%20Synergetics-2009-Gardiner-Gardiner.pdf}, 
year = {2009}
}
@article{2001:Han, 
author = {Han, Cong and Carlin, Bradley P}, 
title = {{Markov Chain Monte Carlo Methods for Computing Bayes Factors: A Comparative Review}}, 
issn = {0162-1459}, 
doi = {10.1198/016214501753208780}, 
pages = {1122--1132}, 
number = {455}, 
volume = {96}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-2001-Han-Carlin.pdf}, 
year = {2001}
}
@article{1997:Rubinstein, 
author = {Rubinstein, Reuven Y}, 
title = {{Optimization of computer simulation models with rare events}}, 
issn = {0377-2217}, 
doi = {10.1016/s0377-2217(96)00385-2}, 
abstract = {{Discrete event simulation systems (DESS) are widely used in many diverse areas such as computer-communication networks, flexible manufacturing systems, project evaluation and review techniques (PERT), and flow networks. Because of their complexity, such systems are typically analyzed via Monte Carlo simulation methods. This paper deals with optimization of complex computer simulation models involving rare events. A classic example is to find an optimal (s, S) policy in a multi-item, multicommodity inventory system, when quality standards require the backlog probability to be extremely small. Our approach is based on change of the probability measure techniques, also called likelihood ratio (LR) and importance sampling (IS) methods. Unfortunately, for arbitrary probability measures the LR estimators and the resulting optimal solution often tend to be unstable and may have large variances. Therefore, the choice of the corresponding importance sampling distribution and in particular its parameters in an optimal way is an important task. We consider the case where the IS distribution comes from the same parametric family as the original (true) one and use the stochastic counterpart method to handle simulation based optimization models. More specifically, we use a two-stage procedure: at the first stage we identify (estimate) the optimal parameter vector at the IS distribution, while at the second stage we estimate the optimal solution of the underlying constrained optimization problem. Particular emphasis will be placed on estimation of rare events and on integration of the associated performance function into stochastic optimization programs. Supporting numerical results are provided as well.}}, 
pages = {89--112}, 
number = {1}, 
volume = {99}, 
journal = {European Journal of Operational Research}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/European%20Journal%20of%20Operational%20Research-1997-Rubinstein-Rubinstein.pdf}, 
year = {1997}
}
@article{2007:Harland, 
author = {Harland, Ben and Sun, Sean X}, 
title = {{Path ensembles and path sampling in nonequilibrium stochastic systems}}, 
issn = {0021-9606}, 
doi = {10.1063/1.2775439}, 
pmid = {17867733}, 
abstract = {{Markovian models based on the stochastic master equation are often encountered in single molecule dynamics, reaction networks, and nonequilibrium problems in chemistry, physics, and biology. An efficient and convenient method to simulate these systems is the kinetic Monte Carlo algorithm which generates continuous-time stochastic trajectories. We discuss an alternative simulation method based on sampling of stochastic paths. Utilizing known probabilities of stochastic paths, it is possible to apply Metropolis Monte Carlo in path space to generate a desired ensemble of stochastic paths. The method is a generalization of the path sampling idea to stochastic dynamics, and is especially suited for the analysis of rare paths which are not often produced in the standard kinetic Monte Carlo procedure. Two generic examples are presented to illustrate the methodology.}}, 
pages = {104103}, 
number = {10}, 
volume = {127}, 
journal = {The Journal of Chemical Physics}, 
keywords = {Stochastic Paths}, 
year = {2007}
}
@article{2020:Busto-Moner, 
author = {Busto-Moner, Luis and Morival, Julien and Ren, Honglei and Fahim, Arjang and Reitz, Zachary and Downing, Timothy L and Read, Elizabeth L}, 
title = {{Stochastic modeling reveals kinetic heterogeneity in post-replication DNA methylation}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007195}, 
pmid = {32275652}, 
abstract = {{DNA methylation is a heritable epigenetic modification that plays an essential role in mammalian development. Genomic methylation patterns are dynamically maintained, with DNA methyltransferases mediating inheritance of methyl marks onto nascent DNA over cycles of replication. A recently developed experimental technique employing immunoprecipitation of bromodeoxyuridine labeled nascent DNA followed by bisulfite sequencing (Repli-BS) measures post-replication temporal evolution of cytosine methylation, thus enabling genome-wide monitoring of methylation maintenance. In this work, we combine statistical analysis and stochastic mathematical modeling to analyze Repli-BS data from human embryonic stem cells. We estimate site-specific kinetic rate constants for the restoration of methyl marks on >10 million uniquely mapped cytosines within the CpG (cytosine-phosphate-guanine) dinucleotide context across the genome using Maximum Likelihood Estimation. We find that post-replication remethylation rate constants span approximately two orders of magnitude, with half-lives of per-site recovery of steady-state methylation levels ranging from shorter than ten minutes to five hours and longer. Furthermore, we find that kinetic constants of maintenance methylation are correlated among neighboring CpG sites. Stochastic mathematical modeling provides insight to the biological mechanisms underlying the inference results, suggesting that enzyme processivity and/or collaboration can produce the observed kinetic correlations. Our combined statistical/mathematical modeling approach expands the utility of genomic datasets and disentangles heterogeneity in methylation patterns arising from replication-associated temporal dynamics versus stable cell-to-cell differences.}}, 
pages = {e1007195}, 
number = {4}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Busto-Moner-Read.pdf}, 
year = {2020}
}
@article{2020:Buijsman, 
author = {Buijsman, P and Bolhuis, P G}, 
title = {{Transition path sampling for non-equilibrium dynamics without predefined reaction coordinates}}, 
issn = {0021-9606}, 
doi = {10.1063/1.5130760}, 
pmid = {32007082}, 
abstract = {{We develop two novel transition path sampling (TPS) algorithms for harvesting ensembles of rare event trajectories using non-equilibrium dynamics. These methods have the advantage that no predefined reaction coordinate is needed. Instead, an instantaneous reaction coordinate is based on the current path. Constituting a Monte Carlo random walk in trajectory space, the algorithms can be viewed as bridging between the original TPS methodology and the Rosenbluth based forward flux sampling methodology. We illustrate the new methods on toy models undergoing equilibrium and non-equilibrium dynamics, including an active Brownian particle system. For the latter, we find that transitions between steady states occur via states that are locally ordered but globally disordered.}}, 
pages = {044108}, 
number = {4}, 
volume = {152}, 
journal = {The Journal of Chemical Physics}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20Chemical%20Physics-2020-Buijsman-Bolhuis.pdf}, 
year = {2020}
}
@article{2011:Ciancarini, 
author = {Ciancarini, Paolo and Iorio, Angelo Di and Furini, Luca and Vitali, Fabio}, 
title = {{High-quality pagination for publishing: HIGH-QUALITY PAGINATION FOR PUBLISHING}}, 
issn = {0038-0644}, 
doi = {10.1002/spe.1096}, 
abstract = {{The problem of line breaking consists of finding the best way to split paragraphs into lines. It has been cleverly addressed by the total-fit algorithm exposed by Knuth and Plass in a well-known paper. Similarly, page-breaking algorithms break the content flow of a document into page units. Formatting languages—such as the World Wide Web Consortium standard Extensible Stylesheet Language Formatting Objects (XSL-FO)—allow users to set which content should be kept in the same page and how many isolated lines are acceptable at the beginning/end of each page. The strategies most formatters adopt to meet these requirements, however, are not satisfactory for many publishing contexts as they very often generate unpleasant empty areas. In that case, typographers are required to manually craft the results in order to completely fill pages. This paper presents a page-breaking algorithm that extends the original Knuth and Plass line-breaking approach and produces high-quality documents without unwanted empty areas. The basic idea consists of delaying the definitive choice of breaks in the line-breaking process in order to provide a larger set of alternatives to the actual pagination step. The algorithm also allows users to decide the set of properties to be adjusted for pagination and their variation ranges. An application of the algorithm to XSL-FO is also presented, with an extension of the language that allows users to drive the pagination process. The tool, named FOP+, is a customized version of the open-source Apache Formatting Objects Processor formatter. Copyright © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {733--751}, 
number = {6}, 
volume = {42}, 
journal = {Software: Practice and Experience}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Software-%20Practice%20and%20Experience-2011-Ciancarini-Vitali.pdf}, 
year = {2011}
}
@article{2016:Mittelbach, 
author = {Mittelbach, Frank}, 
title = {{A General Framework for Globally Optimized Pagination}}, 
doi = {10.1145/2960811.2960820}, 
abstract = {{Pagination problems deal with questions around transforming a source text stream into a formatted document by dividing it up into individual columns and pages, including adding auxiliary elements that have some relationship to the source stream data but may allow a certain amount of variation in placement (such as figures or footnotes). Traditionally the pagination problem has been approached by separating it into one of micro-typography (e.g., breaking text into paragraphs, also known as h\&j) and one of macro-typography (e.g., taking a galley of already formatted paragraphs and breaking them into columns and pages) without much interaction between the two. While early solutions for both problem spaces used simple greedy algorithms, Knuth and Plass introduced in the '80s a global-fit algorithm for line breaking that optimizes the breaks across the whole paragraph [1]. This algorithm was implemented in TeX'82 [2] and has since kept its crown as the best available solution for this space. However, for macro-typography there has been no (successful) attempt to provide globally optimized page layout: all systems to date (including TeX) use greedy algorithms for pagination. Various problems in this area have been researched (e.g., [3,4,5,6]) and the literature documents some prototype development. But none of these prototypes have been made widely available to the research community or ever made it into a generally usable and publicly available system. This paper presents a framework for a global-fit algorithm for page breaking based on the ideas of Knuth/Plass. It is implemented in such a way that it is directly usable without additional executables with any modern TeX installation. It therefore can serve as a test bed for future experiments and extensions in this space. At the same time a cleaned-up version of the current prototype has the potential to become a production tool for the huge number of TeX users world-wide. The paper also discusses two already implemented extensions that increase the flexibility of the pagination process: the ability to automatically consider existing flexibility in paragraph length (by considering paragraph variations with different numbers of lines [7]) and the concept of running the columns on a double spread a line long or short. It concludes with a discussion of the overall approach, its inherent limitations and directions for future research. [1] D. E. Knuth and M. F. Plass. Breaking Paragraphs into Lines. Software-Practice and Experience, 11(11):1119-1184, Nov. 1981. [2] D. E. Knuth. TeX: The Program, volume B of Computers and Typesetting. Addison-Wesley, Reading, MA, USA, 1986. [3] A. Brüggemann-Klein, R. Klein, and S. Wohlfeil. Computer science in perspective. Chapter On the Pagination of Complex Documents, pages 49-68. Springer-Verlag New York, Inc., New York, NY, USA, 2003. [4] C. Jacobs, W. Li, and D. H. Salesin. Adaptive document layout via manifold content. In Second International Workshop on Web Document Analysis (wda2003), Liverpool, UK, 2003, 2003. [5] A. Holkner. Global multiple objective line breaking. Master's thesis, School of Computer Science and Information Technology, RMIT University, Melbourne, Victoria, Australia, 2006. [6] P. Ciancarini, A. Di Iorio, L. Furini, and F. Vitali. High-quality pagination for publishing. Software-Practice and Experience, 42(6):733-751, June 2012. [7] T. Hassan and A. Hunter. Knuth-Plass revisited: Flexible line-breaking for automatic document layout. In Proceedings of the 2015 ACM Symposium on Document Engineering, DocEng '15, pages 17-20, New York, NY, USA, 2015.}}, 
pages = {11--20}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/2960811.2960820.pdf}, 
year = {2016}
}
@article{1981:Knuth, 
author = {Knuth, Donald E and Plass, Michael F}, 
title = {{Breaking paragraphs into lines}}, 
issn = {0038-0644}, 
doi = {10.1002/spe.4380111102}, 
abstract = {{This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called ‘boxes’, ‘glue’, and ‘penalties’ provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.}}, 
pages = {1119--1184}, 
number = {11}, 
volume = {11}, 
journal = {Software: Practice and Experience}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Software-%20Practice%20and%20Experience-1981-Knuth-Plass.pdf}, 
year = {1981}
}
@article{2010:Roh, 
author = {Roh, Min K and Gillespie, Dan T and Petzold, Linda R}, 
title = {{State-dependent biasing method for importance sampling in the weighted stochastic simulation algorithm}}, 
issn = {0021-9606}, 
doi = {10.1063/1.3493460}, 
pmid = {21054005}, 
pmcid = {PMC3188645}, 
abstract = {{The weighted stochastic simulation algorithm (wSSA) was developed by Kuwahara and Mura [J. Chem. Phys. 129, 165101 (2008)] to efficiently estimate the probabilities of rare events in discrete stochastic systems. The wSSA uses importance sampling to enhance the statistical accuracy in the estimation of the probability of the rare event. The original algorithm biases the reaction selection step with a fixed importance sampling parameter. In this paper, we introduce a novel method where the biasing parameter is state-dependent. The new method features improved accuracy, efficiency, and robustness.}}, 
pages = {174106}, 
number = {17}, 
volume = {133}, 
journal = {The Journal of Chemical Physics}, 
keywords = {Importance Sampling}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20Chemical%20Physics-2010-Roh-Petzold.pdf}, 
year = {2010}
}
@article{2018:Warne, 
author = {Warne, David J and Baker, Ruth E and Simpson, Matthew J}, 
title = {{Multilevel rejection sampling for approximate Bayesian computation}}, 
issn = {0167-9473}, 
doi = {10.1016/j.csda.2018.02.009}, 
eprint = {1702.03126}, 
abstract = {{ Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions in an approximate Bayesian computation setting. However, without careful consideration of convergence criteria and selection of proposal kernels, such methods can lead to very biased inference or computationally inefficient sampling. In contrast, rejection sampling for approximate Bayesian computation, despite being computationally intensive, results in independent, identically distributed samples from the approximated posterior. An alternative method is proposed for the acceleration of likelihood-free Bayesian inference that applies multilevel Monte Carlo variance reduction techniques directly to rejection sampling. The resulting method retains the accuracy advantages of rejection sampling while significantly improving the computational efficiency.}}, 
pages = {71--86}, 
volume = {124}, 
journal = {Computational Statistics \& Data Analysis}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Computational%20Statistics%20&%20Data%20Analysis-2018-Warne-Simpson.pdf}, 
year = {2018}
}
@article{2016:Warne, 
author = {Warne, David J and Baker, Ruth E and Simpson, Matthew J}, 
title = {{Accelerating computational Bayesian inference for stochastic biochemical reaction network models using multilevel Monte Carlo sampling}}, 
doi = {10.1101/064170}, 
abstract = {{Abstract Investigating the behavior of stochastic models of biochemical reactionnetworks generally relies upon numerical stochastic simulation methods to generate many realizations of the model. For many practical applications, such numerical simulation can be computationally expensive. The statistical inference of reaction rate parameters based on observed data is, however, a significantly greater computational challenge; often relying upon likelihood-free methods such as approximate Bayesian computation, that requirethe generation of millions of individual stochastic realizations. In this study, we investigate a new approach to computational inference, based on multilevel Monte Carlo sampling: we approximate the posterior cumulative distribution function through a combination of model samples taken over a range of acceptance thresholds. We demonstrate this approach using a variety of discrete-state, continuous-time Markov models of biochemical reactionnetworks. Results show that a computational gain over standard rejection schemes of up to an order of magnitude is achievable without significant loss in estimator accuracy. Author Summary We develop a new method to infer the reaction rate parameters for stochastic models of biochemical reaction networks. Standard computational approaches, based on numerical simulations, are often used to estimate parameters. These computational approaches, however, are extremely expensive, potentially requiring millions of simulations. To alleviate this issue, we apply a different method of sampling allowing us to find an optimal trade-off between performance and accuracy. Our approach is approximately one order of magnitude faster than standard methods, without significant loss in accuracy.}}, 
pages = {064170}, 
journal = {bioRxiv}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/bioRxiv-2016-Warne-Simpson.pdf}, 
year = {2016}
}
@article{2020:Frishman, 
author = {Frishman, Anna and Ronceray, Pierre}, 
title = {{Learning Force Fields from Stochastic Trajectories}}, 
doi = {10.1103/physrevx.10.021009}, 
eprint = {1809.09650}, 
abstract = {{When monitoring the dynamics of stochastic systems, such as interacting particles agitated by thermal noise, disentangling deterministic forces from Brownian motion is challenging. Indeed, we show that there is an information-theoretic bound, the capacity of the system when viewed as a communication channel, that limits the rate at which information about the force field can be extracted from a Brownian trajectory. This capacity provides an upper bound to the system’s entropy production rate and quantifies the rate at which the trajectory becomes distinguishable from pure Brownian motion. We propose a practical and principled method, stochastic force inference, that uses this information to approximate force fields and spatially variable diffusion coefficients. It is data efficient, including in high dimensions, robust to experimental noise, and provides a self-consistent estimate of the inference error. In addition to forces, this technique readily permits the evaluation of out-of-equilibrium currents and the corresponding entropy production with a limited amount of data.}}, 
pages = {021009}, 
number = {2}, 
volume = {10}, 
journal = {Physical Review X}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20X-2020-Frishman-Ronceray.pdf}, 
year = {2020}, 
rating = {4}
}
@article{2020:Rahmani, 
author = {Rahmani, Parisa and Peruani, Fernando and Romanczuk, Pawel}, 
title = {{Flocking in complex environments—Attention trade-offs in collective information processing}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007697}, 
pmid = {32251423}, 
abstract = {{The ability of biological and artificial collectives to outperform solitary individuals in a wide variety of tasks depends crucially on the efficient processing of social and environmental information at the level of the collective. Here, we model collective behavior in complex environments with many potentially distracting cues. Counter-intuitively, large-scale coordination in such environments can be maximized by strongly limiting the cognitive capacity of individuals, where due to self-organized dynamics the collective self-isolates from disrupting information. We observe a fundamental trade-off between coordination and collective responsiveness to environmental cues. Our results offer important insights into possible evolutionary trade-offs in collective behavior in biology and suggests novel principles for design of artificial swarms exploiting attentional bottlenecks.}}, 
pages = {e1007697}, 
number = {4}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Rahmani-Romanczuk.pdf}, 
year = {2020}, 
month = {4}
}
@article{2019:Rodríguez-Sánchez, 
author = {Rodríguez-Sánchez, Pablo and Nes, Egbert H van and Scheffer, Marten}, 
title = {{Climbing Escher's stairs: a way to approximate stability landscapes in multidimensional systems}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007788}, 
pmid = {32275714}, 
eprint = {1903.05615}, 
abstract = {{Stability landscapes are useful for understanding the properties of dynamical systems. These landscapes can be calculated from the system's dynamical equations using the physical concept of scalar potential. Unfortunately, for most biological systems with two or more state variables such potentials do not exist. Here we use an analogy with art to provide an accessible explanation of why this happens. Additionally, we introduce a numerical method for decomposing differential equations into two terms: the gradient term that has an associated potential, and the non-gradient term that lacks it. In regions of the state space where the magnitude of the non-gradient term is small compared to the gradient part, we use the gradient term to approximate the potential as quasi-potential. The non-gradient to gradient ratio can be used to estimate the local error introduced by our approximation. Both the algorithm and a ready-to-use implementation in the form of an R package are provided.}}, 
pages = {e1007788}, 
number = {4}, 
volume = {16}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Rodríguez-Sánchez-Scheffer.pdf}, 
year = {2019}, 
month = {4}
}
@article{1994:Gelfand, 
author = {Gelfand, A E and Dey, D K}, 
title = {{Bayesian Model Choice: Asymptotics and Exact Calculations}}, 
issn = {0035-9246}, 
doi = {10.1111/j.2517-6161.1994.tb01996.x}, 
abstract = {{Model determination is a fundamental data analytic task. Here we consider the problem of choosing among a finite (without loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches that we are aware of. Using Laplace approximations we can conveniently assess and compare the asymptotic behaviour of these approaches. Concern regarding the accuracy of these approximations for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fitted with nested non‐linear models enables comparisons between proposals and between exact and asymptotic values.}}, 
pages = {501--514}, 
number = {3}, 
volume = {56}, 
journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20Royal%20Statistical%20Society-%20Series%20B%20(Methodological)-1994-Gelfand-Dey.pdf}, 
year = {1994}
}
@article{2001:Wangg8b, 
author = {Wang, Fugao and Landau, D P}, 
title = {{Determining the density of states for classical statistical models: A random walk algorithm to produce a flat histogram}}, 
issn = {1063-651X}, 
doi = {10.1103/physreve.64.056101}, 
pmid = {11736008}, 
eprint = {cond-mat/0107006}, 
abstract = {{We describe an efficient Monte Carlo algorithm using a random walk in energy space to obtain a very accurate estimate of the density of states for classical statistical models. The density of states is modified at each step when the energy level is visited to produce a flat histogram. By carefully controlling the modification factor, we allow the density of states to converge to the true value very quickly, even for large systems. This algorithm is especially useful for complex systems with a rough landscape since all possible energy levels are visited with the same probability. In this paper, we apply our algorithm to both 1st and 2nd order phase transitions to demonstrate its efficiency and accuracy. We obtained direct simulational estimates for the density of states for two-dimensional ten-state Potts models on lattices up to \$200 \textbackslashtimes 200 \$ and Ising models on lattices up to \$256 \textbackslashtimes 256\$. Applying this approach to a 3D \$\textbackslashpm J\$ spin glass model we estimate the internal energy and entropy at zero temperature; and, using a two-dimensional random walk in energy and order-parameter space, we obtain the (rough) canonical distribution and energy landscape in order-parameter space. Preliminary data suggest that the glass transition temperature is about 1.2 and that better estimates can be obtained with more extensive application of the method.}}, 
pages = {056101}, 
number = {5}, 
volume = {64}, 
journal = {Physical Review E}, 
keywords = {Wang-Landau-Sampling}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2001-Wang-Landau.pdf}, 
year = {2001}
}
@article{2001:Wang, 
author = {Wang, Fugao and Landau, D P}, 
title = {{Efficient, Multiple-Range Random Walk Algorithm to Calculate the Density of States}}, 
issn = {0031-9007}, 
doi = {10.1103/physrevlett.86.2050}, 
pmid = {11289852}, 
eprint = {cond-mat/0011174}, 
abstract = {{We present a new Monte Carlo algorithm that produces results of high accuracy with reduced simulational effort. Independent random walks are performed (concurrently or serially) in different, restricted ranges of energy, and the resultant density of states is modified continuously to produce locally flat histograms. This method permits us to directly access the free energy and entropy, is independent of temperature, and is efficient for the study of both 1st order and 2nd order phase transitions. It should also be useful for the study of complex systems with a rough energy landscape.}}, 
pages = {2050--2053}, 
number = {10}, 
volume = {86}, 
journal = {Physical Review Letters}, 
keywords = {Wang-Landau-Sampling}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20Letters-2001-Wang-Landau.pdf}, 
year = {2001}
}
@article{1996:Helbing, 
author = {Helbing, Dirk and Molini, Rolf}, 
title = {{Occurrence probabilities of stochastic paths}}, 
issn = {0375-9601}, 
doi = {10.1016/0375-9601(96)00010-2}, 
eprint = {cond-mat/9805324}, 
abstract = {{An analytical formula for the occurrence probability of Markovian stochastic paths with repeatedly visited and/or equal departure rates is derived. This formula is essential for an efficient investigation of the trajectories belonging to random walk models and for a numerical evaluation of the “contracted path integral solution” of the discrete master equation [Phys. Lett. A 195 (1994) 128].}}, 
pages = {130--137}, 
number = {3}, 
volume = {212}, 
journal = {Physics Letters A}, 
keywords = {Stochastic Paths}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physics%20Letters%20A-1996-Helbing-Molini.pdf}, 
year = {1996}
}
@article{2017:Weber, 
author = {Weber, Markus F and Frey, Erwin}, 
title = {{Master equations and the theory of stochastic path integrals}}, 
issn = {0034-4885}, 
doi = {10.1088/1361-6633/aa5ae2}, 
pmid = {28306551}, 
eprint = {1609.02849}, 
abstract = {{This review provides a pedagogic and self-contained introduction to master equations and to their representation by path integrals. Since the 1930s, master equations have served as a fundamental tool to understand the role of fluctuations in complex biological, chemical, and physical systems. Despite their simple appearance, analyses of master equations most often rely on low-noise approximations such as the Kramers-Moyal or the system size expansion, or require ad-hoc closure schemes for the derivation of low-order moment equations. We focus on numerical and analytical methods going beyond the low-noise limit and provide a unified framework for the study of master equations. After deriving the forward and backward master equations from the Chapman-Kolmogorov equation, we show how the two master equations can be cast into either of four linear partial differential equations (PDEs). Three of these PDEs are discussed in detail. The first PDE governs the time evolution of a generalized probability generating function whose basis depends on the stochastic process under consideration. Spectral methods, WKB approximations, and a variational approach have been proposed for the analysis of the PDE. The second PDE is novel and is obeyed by a distribution that is marginalized over an initial state. It proves useful for the computation of mean extinction times. The third PDE describes the time evolution of a 'generating functional', which generalizes the so-called Poisson representation. Subsequently, the solutions of the PDEs are expressed in terms of two path integrals: a 'forward' and a 'backward' path integral. Combined with inverse transformations, one obtains two distinct path integral representations of the conditional probability distribution solving the master equations. We exemplify both path integrals in analysing elementary chemical reactions. Moreover, we show how a well-known path integral representation of averaged observables can be recovered from them. Upon expanding the forward and the backward path integrals around stationary paths, we then discuss and extend a recent method for the computation of rare event probabilities. Besides, we also derive path integral representations for processes with continuous state spaces whose forward and backward master equations admit Kramers-Moyal expansions. A truncation of the backward expansion at the level of a diffusion approximation recovers a classic path integral representation of the (backward) Fokker-Planck equation. One can rewrite this path integral in terms of an Onsager-Machlup function and, for purely diffusive Brownian motion, it simplifies to the path integral of Wiener. To make this review accessible to a broad community, we have used the language of probability theory rather than quantum (field) theory and do not assume any knowledge of the latter. The probabilistic structures underpinning various technical concepts, such as coherent states, the Doi-shift, and normal-ordered observables, are thereby made explicit.}}, 
pages = {046601}, 
number = {4}, 
volume = {80}, 
journal = {Reports on Progress in Physics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Reports%20on%20Progress%20in%20Physics-2017-Weber-Frey.pdf}, 
year = {2017}, 
month = {4}, 
rating = {5}
}
@incollection{2014:Held, 
author = {Held, Leonhard and Bové, Daniel Sabanés}, 
title = {{Applied Statistical Inference}}, 
booktitle = {Applied Statistical Inference}, 
isbn = {9783642378867}, 
abstract = {{This chapter describes numerical methods for Bayesian inference in non-conjugate settings. Standard numerical techniques and the Laplace approximation provide ways to numerically compute posterior characteristics of interest. Monte Carlo methods, including Monte Carlo integration, rejection and importance sampling as well as Markov chain Monte Carlo are described. Finally, numerical computation of the marginal likelihood, necessary for Bayesian model selection, is discussed. Exercises are given at the end.}}, 
pages = {247--289}, 
publisher = {Springer Berlin Heidelberg}, 
address = {Berlin, Heidelberg}, 
year = {2014}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/2014_Book_AppliedStatisticalInference.pdf}
}
@misc{2004:Chan, 
author = {Chan, Ping-Shing}, 
editor = {Kotz, S. and Read, C. B. and Balakrishnan, N. and Vidakovic, B. and Johnson, N. L.}, 
title = {{Encyclopedia of Statistical Sciences}}, 
journal = {Encyclopedia of Statistical Sciences}, 
isbn = {9780471667193}, 
pages = {4363--4366}, 
volume = {7}, 
publisher = {American Cancer Society}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/loggamma.pdf}, 
year = {2004}
}
@article{1991:Gelfand, 
author = {Gelfand, Alan E and Smith, Adrian F M}, 
title = {{Gibbs sampling for marginal posterior expectations}}, 
issn = {0361-0926}, 
doi = {10.1080/03610929108830595}, 
pages = {1747--1766}, 
number = {5-6}, 
volume = {20}, 
journal = {Communications in Statistics - Theory and Methods}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Communications%20in%20Statistics%20-%20Theory%20and%20Methods-1991-Gelfand-Smith.pdf}, 
year = {1991}
}
@article{1990:Gelfand, 
author = {Gelfand, Alan E and Smith, Adrian F M}, 
title = {{Sampling-Based Approaches to Calculating Marginal Densities}}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1990.10476213}, 
abstract = {{Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated.}}, 
pages = {398--409}, 
number = {410}, 
volume = {85}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-1990-Gelfand-Smith.pdf}, 
year = {1990}
}
@article{2003:Paninski, 
author = {Paninski, Liam}, 
title = {{Estimation of Entropy and Mutual Information}}, 
issn = {0899-7667}, 
doi = {10.1162/089976603321780272}, 
abstract = {{We present some new results on the nonparametric estimation of entropy and mutual information. First, we use an exact local expansion of the entropy function to prove almost sure consistency and central limit theorems for three of the most commonly used discretized information estimators. The setup is related to Grenander's method of sieves and places no assumptions on the underlying probability measure generating the data. Second, we prove a converse to these consistency theorems, demonstrating that a misapplication of the most common estimation techniques leads to an arbitrarily poor estimate of the true information, even given unlimited data. This inconsistency theorem leads to an analytical approximation of the bias, valid in surprisingly small sample regimes and more accurate than the usual formula of Miller and Madow over a large region of parameter space. The two most practical implications of these results are negative: (1) information estimates in a certain data regime are likely contaminated by bias, even if bias-corrected estimators are used, and (2) confidence intervals calculated by standard techniques drastically underestimate the error of the most common estimation methods. Finally, we note a very useful connection between the bias of entropy estimators and a certain polynomial approximation problem. By casting bias calculation problems in this approximation theory framework, we obtain the best possible generalization of known asymptotic bias results. More interesting, this framework leads to an estimator with some nice properties: the estimator comes equipped with rigorous bounds on the maximum error over all possible underlying probability distributions, and this maximum error turns out to be surprisingly small. We demonstrate the application of this new estimator on both real and simulated data.}}, 
pages = {1191--1253}, 
number = {6}, 
volume = {15}, 
journal = {Neural Computation}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Neural%20Computation-2003-Paninski-Paninski.pdf}, 
year = {2003}
}
@article{2001:Crooks, 
author = {Crooks, Gavin E and Chandler, David}, 
title = {{Efficient transition path sampling for nonequilibrium stochastic dynamics}}, 
issn = {1063-651X}, 
doi = {10.1103/physreve.64.026109}, 
pmid = {11497653}, 
abstract = {{The transition path sampling methodology is adapted to the efficient sampling of large fluctuations in nonequilibrium systems evolving according to Langevin’s equations of motion. This technique is used to simulate the behavior of the bistable Maier-Stein system at noise intensities much lower than those previously possible.}}, 
pages = {026109}, 
number = {2}, 
volume = {64}, 
journal = {Physical Review E}, 
keywords = {Transition Path Sampling,Stochastic Paths,Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2001-Crooks-Chandler.pdf}, 
year = {2001}
}
@article{2020:Uda, 
author = {Uda, Shinsuke}, 
title = {{Application of information theory in systems biology.}}, 
issn = {1867-2450}, 
doi = {10.1007/s12551-020-00665-w}, 
pmid = {32144740}, 
abstract = {{Over recent years, new light has been shed on aspects of information processing in cells. The quantification of information, as described by Shannon's information theory, is a basic and powerful tool that can be applied to various fields, such as communication, statistics, and computer science, as well as to information processing within cells. It has also been used to infer the network structure of molecular species. However, the difficulty of obtaining sufficient sample sizes and the computational burden associated with the high-dimensional data often encountered in biology can result in bottlenecks in the application of information theory to systems biology. This article provides an overview of the application of information theory to systems biology, discussing the associated bottlenecks and reviewing recent work.}}, 
pages = {377--384}, 
number = {2}, 
volume = {12}, 
journal = {Biophysical reviews}, 
keywords = {Information Theory}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Biophysical%20reviews-2020-Uda-Uda.pdf}, 
year = {2020}, 
rating = {5}
}
@article{2019:Cepeda-Humerez, 
author = {Cepeda-Humerez, Sarah Anhala and Ruess, Jakob and Tkačik, Gašper}, 
title = {{Estimating information in time-varying signals.}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007290}, 
pmid = {31479447}, 
abstract = {{Across diverse biological systems-ranging from neural networks to intracellular signaling and genetic regulatory networks-the information about changes in the environment is frequently encoded in the full temporal dynamics of the network nodes. A pressing data-analysis challenge has thus been to efficiently estimate the amount of information that these dynamics convey from experimental data. Here we develop and evaluate decoding-based estimation methods to lower bound the mutual information about a finite set of inputs, encoded in single-cell high-dimensional time series data. For biological reaction networks governed by the chemical Master equation, we derive model-based information approximations and analytical upper bounds, against which we benchmark our proposed model-free decoding estimators. In contrast to the frequently-used k-nearest-neighbor estimator, decoding-based estimators robustly extract a large fraction of the available information from high-dimensional trajectories with a realistic number of data samples. We apply these estimators to previously published data on Erk and Ca2+ signaling in mammalian cells and to yeast stress-response, and find that substantial amount of information about environmental state can be encoded by non-trivial response statistics even in stationary signals. We argue that these single-cell, decoding-based information estimates, rather than the commonly-used tests for significant differences between selected population response statistics, provide a proper and unbiased measure for the performance of biological signaling networks.}}, 
pages = {e1007290}, 
number = {9}, 
volume = {15}, 
journal = {PLoS computational biology}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLoS%20computational%20biology-2019-Cepeda-Humerez-Tkačik.pdf}, 
year = {2019}
}
@article{2019:Duso, 
author = {Duso, Lorenzo and Zechner, Christoph}, 
title = {{Path mutual information for a class of biochemical reaction networks}}, 
eprint = {1904.01988}, 
abstract = {{Living cells encode and transmit information in the temporal dynamics of biochemical components. Gaining a detailed understanding of the input-output relationship in biological systems therefore requires quantitative measures that capture the interdependence between complete time trajectories of biochemical components. Mutual information provides such a measure but its calculation in the context of stochastic reaction networks is associated with mathematical challenges. Here we show how to estimate the mutual information between complete paths of two molecular species that interact with each other through biochemical reactions. We demonstrate our approach using three simple case studies.}}, 
journal = {arXiv}, 
keywords = {Recommended by PRtW,Stochastic Paths}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Duso-Zechner.pdf}, 
year = {2019}
}
@article{2007:Ziv, 
author = {Ziv, Etay and Nemenman, Ilya and Wiggins, Chris H}, 
title = {{Optimal Signal Processing in Small Stochastic Biochemical Networks}}, 
doi = {10.1371/journal.pone.0001077}, 
pmid = {17957259}, 
pmcid = {PMC2034356}, 
eprint = {q-bio/0612041}, 
abstract = {{We quantify the influence of the topology of a transcriptional regulatory network on its ability to process environmental signals. By posing the problem in terms of information theory, we may do this without specifying the function performed by the network. Specifically, we study the maximum mutual information between the input (chemical) signal and the output (genetic) response attainable by the network in the context of an analytic model of particle number fluctuations. We perform this analysis for all biochemical circuits, including various feedback loops, that can be built out of 3 chemical species, each under the control of one regulator. We find that a generic network, constrained to low molecule numbers and reasonable response times, can transduce more information than a simple binary switch and, in fact, manages to achieve close to the optimal information transmission fidelity. These high-information solutions are robust to tenfold changes in most of the networks' biochemical parameters; moreover they are easier to achieve in networks containing cycles with an odd number of negative regulators (overall negative feedback) due to their decreased molecular noise (a result which we derive analytically). Finally, we demonstrate that a single circuit can support multiple high-information solutions. These findings suggest a potential resolution of the "cross-talk" dilemma as well as the previously unexplained observation that transcription factors which undergo proteolysis are more likely to be auto-repressive.}}, 
pages = {e1077}, 
number = {10}, 
volume = {2}, 
journal = {PLoS ONE}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLoS%20ONE-2007-Ziv-Wiggins.pdf}, 
year = {2007}
}
@article{2019:Blanchard, 
author = {Blanchard, Pierre and Higham, Desmond J and Higham, Nicholas J}, 
title = {{Accurate Computation of the Log-Sum-Exp and Softmax Functions}}, 
eprint = {1909.03469}, 
abstract = {{Evaluating the log-sum-exp function or the softmax function is a key step in many modern data science algorithms, notably in inference and classification. Because of the exponentials that these functions contain, the evaluation is prone to overflow and underflow, especially in low precision arithmetic. Software implementations commonly use alternative formulas that avoid overflow and reduce the chance of harmful underflow, employing a shift or another rewriting. Although mathematically equivalent, these variants behave differently in floating-point arithmetic. We give rounding error analyses of different evaluation algorithms and interpret the error bounds using condition numbers for the functions. We conclude, based on the analysis and numerical experiments, that the shifted formulas are of similar accuracy to the unshifted ones and that the shifted softmax formula is typically more accurate than a division-free variant.}}, 
journal = {arXiv}, 
keywords = {special functions}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Blanchard-Higham.pdf}, 
year = {2019}
}
@article{2005:Samad, 
author = {Samad, Hana El and Khammash, Mustafa and Petzold, Linda and Gillespie, Dan}, 
title = {{Stochastic modelling of gene regulatory networks}}, 
issn = {1049-8923}, 
doi = {10.1002/rnc.1018}, 
abstract = {{Gene regulatory networks are dynamic and stochastic in nature, and exhibit exquisite feedback and feedforward control loops that regulate their biological function at different levels. Modelling of such networks poses new challenges due, in part, to the small number of molecules involved and the stochastic nature of their interactions. In this article, we motivate the stochastic modelling of genetic networks and demonstrate the approach using several examples. We discuss the mathematics of molecular noise models including the chemical master equation, the chemical Langevin equation, and the reaction rate equation. We then discuss numerical simulation approaches using the stochastic simulation algorithm (SSA) and its variants. Finally, we present some recent advances for dealing with stochastic stiffness, which is the key challenge in efficiently simulating stochastic chemical kinetics. Copyright © 2005 John Wiley \& Sons, Ltd.}}, 
pages = {691--711}, 
number = {15}, 
volume = {15}, 
journal = {International Journal of Robust and Nonlinear Control}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/International%20Journal%20of%20Robust%20and%20Nonlinear%20Control-2005-Samad-Gillespie.pdf}, 
year = {2005}
}
@article{2012:Warren, 
author = {Warren, Patrick B and Allen, Rosalind J}, 
title = {{Steady-state parameter sensitivity in stochastic modeling via trajectory reweighting.}}, 
issn = {0021-9606}, 
doi = {10.1063/1.3690092}, 
pmid = {22423827}, 
eprint = {1202.4704}, 
abstract = {{Parameter sensitivity analysis is a powerful tool in the building and analysis of biochemical network models. For stochastic simulations, parameter sensitivity analysis can be computationally expensive, requiring multiple simulations for perturbed values of the parameters. Here, we use trajectory reweighting to derive a method for computing sensitivity coefficients in stochastic simulations without explicitly perturbing the parameter values, avoiding the need for repeated simulations. The method allows the simultaneous computation of multiple sensitivity coefficients. Our approach recovers results originally obtained by application of the Girsanov measure transform in the general theory of stochastic processes [A. Plyasunov and A. P. Arkin, J. Comput. Phys. 221, 724 (2007)]. We build on these results to show how the method can be used to compute steady-state sensitivity coefficients from a single simulation run, and we present various efficiency improvements. For models of biochemical signaling networks, the method has a particularly simple implementation. We demonstrate its application to a signaling network showing stochastic focussing and to a bistable genetic switch, and present exact results for models with linear propensity functions.}}, 
pages = {104106}, 
number = {10}, 
volume = {136}, 
journal = {The Journal of chemical physics}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20chemical%20physics-2012-Warren-Allen.pdf}, 
year = {2012}
}
@article{2014:Chan, 
author = {Chan, Joshua C C and Eisenstat, Eric}, 
title = {{Marginal Likelihood Estimation with the Cross-Entropy Method}}, 
issn = {0747-4938}, 
doi = {10.1080/07474938.2014.944474}, 
abstract = {{We consider an adaptive importance sampling approach to estimating the marginal likelihood, a quantity that is fundamental in Bayesian model comparison and Bayesian model averaging. This approach is motivated by the difficulty of obtaining an accurate estimate through existing algorithms that use Markov chain Monte Carlo (MCMC) draws, where the draws are typically costly to obtain and highly correlated in high-dimensional settings. In contrast, we use the cross-entropy (CE) method, a versatile adaptive Monte Carlo algorithm originally developed for rare-event simulation. The main advantage of the importance sampling approach is that random samples can be obtained from some convenient density with little additional costs. As we are generating independent draws instead of correlated MCMC draws, the increase in simulation effort is much smaller should one wish to reduce the numerical standard error of the estimator. Moreover, the importance density derived via the CE method is grounded in information theory, and therefore, is in a well-defined sense optimal. We demonstrate the utility of the proposed approach by two empirical applications involving women's labor market participation and U.S. macroeconomic time series. In both applications, the proposed CE method compares favorably to existing estimators.}}, 
pages = {256--285}, 
number = {3}, 
volume = {34}, 
journal = {Econometric Reviews}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Econometric%20Reviews-2014-Chan-Eisenstat.pdf}, 
year = {2014}
}
@article{2010:Tostevin, 
author = {Tostevin, Filipe and Wolde, Pieter Rein ten}, 
title = {{Mutual information in time-varying biochemical systems}}, 
issn = {1539-3755}, 
doi = {10.1103/physreve.81.061917}, 
pmid = {20866450}, 
eprint = {1002.4273}, 
abstract = {{Cells must continuously sense and respond to time-varying environmental stimuli. These signals are transmitted and processed by biochemical signalling networks. However, the biochemical reactions making up these networks are intrinsically noisy, which limits the reliability of intracellular signalling. Here we use information theory to characterise the reliability of transmission of time-varying signals through elementary biochemical reactions in the presence of noise. We calculate the mutual information for both instantaneous measurements and trajectories of biochemical systems for a Gaussian model. Our results indicate that the same network can have radically different characteristics for the transmission of instantaneous signals and trajectories. For trajectories, the ability of a network to respond to changes in the input signal is determined by the timing of reaction events, and is independent of the correlation time of the output of the network. We also study how reliably signals on different time-scales can be transmitted by considering the frequency-dependent coherence and gain-to-noise ratio. We find that a detector that does not consume the ligand molecule upon detection can more reliably transmit slowly varying signals, while an absorbing detector can more reliably transmit rapidly varying signals. Furthermore, we find that while one reaction may more reliably transmit information than another when considered in isolation, when placed within a signalling cascade the relative performance of the two reactions can be reversed. This means that optimising signal transmission at a single level of a signalling cascade can reduce signalling performance for the cascade as a whole.}}, 
pages = {061917}, 
number = {6}, 
volume = {81}, 
journal = {Physical Review E}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2010-Tostevin-Wolde.pdf}, 
year = {2010}
}
@article{2019:Meijers, 
author = {Meijers, Matthijs and Ito, Sosuke and Wolde, Pieter Rein ten}, 
title = {{The behaviour of information flow near criticality}}, 
eprint = {1906.00787}, 
abstract = {{Recent experiments have indicated that many biological systems self-organise near their critical point, which hints at a common design principle. While it has been suggested that information transmission is optimized near the critical point, it remains unclear how information transmission depends on the dynamics of the input signal, the distance over which the information needs to be transmitted, and the distance to the critical point. Here we employ stochastic simulations of a driven 2D Ising system and study the instantaneous mutual information and the information transmission rate between a driven input spin and an output spin. The instantaneous mutual information varies non-monotonically with the temperature, but increases monotonically with the correlation time of the input signal. In contrast, the information transmission rate exhibits a maximum as a function of the input correlation time. Moreover, there exists an optimal temperature that maximizes this maximum information transmission rate. It arises from a tradeoff between the necessity to respond fast to changes in the input so that more information per unit amount of time can be transmitted, and the need to respond to reliably. The optimal temperature lies above the critical point, but moves towards it as the distance between the input and output spin is increased.}}, 
journal = {arXiv}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Meijers-Wolde.pdf}, 
year = {2019}
}
@article{1993:Gelfand, 
author = {Gelfand, Alan E and Dey, D K}, 
title = {{Bayesian Model Choice: Asymptotics and Exact Calculations}}, 
issn = {0035-9246}, 
doi = {10.21236/ada269067}, 
abstract = {{Model determination is a fundamental data analytic task. Here we consider the problem of choosing amongst a finite (with loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches we are aware of. Using Laplace approximations we can conveniently assess and compare asymptotic behavior of these approaches. Concern regarding the accuracy of these approximation for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fit with nested non linear models enables comparison between proposals and between exact and asymptotic values.}}, 
pages = {501--514}, 
number = {3}, 
volume = {56}, 
journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20Royal%20Statistical%20Society-%20Series%20B%20(Methodological)-1993-Gelfand-Dey.pdf}, 
year = {1993}
}
@article{2004:Nemenman, 
author = {Nemenman, Ilya and Bialek, William and Steveninck, Rob de Ruyter van}, 
title = {{Entropy and information in neural spike trains: Progress on the sampling problem}}, 
issn = {1539-3755}, 
doi = {10.1103/physreve.69.056111}, 
pmid = {15244887}, 
eprint = {physics/0306063}, 
abstract = {{The major problem in information theoretic analysis of neural responses and other biological data is the reliable estimation of entropy--like quantities from small samples. We apply a recently introduced Bayesian entropy estimator to synthetic data inspired by experiments, and to real experimental spike trains. The estimator performs admirably even very deep in the undersampled regime, where other techniques fail. This opens new possibilities for the information theoretic analysis of experiments, and may be of general interest as an example of learning from limited data.}}, 
pages = {056111}, 
number = {5}, 
volume = {69}, 
journal = {Physical Review E}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2004-Nemenman-Steveninck.pdf}, 
year = {2004}
}
@phdthesis{1993:Chan, 
title = {{A Statistical Study of Log-Gamma Distribution}}, 
author = {Chan, Shing Ping and Balakrishnan, Narayanaswamy}, 
keywords = {special functions}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Unknown-1993-Chan-Balakrishnan.pdf}, 
year = {1993}
}
@article{1996:Unknown, 
author = {Meng, Xiao-Li and Wong, Wing Hung}, 
title = {{Simulating Ratios of Normalizing Constants Via a Simple Identity: A Theoretical Exploration}}, 
url = {http://www.jstor.org/stable/24306045}, 
abstract = {{Let pi(w),i = 1,2, be two densities with common support where each density is known up to a normalizing constant: pi(w) = qi(w)/ci. We have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a computational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: (c₁/c₂) = ((E₂[q₁(w)α (w)])/(E₁[q₂(w)α (w)])) Here Ei denotes the expectation with respect to pi (i = 1,2), and α is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of α. Using a simple but informative example, we demonstrate that with sensible (not necessarily optimal) choices of α, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to α = 1/q2. We also introduce several generalizations of this identity for handling more complicated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.}}, 
pages = {831---860}, 
number = {4}, 
volume = {6}, 
journal = {Statistica Sinica}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Statistica%20Sinica-1996-Unknown-Wong.pdf}, 
year = {1996}
}
@article{1998:Gelman, 
author = {Gelman, Andrew and Meng, Xiao-Li}, 
title = {{Simulating normalizing constants: from importance sampling to bridge sampling to path sampling}}, 
issn = {0883-4237}, 
doi = {10.1214/ss/1028905934}, 
abstract = {{Computing (ratios of) normalizing constants of probability models is a fundamental computational problem for many statistical and scientific studies. Monte Carlo simulation is an effective technique, especially with complex and high-dimensional models. This paper aims to bring to the attention of general statistical audiences of some effective methods originating from theoretical physics and at the same time to explore these methods from a more statistical perspective, through establishing theoretical connections and illustrating their uses with statistical problems. We show that the acceptance ratio method and thermodynamic integration are natural generalizations of importance sampling, which is most familiar to statistical audiences. The former generalizes importance sampling through the use of a single "bridge" density and is thus a case of bridge sampling in the sense of Meng and Wong. Thermodynamic integration, which is also known in the numerical analysis literature as Ogata's method for high-dimensional integration, corresponds to the use of infinitely many and continuously connected bridges (and thus a "path"). Our path sampling formulation offers more flexibility and thus potential efficiency to thermodynamic integration, and the search of optimal paths turns out to have close connections with the Jeffreys prior density and the Rao and Hellinger distances between two densities. We provide an informative theoretical example as well as two empirical examples (involving 17- to 70-dimensional integrations) to illustrate the potential and implementation of path sampling. We also discuss some open problems.}}, 
pages = {163--185}, 
number = {2}, 
volume = {13}, 
journal = {Statistical Science}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Statistical%20Science-1998-Gelman-Meng.pdf}, 
year = {1998}, 
rating = {5}
}
@article{1994:Tierney, 
author = {Tierney, Luke}, 
title = {{Markov Chains for Exploring Posterior Distributions}}, 
issn = {0090-5364}, 
doi = {10.1214/aos/1176325750}, 
url = {http://www.jstor.org/stable/2242477}, 
abstract = {{Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.}}, 
pages = {1701--1728}, 
number = {4}, 
volume = {22}, 
journal = {The Annals of Statistics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Annals%20of%20Statistics-1994-Tierney-Tierney.pdf}, 
year = {1994}
}
@article{1990:Carlin, 
author = {Carlin, Bradley P and Gelfand, Alan E and Smith, Adrian F}, 
title = {{Hierarchical Bayesian Analysis of Change Point Problems}}, 
doi = {10.21236/ada228179}, 
abstract = {{A general approach to hierarchical Bayes change point models is presented. In particular desired marginal posterior densities are obtained utilizing the Gibbs sampler, an iterative Monte Carlo method. This approach avoids sophisticated analytic and numerical high dimensional integration procedures. We include application to changing regressions, changing Poisson processes, and changing Markov chains. Within these contexts we handle several previously inaccessible problems. (kr)}}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Hierarchical%20Bayesian%20Analysis%20of%20Change%20Point%20Problems.pdf}, 
year = {1990}
}
@article{2020:Rumyantsev, 
author = {Rumyantsev, Oleg I and Lecoq, Jérôme A and Hernandez, Oscar and Zhang, Yanping and Savall, Joan and Chrapkiewicz, Radosław and Li, Jane and Zeng, Hongkui and Ganguli, Surya and Schnitzer, Mark J}, 
title = {{Fundamental bounds on the fidelity of sensory cortical coding.}}, 
issn = {0028-0836}, 
doi = {10.1038/s41586-020-2130-2}, 
pmid = {32238928}, 
abstract = {{How the brain processes information accurately despite stochastic neural activity is a longstanding question1. For instance, perception is fundamentally limited by the information that the brain can extract from the noisy dynamics of sensory neurons. Seminal experiments2,3 suggest that correlated noise in sensory cortical neural ensembles is what limits their coding accuracy4-6, although how correlated noise affects neural codes remains debated7-11. Recent theoretical work proposes that how a neural ensemble's sensory tuning properties relate statistically to its correlated noise patterns is a greater determinant of coding accuracy than is absolute noise strength12-14. However, without simultaneous recordings from thousands of cortical neurons with shared sensory inputs, it is unknown whether correlated noise limits coding fidelity. Here we present a 16-beam, two-photon microscope to monitor activity across the mouse primary visual cortex, along with analyses to quantify the information conveyed by large neural ensembles. We found that, in the visual cortex, correlated noise constrained signalling for ensembles with 800-1,300 neurons. Several noise components of the ensemble dynamics grew proportionally to the ensemble size and the encoded visual signals, revealing the predicted information-limiting correlations12-14. Notably, visual signals were perpendicular to the largest noise mode, which therefore did not limit coding fidelity. The information-limiting noise modes were approximately ten times smaller and concordant with mouse visual acuity15. Therefore, cortical design principles appear to enhance coding accuracy by restricting around 90\% of noise fluctuations to modes that do not limit signalling fidelity, whereas much weaker correlated noise modes inherently bound sensory discrimination.}}, 
pages = {100--105}, 
number = {7801}, 
volume = {580}, 
journal = {Nature}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Nature-2020-Rumyantsev-Schnitzer.pdf}, 
year = {2020}
}
@article{1997:Diciccio, 
author = {Diciccio, Thomas J and Kass, Robert E and Raftery, Adrian and Wasserman, Larry}, 
title = {{Computing Bayes Factors by Combining Simulation and Asymptotic Approximations}}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1997.10474045}, 
abstract = {{The Bayes factor is a ratio of two posterior normalizing constants, which may be difficult to compute. We compare several methods of estimating Bayes factors when it is possible to simulate observations from the posterior distributions, via Markov chain Monte Carlo or other techniques. The methods that we study are all easily applied without consideration of special features of the problem, provided that each posterior distribution is well behaved in the sense of having a single dominant mode. We consider a simulated version of Laplace's method, a simulated version of Bartlett correction, importance sampling, and a reciprocal importance sampling technique. We also introduce local volume corrections for each of these. In addition, we apply the bridge sampling method of Meng and Wong. We find that a simulated version of Laplace's method, with local volume correction, furnishes an accurate approximation that is especially useful when likelihood function evaluations are costly. A simple bridge sampling technique in conjunction with Laplace's method often achieves an order of magnitude improvement in accuracy.}}, 
pages = {903--915}, 
number = {439}, 
volume = {92}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-1997-Diciccio-Wasserman.pdf}, 
year = {1997}
}
@article{1994:Newton, 
author = {Newton, Michael A and Raftery, Adrian E}, 
title = {{Approximate Bayesian Inference with the Weighted Likelihood Bootstrap}}, 
issn = {0035-9246}, 
doi = {10.1111/j.2517-6161.1994.tb01956.x}, 
abstract = {{We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling‐importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR‐adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second‐order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models.}}, 
pages = {3--26}, 
number = {1}, 
volume = {56}, 
journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20Royal%20Statistical%20Society-%20Series%20B%20(Methodological)-1994-Newton-Raftery.pdf}, 
year = {1994}
}
@article{1987:Dijk, 
author = {Dijk, Herman K Van and Hop, J Peter and Louter, Adri S}, 
title = {{An Algorithm for the Computation of Posterior Moments and Densities Using Simple Importance Sampling}}, 
issn = {0039-0526}, 
doi = {10.2307/2348500}, 
abstract = {{In earlier work (van Dijk, 1984, Chapter 3) one of the authors discussed the use of Monte Carlo integration methods for the computation of the multivariate integrals that are defined in the posterior moments and densities of the parameters of interest of econometric models. In the present paper we describe the computational steps of one Monte Carlo method, which is known in the literature as importance sampling. Further, a set of standard programs is available, which may be used for the implementation of a simple case of importance sampling. The computer programs have been written in FORTRAN 77.}}, 
pages = {83}, 
number = {2/3}, 
volume = {36}, 
journal = {The Statistician}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Statistician-1987-Dijk-Louter.pdf}, 
year = {1987}
}
@techreport{1991:Müller, 
author = {Müller, Peter}, 
title = {{A Generic Approach to Posterior Integration and Gibbs Sampling}}, 
institution = {Purdue University}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Unknown-1991-Müller-Müller.pdf}, 
year = {1991}
}
@article{2017:Ouldridgeaxe, 
author = {Ouldridge, Thomas E and Govern, Christopher C and Wolde, Pieter Rein ten}, 
title = {{Thermodynamics of Computational Copying in Biochemical Systems}}, 
doi = {10.1103/physrevx.7.021004}, 
eprint = {1503.00909}, 
abstract = {{Living cells use readout molecules to record the state of receptor proteins, similar to measurements or copies in typical computational devices. But is this analogy rigorous? Can cells be optimally efficient, and if not, why? We show that, as in computation, a canonical biochemical readout network generates correlations; extracting no work from these correlations sets a lower bound on dissipation. For general input, the biochemical network cannot reach this bound, even with arbitrarily slow reactions or weak thermodynamic driving. It faces an accuracy-dissipation trade-off that is qualitatively distinct from and worse than implied by the bound, and more complex steady-state copy processes cannot perform better. Nonetheless, the cost remains close to the thermodynamic bound unless accuracy is extremely high. Additionally, we show that biomolecular reactions could be used in thermodynamically optimal devices under exogenous manipulation of chemical fuels, suggesting an experimental system for testing computational thermodynamics.}}, 
pages = {021004}, 
number = {2}, 
volume = {7}, 
journal = {Physical Review X}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20X-2017-Ouldridge-Wolde.pdf}, 
year = {2017}
}
@article{2014:Perrakis, 
author = {Perrakis, Konstantinos and Ntzoufras, Ioannis and Tsionas, Efthymios G}, 
title = {{On the use of marginal posteriors in marginal likelihood estimation via importance sampling}}, 
issn = {0167-9473}, 
doi = {10.1016/j.csda.2014.03.004}, 
eprint = {1311.0674}, 
abstract = {{The efficiency of a marginal likelihood estimator where the product of the marginal posterior distributions is used as an importance sampling function is investigated. The approach is generally applicable to multi-block parameter vector settings, does not require additional Markov Chain Monte Carlo (MCMC) sampling and is not dependent on the type of MCMC scheme used to sample from the posterior. The proposed approach is applied to normal regression models, finite normal mixtures and longitudinal Poisson models, and leads to accurate marginal likelihood estimates.}}, 
pages = {54--69}, 
volume = {77}, 
journal = {Computational Statistics \& Data Analysis}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Computational%20Statistics%20&%20Data%20Analysis-2014-Perrakis-Tsionas.pdf}, 
year = {2014}
}
@article{2017:Ouldridge, 
author = {Ouldridge, Thomas E and Wolde, Pieter Rein Ten}, 
title = {{Fundamental Costs in the Production and Destruction of Persistent Polymer Copies.}}, 
issn = {0031-9007}, 
doi = {10.1103/physrevlett.118.158103}, 
pmid = {28452507}, 
eprint = {1609.05554}, 
abstract = {{Producing a polymer copy of a polymer template is central to biology, and effective copies must persist after template separation. We show that this separation has three fundamental thermodynamic effects. First, polymer-template interactions do not contribute to overall reaction thermodynamics and hence cannot drive the process. Second, the equilibrium state of the copied polymer is template independent and so additional work is required to provide specificity. Finally, the mixing of copies from distinct templates makes correlations between template and copy sequences unexploitable, combining with copying inaccuracy to reduce the free energy stored in a polymer ensemble. These basic principles set limits on the underlying costs and resource requirements, and suggest design principles, for autonomous copying and replication in biological and synthetic systems.}}, 
pages = {158103}, 
number = {15}, 
volume = {118}, 
journal = {Physical review letters}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20review%20letters-2017-Ouldridge-Wolde.pdf}, 
year = {2017}
}
@article{1995:Wolpert, 
author = {Wolpert, David H and Wolf, David R}, 
title = {{Estimating functions of probability distributions from a finite set of samples}}, 
issn = {1063-651X}, 
doi = {10.1103/physreve.52.6841}, 
pmid = {9964199}, 
abstract = {{This paper addresses the problem of estimating a function of a probability distribution from a finite set of samples of that distribution. A Bayesian analysis of this problem is presented, the optimal properties of the Bayes estimators are discussed, and as an example of the formalism, closed form expressions for the Bayes estimators for the moments of the Shannon entropy function are derived. Then numerical results are presented that compare the Bayes estimator to the frequency-counts estimator for the Shannon entropy. We also present the closed form estimators, all derived elsewhere, for the mutual information, χ2 covariance, and some other statistics. (c) 1995 The American Physical Society}}, 
pages = {6841--6854}, 
number = {6}, 
volume = {52}, 
journal = {Physical Review E}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-1995-Wolpert-Wolf.pdf}, 
year = {1995}
}
@article{1948:Shannon, 
author = {Shannon, C E}, 
title = {{A Mathematical Theory of Communication}}, 
issn = {0005-8580}, 
doi = {10.1002/j.1538-7305.1948.tb01338.x}, 
abstract = {{The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.}}, 
pages = {379--423}, 
number = {3}, 
volume = {27}, 
journal = {Bell System Technical Journal}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Bell%20System%20Technical%20Journal-1948-Shannon-Shannon.pdf}, 
year = {1948}
}
@techreport{Beirlant, 
author = {Beirlant, Jan and Dudewicz, Edward J. and Györfi, László and Meulen, Edward C. Van der}, 
title = {{Nonparametric entropy estimation: An overview}}, 
pages = {17---39}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Unknown-null-Beirlant-Meulen.pdf}
}