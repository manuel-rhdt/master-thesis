@article{1995:Chib, 
author = {Chib, Siddhartha}, 
title = {{Marginal Likelihood from the Gibbs Output}}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1995.10476635}, 
abstract = {{In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.}}, 
pages = {1313--1321}, 
number = {432}, 
volume = {90}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-1995-Chib-Chib.pdf}, 
year = {1995}
}
@article{1970:Hastings, 
author = {Hastings, W K}, 
title = {{Monte Carlo sampling methods using Markov chains and their applications}}, 
issn = {0006-3444}, 
doi = {10.1093/biomet/57.1.97}, 
pages = {97--109}, 
number = {1}, 
volume = {57}, 
journal = {Biometrika}, 
year = {1970}
}
@article{2006:Skilling, 
author = {Skilling, John}, 
title = {{Nested sampling for general Bayesian computation}}, 
issn = {1936-0975}, 
doi = {10.1214/06-ba127}, 
abstract = {{Nested sampling estimates directly how the likelihood function relates to prior mass. The evidence (alternatively the marginal likelihood, marginal density of the data, or the prior predictive) is immediately obtained by summation. It is the prime result of the computation, and is accompanied by an estimate of numerical uncertainty. Samples from the posterior distribution are an optional by-product, obtainable for any temperature. The method relies on sampling within a hard constraint on likelihood value, as opposed to the softened likelihood of annealing methods. Progress depends only on the shape of the "nested" contours of likelihood, and not on the likelihood values. This invariance (over monotonic re-labelling) allows the method to deal with a class of phase-change problems which effectively defeat thermal annealing.}}, 
pages = {833--859}, 
number = {4}, 
volume = {1}, 
journal = {Bayesian Analysis}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Bayesian%20Analysis-2006-Skilling-Skilling.pdf}, 
year = {2006}
}
@article{2019:Rotskoff, 
author = {Rotskoff, Grant M and Vanden-Eijnden, Eric}, 
title = {{Dynamical Computation of the Density of States and Bayes Factors Using Nonequilibrium Importance Sampling}}, 
issn = {0031-9007}, 
doi = {10.1103/physrevlett.122.150602}, 
pmid = {31050526}, 
eprint = {1809.11132}, 
abstract = {{Nonequilibrium sampling is potentially much more versatile than its equilibrium counterpart, but it comes with challenges because the invariant distribution is not typically known when the dynamics breaks detailed balance. Here, we derive a generic importance sampling technique that leverages the statistical power of configurations transported by nonequilibrium trajectories and can be used to compute averages with respect to arbitrary target distributions. As a dissipative reweighting scheme, the method can be viewed in relation to the annealed importance sampling (AIS) method and the related Jarzynski equality. Unlike AIS, our approach gives an unbiased estimator, with a provably lower variance than directly estimating the average of an observable. We also establish a direct relation between a dynamical quantity, the dissipation, and the volume of phase space, from which we can compute quantities such as the density of states and Bayes factors. We illustrate the properties of estimators relying on this sampling technique in the context of density of state calculations, showing that it scales favorable with dimensionality—in particular, we show that it can be used to compute the phase diagram of the mean-field Ising model from a single nonequilibrium trajectory. We also demonstrate the robustness and efficiency of the approach with an application to a Bayesian model comparison problem of the type encountered in astrophysics and machine learning.}}, 
pages = {150602}, 
number = {15}, 
volume = {122}, 
journal = {Physical Review Letters}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20Letters-2019-Rotskoff-Vanden-Eijnden.pdf}, 
year = {2019}
}
@book{2009:Gardiner, 
author = {Gardiner, Crispin}, 
title = {{Stochastic Methods}}, 
isbn = {978-3-540-70712-7}, 
series = {Springer Series in Synergetics}, 
number = {13}, 
publisher = {Springer-Verlag}, 
address = {Berlin Heidelberg}, 
edition = {4}, 
year = {2009}
}
@article{2001:Han, 
author = {Han, Cong and Carlin, Bradley P}, 
title = {{Markov Chain Monte Carlo Methods for Computing Bayes Factors: A Comparative Review}}, 
issn = {0162-1459}, 
doi = {10.1198/016214501753208780}, 
pages = {1122--1132}, 
number = {455}, 
volume = {96}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-2001-Han-Carlin.pdf}, 
year = {2001}
}
@article{1997:Rubinstein, 
author = {Rubinstein, Reuven Y}, 
title = {{Optimization of computer simulation models with rare events}}, 
issn = {0377-2217}, 
doi = {10.1016/s0377-2217(96)00385-2}, 
abstract = {{Discrete event simulation systems (DESS) are widely used in many diverse areas such as computer-communication networks, flexible manufacturing systems, project evaluation and review techniques (PERT), and flow networks. Because of their complexity, such systems are typically analyzed via Monte Carlo simulation methods. This paper deals with optimization of complex computer simulation models involving rare events. A classic example is to find an optimal (s, S) policy in a multi-item, multicommodity inventory system, when quality standards require the backlog probability to be extremely small. Our approach is based on change of the probability measure techniques, also called likelihood ratio (LR) and importance sampling (IS) methods. Unfortunately, for arbitrary probability measures the LR estimators and the resulting optimal solution often tend to be unstable and may have large variances. Therefore, the choice of the corresponding importance sampling distribution and in particular its parameters in an optimal way is an important task. We consider the case where the IS distribution comes from the same parametric family as the original (true) one and use the stochastic counterpart method to handle simulation based optimization models. More specifically, we use a two-stage procedure: at the first stage we identify (estimate) the optimal parameter vector at the IS distribution, while at the second stage we estimate the optimal solution of the underlying constrained optimization problem. Particular emphasis will be placed on estimation of rare events and on integration of the associated performance function into stochastic optimization programs. Supporting numerical results are provided as well.}}, 
pages = {89--112}, 
number = {1}, 
volume = {99}, 
journal = {European Journal of Operational Research}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/European%20Journal%20of%20Operational%20Research-1997-Rubinstein-Rubinstein.pdf}, 
year = {1997}
}
@article{2007:Harland, 
author = {Harland, Ben and Sun, Sean X}, 
title = {{Path ensembles and path sampling in nonequilibrium stochastic systems}}, 
issn = {0021-9606}, 
doi = {10.1063/1.2775439}, 
pmid = {17867733}, 
abstract = {{Markovian models based on the stochastic master equation are often encountered in single molecule dynamics, reaction networks, and nonequilibrium problems in chemistry, physics, and biology. An efficient and convenient method to simulate these systems is the kinetic Monte Carlo algorithm which generates continuous-time stochastic trajectories. We discuss an alternative simulation method based on sampling of stochastic paths. Utilizing known probabilities of stochastic paths, it is possible to apply Metropolis Monte Carlo in path space to generate a desired ensemble of stochastic paths. The method is a generalization of the path sampling idea to stochastic dynamics, and is especially suited for the analysis of rare paths which are not often produced in the standard kinetic Monte Carlo procedure. Two generic examples are presented to illustrate the methodology.}}, 
pages = {104103}, 
number = {10}, 
volume = {127}, 
journal = {The Journal of Chemical Physics}, 
keywords = {Stochastic Paths}, 
year = {2007}
}
@article{2020:Busto-Moner, 
author = {Busto-Moner, Luis and Morival, Julien and Ren, Honglei and Fahim, Arjang and Reitz, Zachary and Downing, Timothy L and Read, Elizabeth L}, 
title = {{Stochastic modeling reveals kinetic heterogeneity in post-replication DNA methylation}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007195}, 
pmid = {32275652}, 
abstract = {{DNA methylation is a heritable epigenetic modification that plays an essential role in mammalian development. Genomic methylation patterns are dynamically maintained, with DNA methyltransferases mediating inheritance of methyl marks onto nascent DNA over cycles of replication. A recently developed experimental technique employing immunoprecipitation of bromodeoxyuridine labeled nascent DNA followed by bisulfite sequencing (Repli-BS) measures post-replication temporal evolution of cytosine methylation, thus enabling genome-wide monitoring of methylation maintenance. In this work, we combine statistical analysis and stochastic mathematical modeling to analyze Repli-BS data from human embryonic stem cells. We estimate site-specific kinetic rate constants for the restoration of methyl marks on >10 million uniquely mapped cytosines within the CpG (cytosine-phosphate-guanine) dinucleotide context across the genome using Maximum Likelihood Estimation. We find that post-replication remethylation rate constants span approximately two orders of magnitude, with half-lives of per-site recovery of steady-state methylation levels ranging from shorter than ten minutes to five hours and longer. Furthermore, we find that kinetic constants of maintenance methylation are correlated among neighboring CpG sites. Stochastic mathematical modeling provides insight to the biological mechanisms underlying the inference results, suggesting that enzyme processivity and/or collaboration can produce the observed kinetic correlations. Our combined statistical/mathematical modeling approach expands the utility of genomic datasets and disentangles heterogeneity in methylation patterns arising from replication-associated temporal dynamics versus stable cell-to-cell differences.}}, 
pages = {e1007195}, 
number = {4}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Busto-Moner-Read.pdf}, 
year = {2020}
}
@article{2020:Buijsman, 
author = {Buijsman, P and Bolhuis, P G}, 
title = {{Transition path sampling for non-equilibrium dynamics without predefined reaction coordinates}}, 
issn = {0021-9606}, 
doi = {10.1063/1.5130760}, 
pmid = {32007082}, 
abstract = {{We develop two novel transition path sampling (TPS) algorithms for harvesting ensembles of rare event trajectories using non-equilibrium dynamics. These methods have the advantage that no predefined reaction coordinate is needed. Instead, an instantaneous reaction coordinate is based on the current path. Constituting a Monte Carlo random walk in trajectory space, the algorithms can be viewed as bridging between the original TPS methodology and the Rosenbluth based forward flux sampling methodology. We illustrate the new methods on toy models undergoing equilibrium and non-equilibrium dynamics, including an active Brownian particle system. For the latter, we find that transitions between steady states occur via states that are locally ordered but globally disordered.}}, 
pages = {044108}, 
number = {4}, 
volume = {152}, 
journal = {The Journal of Chemical Physics}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20Chemical%20Physics-2020-Buijsman-Bolhuis.pdf}, 
year = {2020}
}
@article{2011:Ciancarini, 
author = {Ciancarini, Paolo and Iorio, Angelo Di and Furini, Luca and Vitali, Fabio}, 
title = {{High-quality pagination for publishing: HIGH-QUALITY PAGINATION FOR PUBLISHING}}, 
issn = {0038-0644}, 
doi = {10.1002/spe.1096}, 
abstract = {{The problem of line breaking consists of finding the best way to split paragraphs into lines. It has been cleverly addressed by the total-fit algorithm exposed by Knuth and Plass in a well-known paper. Similarly, page-breaking algorithms break the content flow of a document into page units. Formatting languages—such as the World Wide Web Consortium standard Extensible Stylesheet Language Formatting Objects (XSL-FO)—allow users to set which content should be kept in the same page and how many isolated lines are acceptable at the beginning/end of each page. The strategies most formatters adopt to meet these requirements, however, are not satisfactory for many publishing contexts as they very often generate unpleasant empty areas. In that case, typographers are required to manually craft the results in order to completely fill pages. This paper presents a page-breaking algorithm that extends the original Knuth and Plass line-breaking approach and produces high-quality documents without unwanted empty areas. The basic idea consists of delaying the definitive choice of breaks in the line-breaking process in order to provide a larger set of alternatives to the actual pagination step. The algorithm also allows users to decide the set of properties to be adjusted for pagination and their variation ranges. An application of the algorithm to XSL-FO is also presented, with an extension of the language that allows users to drive the pagination process. The tool, named FOP+, is a customized version of the open-source Apache Formatting Objects Processor formatter. Copyright © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {733--751}, 
number = {6}, 
volume = {42}, 
journal = {Software: Practice and Experience}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Software-%20Practice%20and%20Experience-2011-Ciancarini-Vitali.pdf}, 
year = {2011}
}
@article{2016:Mittelbach, 
author = {Mittelbach, Frank}, 
title = {{A General Framework for Globally Optimized Pagination}}, 
doi = {10.1145/2960811.2960820}, 
abstract = {{Pagination problems deal with questions around transforming a source text stream into a formatted document by dividing it up into individual columns and pages, including adding auxiliary elements that have some relationship to the source stream data but may allow a certain amount of variation in placement (such as figures or footnotes). Traditionally the pagination problem has been approached by separating it into one of micro-typography (e.g., breaking text into paragraphs, also known as h\&j) and one of macro-typography (e.g., taking a galley of already formatted paragraphs and breaking them into columns and pages) without much interaction between the two. While early solutions for both problem spaces used simple greedy algorithms, Knuth and Plass introduced in the '80s a global-fit algorithm for line breaking that optimizes the breaks across the whole paragraph [1]. This algorithm was implemented in TeX'82 [2] and has since kept its crown as the best available solution for this space. However, for macro-typography there has been no (successful) attempt to provide globally optimized page layout: all systems to date (including TeX) use greedy algorithms for pagination. Various problems in this area have been researched (e.g., [3,4,5,6]) and the literature documents some prototype development. But none of these prototypes have been made widely available to the research community or ever made it into a generally usable and publicly available system. This paper presents a framework for a global-fit algorithm for page breaking based on the ideas of Knuth/Plass. It is implemented in such a way that it is directly usable without additional executables with any modern TeX installation. It therefore can serve as a test bed for future experiments and extensions in this space. At the same time a cleaned-up version of the current prototype has the potential to become a production tool for the huge number of TeX users world-wide. The paper also discusses two already implemented extensions that increase the flexibility of the pagination process: the ability to automatically consider existing flexibility in paragraph length (by considering paragraph variations with different numbers of lines [7]) and the concept of running the columns on a double spread a line long or short. It concludes with a discussion of the overall approach, its inherent limitations and directions for future research. [1] D. E. Knuth and M. F. Plass. Breaking Paragraphs into Lines. Software-Practice and Experience, 11(11):1119-1184, Nov. 1981. [2] D. E. Knuth. TeX: The Program, volume B of Computers and Typesetting. Addison-Wesley, Reading, MA, USA, 1986. [3] A. Brüggemann-Klein, R. Klein, and S. Wohlfeil. Computer science in perspective. Chapter On the Pagination of Complex Documents, pages 49-68. Springer-Verlag New York, Inc., New York, NY, USA, 2003. [4] C. Jacobs, W. Li, and D. H. Salesin. Adaptive document layout via manifold content. In Second International Workshop on Web Document Analysis (wda2003), Liverpool, UK, 2003, 2003. [5] A. Holkner. Global multiple objective line breaking. Master's thesis, School of Computer Science and Information Technology, RMIT University, Melbourne, Victoria, Australia, 2006. [6] P. Ciancarini, A. Di Iorio, L. Furini, and F. Vitali. High-quality pagination for publishing. Software-Practice and Experience, 42(6):733-751, June 2012. [7] T. Hassan and A. Hunter. Knuth-Plass revisited: Flexible line-breaking for automatic document layout. In Proceedings of the 2015 ACM Symposium on Document Engineering, DocEng '15, pages 17-20, New York, NY, USA, 2015.}}, 
pages = {11--20}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/2960811.2960820.pdf}, 
year = {2016}
}
@article{1981:Knuth, 
author = {Knuth, Donald E and Plass, Michael F}, 
title = {{Breaking paragraphs into lines}}, 
issn = {0038-0644}, 
doi = {10.1002/spe.4380111102}, 
abstract = {{This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called ‘boxes’, ‘glue’, and ‘penalties’ provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.}}, 
pages = {1119--1184}, 
number = {11}, 
volume = {11}, 
journal = {Software: Practice and Experience}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Software-%20Practice%20and%20Experience-1981-Knuth-Plass.pdf}, 
year = {1981}
}
@article{2010:Roh, 
author = {Roh, Min K and Gillespie, Dan T and Petzold, Linda R}, 
title = {{State-dependent biasing method for importance sampling in the weighted stochastic simulation algorithm}}, 
issn = {0021-9606}, 
doi = {10.1063/1.3493460}, 
pmid = {21054005}, 
pmcid = {PMC3188645}, 
abstract = {{The weighted stochastic simulation algorithm (wSSA) was developed by Kuwahara and Mura [J. Chem. Phys. 129, 165101 (2008)] to efficiently estimate the probabilities of rare events in discrete stochastic systems. The wSSA uses importance sampling to enhance the statistical accuracy in the estimation of the probability of the rare event. The original algorithm biases the reaction selection step with a fixed importance sampling parameter. In this paper, we introduce a novel method where the biasing parameter is state-dependent. The new method features improved accuracy, efficiency, and robustness.}}, 
pages = {174106}, 
number = {17}, 
volume = {133}, 
journal = {The Journal of Chemical Physics}, 
keywords = {Importance Sampling}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20Chemical%20Physics-2010-Roh-Petzold.pdf}, 
year = {2010}
}
@article{2018:Warne, 
author = {Warne, David J and Baker, Ruth E and Simpson, Matthew J}, 
title = {{Multilevel rejection sampling for approximate Bayesian computation}}, 
issn = {0167-9473}, 
doi = {10.1016/j.csda.2018.02.009}, 
eprint = {1702.03126}, 
abstract = {{ Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions in an approximate Bayesian computation setting. However, without careful consideration of convergence criteria and selection of proposal kernels, such methods can lead to very biased inference or computationally inefficient sampling. In contrast, rejection sampling for approximate Bayesian computation, despite being computationally intensive, results in independent, identically distributed samples from the approximated posterior. An alternative method is proposed for the acceleration of likelihood-free Bayesian inference that applies multilevel Monte Carlo variance reduction techniques directly to rejection sampling. The resulting method retains the accuracy advantages of rejection sampling while significantly improving the computational efficiency.}}, 
pages = {71--86}, 
volume = {124}, 
journal = {Computational Statistics \& Data Analysis}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Computational%20Statistics%20&%20Data%20Analysis-2018-Warne-Simpson.pdf}, 
year = {2018}
}
@article{2016:Warne, 
author = {Warne, David J and Baker, Ruth E and Simpson, Matthew J}, 
title = {{Accelerating computational Bayesian inference for stochastic biochemical reaction network models using multilevel Monte Carlo sampling}}, 
doi = {10.1101/064170}, 
abstract = {{Abstract Investigating the behavior of stochastic models of biochemical reactionnetworks generally relies upon numerical stochastic simulation methods to generate many realizations of the model. For many practical applications, such numerical simulation can be computationally expensive. The statistical inference of reaction rate parameters based on observed data is, however, a significantly greater computational challenge; often relying upon likelihood-free methods such as approximate Bayesian computation, that requirethe generation of millions of individual stochastic realizations. In this study, we investigate a new approach to computational inference, based on multilevel Monte Carlo sampling: we approximate the posterior cumulative distribution function through a combination of model samples taken over a range of acceptance thresholds. We demonstrate this approach using a variety of discrete-state, continuous-time Markov models of biochemical reactionnetworks. Results show that a computational gain over standard rejection schemes of up to an order of magnitude is achievable without significant loss in estimator accuracy. Author Summary We develop a new method to infer the reaction rate parameters for stochastic models of biochemical reaction networks. Standard computational approaches, based on numerical simulations, are often used to estimate parameters. These computational approaches, however, are extremely expensive, potentially requiring millions of simulations. To alleviate this issue, we apply a different method of sampling allowing us to find an optimal trade-off between performance and accuracy. Our approach is approximately one order of magnitude faster than standard methods, without significant loss in accuracy.}}, 
pages = {064170}, 
journal = {bioRxiv}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/bioRxiv-2016-Warne-Simpson.pdf}, 
year = {2016}
}
@article{2020:Frishman, 
author = {Frishman, Anna and Ronceray, Pierre}, 
title = {{Learning Force Fields from Stochastic Trajectories}}, 
doi = {10.1103/physrevx.10.021009}, 
eprint = {1809.09650}, 
abstract = {{When monitoring the dynamics of stochastic systems, such as interacting particles agitated by thermal noise, disentangling deterministic forces from Brownian motion is challenging. Indeed, we show that there is an information-theoretic bound, the capacity of the system when viewed as a communication channel, that limits the rate at which information about the force field can be extracted from a Brownian trajectory. This capacity provides an upper bound to the system’s entropy production rate and quantifies the rate at which the trajectory becomes distinguishable from pure Brownian motion. We propose a practical and principled method, stochastic force inference, that uses this information to approximate force fields and spatially variable diffusion coefficients. It is data efficient, including in high dimensions, robust to experimental noise, and provides a self-consistent estimate of the inference error. In addition to forces, this technique readily permits the evaluation of out-of-equilibrium currents and the corresponding entropy production with a limited amount of data.}}, 
pages = {021009}, 
number = {2}, 
volume = {10}, 
journal = {Physical Review X}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20X-2020-Frishman-Ronceray.pdf}, 
year = {2020}, 
rating = {4}
}
@article{2020:Rahmani, 
author = {Rahmani, Parisa and Peruani, Fernando and Romanczuk, Pawel}, 
title = {{Flocking in complex environments—Attention trade-offs in collective information processing}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007697}, 
pmid = {32251423}, 
abstract = {{The ability of biological and artificial collectives to outperform solitary individuals in a wide variety of tasks depends crucially on the efficient processing of social and environmental information at the level of the collective. Here, we model collective behavior in complex environments with many potentially distracting cues. Counter-intuitively, large-scale coordination in such environments can be maximized by strongly limiting the cognitive capacity of individuals, where due to self-organized dynamics the collective self-isolates from disrupting information. We observe a fundamental trade-off between coordination and collective responsiveness to environmental cues. Our results offer important insights into possible evolutionary trade-offs in collective behavior in biology and suggests novel principles for design of artificial swarms exploiting attentional bottlenecks.}}, 
pages = {e1007697}, 
number = {4}, 
volume = {16}, 
journal = {PLOS Computational Biology}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLOS%20Computational%20Biology-2020-Rahmani-Romanczuk.pdf}, 
year = {2020}, 
month = {4}
}
@article{2019:Rodríguez-Sánchez, 
author = {Rodríguez-Sánchez, Pablo and Nes, Egbert H van and Scheffer, Marten}, 
title = {{Climbing Escher's stairs: a way to approximate stability landscapes in multidimensional systems}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007788}, 
pmid = {32275714}, 
eprint = {1903.05615}, 
abstract = {{Stability landscapes are useful for understanding the properties of dynamical systems. These landscapes can be calculated from the system's dynamical equations using the physical concept of scalar potential. Unfortunately, for most biological systems with two or more state variables such potentials do not exist. Here we use an analogy with art to provide an accessible explanation of why this happens. Additionally, we introduce a numerical method for decomposing differential equations into two terms: the gradient term that has an associated potential, and the non-gradient term that lacks it. In regions of the state space where the magnitude of the non-gradient term is small compared to the gradient part, we use the gradient term to approximate the potential as quasi-potential. The non-gradient to gradient ratio can be used to estimate the local error introduced by our approximation. Both the algorithm and a ready-to-use implementation in the form of an R package are provided.}}, 
pages = {e1007788}, 
number = {4}, 
volume = {16}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Rodríguez-Sánchez-Scheffer.pdf}, 
year = {2019}, 
month = {4}
}
@article{1994:Gelfand, 
author = {Gelfand, A E and Dey, D K}, 
title = {{Bayesian Model Choice: Asymptotics and Exact Calculations}}, 
issn = {0035-9246}, 
doi = {10.1111/j.2517-6161.1994.tb01996.x}, 
abstract = {{Model determination is a fundamental data analytic task. Here we consider the problem of choosing among a finite (without loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches that we are aware of. Using Laplace approximations we can conveniently assess and compare the asymptotic behaviour of these approaches. Concern regarding the accuracy of these approximations for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fitted with nested non‐linear models enables comparisons between proposals and between exact and asymptotic values.}}, 
pages = {501--514}, 
number = {3}, 
volume = {56}, 
journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20Royal%20Statistical%20Society-%20Series%20B%20(Methodological)-1994-Gelfand-Dey.pdf}, 
year = {1994}
}
@article{2001:Wangg8b, 
author = {Wang, Fugao and Landau, D P}, 
title = {{Determining the density of states for classical statistical models: A random walk algorithm to produce a flat histogram}}, 
issn = {1063-651X}, 
doi = {10.1103/physreve.64.056101}, 
pmid = {11736008}, 
eprint = {cond-mat/0107006}, 
abstract = {{We describe an efficient Monte Carlo algorithm using a random walk in energy space to obtain a very accurate estimate of the density of states for classical statistical models. The density of states is modified at each step when the energy level is visited to produce a flat histogram. By carefully controlling the modification factor, we allow the density of states to converge to the true value very quickly, even for large systems. This algorithm is especially useful for complex systems with a rough landscape since all possible energy levels are visited with the same probability. In this paper, we apply our algorithm to both 1st and 2nd order phase transitions to demonstrate its efficiency and accuracy. We obtained direct simulational estimates for the density of states for two-dimensional ten-state Potts models on lattices up to \$200 \textbackslashtimes 200 \$ and Ising models on lattices up to \$256 \textbackslashtimes 256\$. Applying this approach to a 3D \$\textbackslashpm J\$ spin glass model we estimate the internal energy and entropy at zero temperature; and, using a two-dimensional random walk in energy and order-parameter space, we obtain the (rough) canonical distribution and energy landscape in order-parameter space. Preliminary data suggest that the glass transition temperature is about 1.2 and that better estimates can be obtained with more extensive application of the method.}}, 
pages = {056101}, 
number = {5}, 
volume = {64}, 
journal = {Physical Review E}, 
keywords = {Wang-Landau-Sampling}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2001-Wang-Landau.pdf}, 
year = {2001}
}
@article{2001:Wang, 
author = {Wang, Fugao and Landau, D P}, 
title = {{Efficient, Multiple-Range Random Walk Algorithm to Calculate the Density of States}}, 
issn = {0031-9007}, 
doi = {10.1103/physrevlett.86.2050}, 
pmid = {11289852}, 
eprint = {cond-mat/0011174}, 
abstract = {{We present a new Monte Carlo algorithm that produces results of high accuracy with reduced simulational effort. Independent random walks are performed (concurrently or serially) in different, restricted ranges of energy, and the resultant density of states is modified continuously to produce locally flat histograms. This method permits us to directly access the free energy and entropy, is independent of temperature, and is efficient for the study of both 1st order and 2nd order phase transitions. It should also be useful for the study of complex systems with a rough energy landscape.}}, 
pages = {2050--2053}, 
number = {10}, 
volume = {86}, 
journal = {Physical Review Letters}, 
keywords = {Wang-Landau-Sampling}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20Letters-2001-Wang-Landau.pdf}, 
year = {2001}
}
@article{1996:Helbing, 
author = {Helbing, Dirk and Molini, Rolf}, 
title = {{Occurrence probabilities of stochastic paths}}, 
issn = {0375-9601}, 
doi = {10.1016/0375-9601(96)00010-2}, 
eprint = {cond-mat/9805324}, 
abstract = {{An analytical formula for the occurrence probability of Markovian stochastic paths with repeatedly visited and/or equal departure rates is derived. This formula is essential for an efficient investigation of the trajectories belonging to random walk models and for a numerical evaluation of the “contracted path integral solution” of the discrete master equation [Phys. Lett. A 195 (1994) 128].}}, 
pages = {130--137}, 
number = {3}, 
volume = {212}, 
journal = {Physics Letters A}, 
keywords = {Stochastic Paths}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physics%20Letters%20A-1996-Helbing-Molini.pdf}, 
year = {1996}
}
@article{2017:Weber, 
author = {Weber, Markus F and Frey, Erwin}, 
title = {{Master equations and the theory of stochastic path integrals}}, 
issn = {0034-4885}, 
doi = {10.1088/1361-6633/aa5ae2}, 
pmid = {28306551}, 
eprint = {1609.02849}, 
abstract = {{This review provides a pedagogic and self-contained introduction to master equations and to their representation by path integrals. Since the 1930s, master equations have served as a fundamental tool to understand the role of fluctuations in complex biological, chemical, and physical systems. Despite their simple appearance, analyses of master equations most often rely on low-noise approximations such as the Kramers-Moyal or the system size expansion, or require ad-hoc closure schemes for the derivation of low-order moment equations. We focus on numerical and analytical methods going beyond the low-noise limit and provide a unified framework for the study of master equations. After deriving the forward and backward master equations from the Chapman-Kolmogorov equation, we show how the two master equations can be cast into either of four linear partial differential equations (PDEs). Three of these PDEs are discussed in detail. The first PDE governs the time evolution of a generalized probability generating function whose basis depends on the stochastic process under consideration. Spectral methods, WKB approximations, and a variational approach have been proposed for the analysis of the PDE. The second PDE is novel and is obeyed by a distribution that is marginalized over an initial state. It proves useful for the computation of mean extinction times. The third PDE describes the time evolution of a 'generating functional', which generalizes the so-called Poisson representation. Subsequently, the solutions of the PDEs are expressed in terms of two path integrals: a 'forward' and a 'backward' path integral. Combined with inverse transformations, one obtains two distinct path integral representations of the conditional probability distribution solving the master equations. We exemplify both path integrals in analysing elementary chemical reactions. Moreover, we show how a well-known path integral representation of averaged observables can be recovered from them. Upon expanding the forward and the backward path integrals around stationary paths, we then discuss and extend a recent method for the computation of rare event probabilities. Besides, we also derive path integral representations for processes with continuous state spaces whose forward and backward master equations admit Kramers-Moyal expansions. A truncation of the backward expansion at the level of a diffusion approximation recovers a classic path integral representation of the (backward) Fokker-Planck equation. One can rewrite this path integral in terms of an Onsager-Machlup function and, for purely diffusive Brownian motion, it simplifies to the path integral of Wiener. To make this review accessible to a broad community, we have used the language of probability theory rather than quantum (field) theory and do not assume any knowledge of the latter. The probabilistic structures underpinning various technical concepts, such as coherent states, the Doi-shift, and normal-ordered observables, are thereby made explicit.}}, 
pages = {046601}, 
number = {4}, 
volume = {80}, 
journal = {Reports on Progress in Physics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Reports%20on%20Progress%20in%20Physics-2017-Weber-Frey.pdf}, 
year = {2017}, 
month = {4}, 
rating = {5}
}
@incollection{2014:Held, 
author = {Held, Leonhard and Bové, Daniel Sabanés}, 
title = {{Applied Statistical Inference}}, 
booktitle = {Applied Statistical Inference}, 
isbn = {9783642378867}, 
abstract = {{This chapter describes numerical methods for Bayesian inference in non-conjugate settings. Standard numerical techniques and the Laplace approximation provide ways to numerically compute posterior characteristics of interest. Monte Carlo methods, including Monte Carlo integration, rejection and importance sampling as well as Markov chain Monte Carlo are described. Finally, numerical computation of the marginal likelihood, necessary for Bayesian model selection, is discussed. Exercises are given at the end.}}, 
pages = {247--289}, 
publisher = {Springer Berlin Heidelberg}, 
address = {Berlin, Heidelberg}, 
year = {2014}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/2014_Book_AppliedStatisticalInference.pdf}
}
@misc{2004:Chan, 
author = {Chan, Ping-Shing}, 
editor = {Kotz, S. and Read, C. B. and Balakrishnan, N. and Vidakovic, B. and Johnson, N. L.}, 
title = {{Encyclopedia of Statistical Sciences}}, 
journal = {Encyclopedia of Statistical Sciences}, 
isbn = {9780471667193}, 
pages = {4363--4366}, 
volume = {7}, 
publisher = {American Cancer Society}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/loggamma.pdf}, 
year = {2004}
}
@article{1991:Gelfand, 
author = {Gelfand, Alan E and Smith, Adrian F M}, 
title = {{Gibbs sampling for marginal posterior expectations}}, 
issn = {0361-0926}, 
doi = {10.1080/03610929108830595}, 
pages = {1747--1766}, 
number = {5-6}, 
volume = {20}, 
journal = {Communications in Statistics - Theory and Methods}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Communications%20in%20Statistics%20-%20Theory%20and%20Methods-1991-Gelfand-Smith.pdf}, 
year = {1991}
}
@article{1990:Gelfand, 
author = {Gelfand, Alan E and Smith, Adrian F M}, 
title = {{Sampling-Based Approaches to Calculating Marginal Densities}}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1990.10476213}, 
abstract = {{Stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm can be viewed as three alternative sampling- (or Monte Carlo-) based approaches to the calculation of numerical estimates of marginal probability distributions. The three approaches will be reviewed, compared, and contrasted in relation to various joint probability structures frequently encountered in applications. In particular, the relevance of the approaches to calculating Bayesian posterior densities for a variety of structured models will be discussed and illustrated.}}, 
pages = {398--409}, 
number = {410}, 
volume = {85}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-1990-Gelfand-Smith.pdf}, 
year = {1990}
}
@article{2003:Paninski, 
author = {Paninski, Liam}, 
title = {{Estimation of Entropy and Mutual Information}}, 
issn = {0899-7667}, 
doi = {10.1162/089976603321780272}, 
abstract = {{We present some new results on the nonparametric estimation of entropy and mutual information. First, we use an exact local expansion of the entropy function to prove almost sure consistency and central limit theorems for three of the most commonly used discretized information estimators. The setup is related to Grenander's method of sieves and places no assumptions on the underlying probability measure generating the data. Second, we prove a converse to these consistency theorems, demonstrating that a misapplication of the most common estimation techniques leads to an arbitrarily poor estimate of the true information, even given unlimited data. This inconsistency theorem leads to an analytical approximation of the bias, valid in surprisingly small sample regimes and more accurate than the usual formula of Miller and Madow over a large region of parameter space. The two most practical implications of these results are negative: (1) information estimates in a certain data regime are likely contaminated by bias, even if bias-corrected estimators are used, and (2) confidence intervals calculated by standard techniques drastically underestimate the error of the most common estimation methods. Finally, we note a very useful connection between the bias of entropy estimators and a certain polynomial approximation problem. By casting bias calculation problems in this approximation theory framework, we obtain the best possible generalization of known asymptotic bias results. More interesting, this framework leads to an estimator with some nice properties: the estimator comes equipped with rigorous bounds on the maximum error over all possible underlying probability distributions, and this maximum error turns out to be surprisingly small. We demonstrate the application of this new estimator on both real and simulated data.}}, 
pages = {1191--1253}, 
number = {6}, 
volume = {15}, 
journal = {Neural Computation}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Neural%20Computation-2003-Paninski-Paninski.pdf}, 
year = {2003}
}
@article{2001:Crooks, 
author = {Crooks, Gavin E and Chandler, David}, 
title = {{Efficient transition path sampling for nonequilibrium stochastic dynamics}}, 
issn = {1063-651X}, 
doi = {10.1103/physreve.64.026109}, 
pmid = {11497653}, 
abstract = {{The transition path sampling methodology is adapted to the efficient sampling of large fluctuations in nonequilibrium systems evolving according to Langevin’s equations of motion. This technique is used to simulate the behavior of the bistable Maier-Stein system at noise intensities much lower than those previously possible.}}, 
pages = {026109}, 
number = {2}, 
volume = {64}, 
journal = {Physical Review E}, 
keywords = {Transition Path Sampling,Stochastic Paths,Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2001-Crooks-Chandler.pdf}, 
year = {2001}
}
@article{2020:Uda, 
author = {Uda, Shinsuke}, 
title = {{Application of information theory in systems biology.}}, 
issn = {1867-2450}, 
doi = {10.1007/s12551-020-00665-w}, 
pmid = {32144740}, 
abstract = {{Over recent years, new light has been shed on aspects of information processing in cells. The quantification of information, as described by Shannon's information theory, is a basic and powerful tool that can be applied to various fields, such as communication, statistics, and computer science, as well as to information processing within cells. It has also been used to infer the network structure of molecular species. However, the difficulty of obtaining sufficient sample sizes and the computational burden associated with the high-dimensional data often encountered in biology can result in bottlenecks in the application of information theory to systems biology. This article provides an overview of the application of information theory to systems biology, discussing the associated bottlenecks and reviewing recent work.}}, 
pages = {377--384}, 
number = {2}, 
volume = {12}, 
journal = {Biophysical reviews}, 
keywords = {Information Theory}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Biophysical%20reviews-2020-Uda-Uda.pdf}, 
year = {2020}, 
rating = {5}
}
@article{2019:Cepeda-Humerez, 
author = {Cepeda-Humerez, Sarah Anhala and Ruess, Jakob and Tkačik, Gašper}, 
title = {{Estimating information in time-varying signals.}}, 
issn = {1553-734X}, 
doi = {10.1371/journal.pcbi.1007290}, 
pmid = {31479447}, 
abstract = {{Across diverse biological systems-ranging from neural networks to intracellular signaling and genetic regulatory networks-the information about changes in the environment is frequently encoded in the full temporal dynamics of the network nodes. A pressing data-analysis challenge has thus been to efficiently estimate the amount of information that these dynamics convey from experimental data. Here we develop and evaluate decoding-based estimation methods to lower bound the mutual information about a finite set of inputs, encoded in single-cell high-dimensional time series data. For biological reaction networks governed by the chemical Master equation, we derive model-based information approximations and analytical upper bounds, against which we benchmark our proposed model-free decoding estimators. In contrast to the frequently-used k-nearest-neighbor estimator, decoding-based estimators robustly extract a large fraction of the available information from high-dimensional trajectories with a realistic number of data samples. We apply these estimators to previously published data on Erk and Ca2+ signaling in mammalian cells and to yeast stress-response, and find that substantial amount of information about environmental state can be encoded by non-trivial response statistics even in stationary signals. We argue that these single-cell, decoding-based information estimates, rather than the commonly-used tests for significant differences between selected population response statistics, provide a proper and unbiased measure for the performance of biological signaling networks.}}, 
pages = {e1007290}, 
number = {9}, 
volume = {15}, 
journal = {PLoS computational biology}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLoS%20computational%20biology-2019-Cepeda-Humerez-Tkačik.pdf}, 
year = {2019}
}
@article{2019:Duso, 
author = {Duso, Lorenzo and Zechner, Christoph}, 
title = {{Path mutual information for a class of biochemical reaction networks}}, 
eprint = {1904.01988}, 
abstract = {{Living cells encode and transmit information in the temporal dynamics of biochemical components. Gaining a detailed understanding of the input-output relationship in biological systems therefore requires quantitative measures that capture the interdependence between complete time trajectories of biochemical components. Mutual information provides such a measure but its calculation in the context of stochastic reaction networks is associated with mathematical challenges. Here we show how to estimate the mutual information between complete paths of two molecular species that interact with each other through biochemical reactions. We demonstrate our approach using three simple case studies.}}, 
journal = {arXiv}, 
keywords = {Recommended by PRtW,Stochastic Paths}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Duso-Zechner.pdf}, 
year = {2019}
}
@article{2007:Ziv, 
author = {Ziv, Etay and Nemenman, Ilya and Wiggins, Chris H}, 
title = {{Optimal Signal Processing in Small Stochastic Biochemical Networks}}, 
doi = {10.1371/journal.pone.0001077}, 
pmid = {17957259}, 
pmcid = {PMC2034356}, 
eprint = {q-bio/0612041}, 
abstract = {{We quantify the influence of the topology of a transcriptional regulatory network on its ability to process environmental signals. By posing the problem in terms of information theory, we may do this without specifying the function performed by the network. Specifically, we study the maximum mutual information between the input (chemical) signal and the output (genetic) response attainable by the network in the context of an analytic model of particle number fluctuations. We perform this analysis for all biochemical circuits, including various feedback loops, that can be built out of 3 chemical species, each under the control of one regulator. We find that a generic network, constrained to low molecule numbers and reasonable response times, can transduce more information than a simple binary switch and, in fact, manages to achieve close to the optimal information transmission fidelity. These high-information solutions are robust to tenfold changes in most of the networks' biochemical parameters; moreover they are easier to achieve in networks containing cycles with an odd number of negative regulators (overall negative feedback) due to their decreased molecular noise (a result which we derive analytically). Finally, we demonstrate that a single circuit can support multiple high-information solutions. These findings suggest a potential resolution of the "cross-talk" dilemma as well as the previously unexplained observation that transcription factors which undergo proteolysis are more likely to be auto-repressive.}}, 
pages = {e1077}, 
number = {10}, 
volume = {2}, 
journal = {PLoS ONE}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/PLoS%20ONE-2007-Ziv-Wiggins.pdf}, 
year = {2007}
}
@article{2019:Blanchard, 
author = {Blanchard, Pierre and Higham, Desmond J and Higham, Nicholas J}, 
title = {{Accurate Computation of the Log-Sum-Exp and Softmax Functions}}, 
eprint = {1909.03469}, 
abstract = {{Evaluating the log-sum-exp function or the softmax function is a key step in many modern data science algorithms, notably in inference and classification. Because of the exponentials that these functions contain, the evaluation is prone to overflow and underflow, especially in low precision arithmetic. Software implementations commonly use alternative formulas that avoid overflow and reduce the chance of harmful underflow, employing a shift or another rewriting. Although mathematically equivalent, these variants behave differently in floating-point arithmetic. We give rounding error analyses of different evaluation algorithms and interpret the error bounds using condition numbers for the functions. We conclude, based on the analysis and numerical experiments, that the shifted formulas are of similar accuracy to the unshifted ones and that the shifted softmax formula is typically more accurate than a division-free variant.}}, 
journal = {arXiv}, 
keywords = {special functions}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Blanchard-Higham.pdf}, 
year = {2019}
}
@article{2005:Samad, 
author = {Samad, Hana El and Khammash, Mustafa and Petzold, Linda and Gillespie, Dan}, 
title = {{Stochastic modelling of gene regulatory networks}}, 
issn = {1049-8923}, 
doi = {10.1002/rnc.1018}, 
abstract = {{Gene regulatory networks are dynamic and stochastic in nature, and exhibit exquisite feedback and feedforward control loops that regulate their biological function at different levels. Modelling of such networks poses new challenges due, in part, to the small number of molecules involved and the stochastic nature of their interactions. In this article, we motivate the stochastic modelling of genetic networks and demonstrate the approach using several examples. We discuss the mathematics of molecular noise models including the chemical master equation, the chemical Langevin equation, and the reaction rate equation. We then discuss numerical simulation approaches using the stochastic simulation algorithm (SSA) and its variants. Finally, we present some recent advances for dealing with stochastic stiffness, which is the key challenge in efficiently simulating stochastic chemical kinetics. Copyright © 2005 John Wiley \& Sons, Ltd.}}, 
pages = {691--711}, 
number = {15}, 
volume = {15}, 
journal = {International Journal of Robust and Nonlinear Control}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/International%20Journal%20of%20Robust%20and%20Nonlinear%20Control-2005-Samad-Gillespie.pdf}, 
year = {2005}
}
@article{2012:Warren, 
author = {Warren, Patrick B and Allen, Rosalind J}, 
title = {{Steady-state parameter sensitivity in stochastic modeling via trajectory reweighting.}}, 
issn = {0021-9606}, 
doi = {10.1063/1.3690092}, 
pmid = {22423827}, 
eprint = {1202.4704}, 
abstract = {{Parameter sensitivity analysis is a powerful tool in the building and analysis of biochemical network models. For stochastic simulations, parameter sensitivity analysis can be computationally expensive, requiring multiple simulations for perturbed values of the parameters. Here, we use trajectory reweighting to derive a method for computing sensitivity coefficients in stochastic simulations without explicitly perturbing the parameter values, avoiding the need for repeated simulations. The method allows the simultaneous computation of multiple sensitivity coefficients. Our approach recovers results originally obtained by application of the Girsanov measure transform in the general theory of stochastic processes [A. Plyasunov and A. P. Arkin, J. Comput. Phys. 221, 724 (2007)]. We build on these results to show how the method can be used to compute steady-state sensitivity coefficients from a single simulation run, and we present various efficiency improvements. For models of biochemical signaling networks, the method has a particularly simple implementation. We demonstrate its application to a signaling network showing stochastic focussing and to a bistable genetic switch, and present exact results for models with linear propensity functions.}}, 
pages = {104106}, 
number = {10}, 
volume = {136}, 
journal = {The Journal of chemical physics}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Journal%20of%20chemical%20physics-2012-Warren-Allen.pdf}, 
year = {2012}
}
@article{2014:Chan, 
author = {Chan, Joshua C C and Eisenstat, Eric}, 
title = {{Marginal Likelihood Estimation with the Cross-Entropy Method}}, 
issn = {0747-4938}, 
doi = {10.1080/07474938.2014.944474}, 
abstract = {{We consider an adaptive importance sampling approach to estimating the marginal likelihood, a quantity that is fundamental in Bayesian model comparison and Bayesian model averaging. This approach is motivated by the difficulty of obtaining an accurate estimate through existing algorithms that use Markov chain Monte Carlo (MCMC) draws, where the draws are typically costly to obtain and highly correlated in high-dimensional settings. In contrast, we use the cross-entropy (CE) method, a versatile adaptive Monte Carlo algorithm originally developed for rare-event simulation. The main advantage of the importance sampling approach is that random samples can be obtained from some convenient density with little additional costs. As we are generating independent draws instead of correlated MCMC draws, the increase in simulation effort is much smaller should one wish to reduce the numerical standard error of the estimator. Moreover, the importance density derived via the CE method is grounded in information theory, and therefore, is in a well-defined sense optimal. We demonstrate the utility of the proposed approach by two empirical applications involving women's labor market participation and U.S. macroeconomic time series. In both applications, the proposed CE method compares favorably to existing estimators.}}, 
pages = {256--285}, 
number = {3}, 
volume = {34}, 
journal = {Econometric Reviews}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Econometric%20Reviews-2014-Chan-Eisenstat.pdf}, 
year = {2014}
}
@article{2010:Tostevin, 
author = {Tostevin, Filipe and Wolde, Pieter Rein ten}, 
title = {{Mutual information in time-varying biochemical systems}}, 
issn = {1539-3755}, 
doi = {10.1103/physreve.81.061917}, 
pmid = {20866450}, 
eprint = {1002.4273}, 
abstract = {{Cells must continuously sense and respond to time-varying environmental stimuli. These signals are transmitted and processed by biochemical signalling networks. However, the biochemical reactions making up these networks are intrinsically noisy, which limits the reliability of intracellular signalling. Here we use information theory to characterise the reliability of transmission of time-varying signals through elementary biochemical reactions in the presence of noise. We calculate the mutual information for both instantaneous measurements and trajectories of biochemical systems for a Gaussian model. Our results indicate that the same network can have radically different characteristics for the transmission of instantaneous signals and trajectories. For trajectories, the ability of a network to respond to changes in the input signal is determined by the timing of reaction events, and is independent of the correlation time of the output of the network. We also study how reliably signals on different time-scales can be transmitted by considering the frequency-dependent coherence and gain-to-noise ratio. We find that a detector that does not consume the ligand molecule upon detection can more reliably transmit slowly varying signals, while an absorbing detector can more reliably transmit rapidly varying signals. Furthermore, we find that while one reaction may more reliably transmit information than another when considered in isolation, when placed within a signalling cascade the relative performance of the two reactions can be reversed. This means that optimising signal transmission at a single level of a signalling cascade can reduce signalling performance for the cascade as a whole.}}, 
pages = {061917}, 
number = {6}, 
volume = {81}, 
journal = {Physical Review E}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2010-Tostevin-Wolde.pdf}, 
year = {2010}
}
@article{2019:Meijers, 
author = {Meijers, Matthijs and Ito, Sosuke and Wolde, Pieter Rein ten}, 
title = {{The behaviour of information flow near criticality}}, 
eprint = {1906.00787}, 
abstract = {{Recent experiments have indicated that many biological systems self-organise near their critical point, which hints at a common design principle. While it has been suggested that information transmission is optimized near the critical point, it remains unclear how information transmission depends on the dynamics of the input signal, the distance over which the information needs to be transmitted, and the distance to the critical point. Here we employ stochastic simulations of a driven 2D Ising system and study the instantaneous mutual information and the information transmission rate between a driven input spin and an output spin. The instantaneous mutual information varies non-monotonically with the temperature, but increases monotonically with the correlation time of the input signal. In contrast, the information transmission rate exhibits a maximum as a function of the input correlation time. Moreover, there exists an optimal temperature that maximizes this maximum information transmission rate. It arises from a tradeoff between the necessity to respond fast to changes in the input so that more information per unit amount of time can be transmitted, and the need to respond to reliably. The optimal temperature lies above the critical point, but moves towards it as the distance between the input and output spin is increased.}}, 
journal = {arXiv}, 
keywords = {Recommended by PRtW}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/arXiv-2019-Meijers-Wolde.pdf}, 
year = {2019}
}
@article{1993:Gelfand, 
author = {Gelfand, Alan E and Dey, D K}, 
title = {{Bayesian Model Choice: Asymptotics and Exact Calculations}}, 
issn = {0035-9246}, 
doi = {10.21236/ada269067}, 
abstract = {{Model determination is a fundamental data analytic task. Here we consider the problem of choosing amongst a finite (with loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches we are aware of. Using Laplace approximations we can conveniently assess and compare asymptotic behavior of these approaches. Concern regarding the accuracy of these approximation for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fit with nested non linear models enables comparison between proposals and between exact and asymptotic values.}}, 
pages = {501--514}, 
number = {3}, 
volume = {56}, 
journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20Royal%20Statistical%20Society-%20Series%20B%20(Methodological)-1993-Gelfand-Dey.pdf}, 
year = {1993}
}
@article{2004:Nemenman, 
author = {Nemenman, Ilya and Bialek, William and Steveninck, Rob de Ruyter van}, 
title = {{Entropy and information in neural spike trains: Progress on the sampling problem}}, 
issn = {1539-3755}, 
doi = {10.1103/physreve.69.056111}, 
pmid = {15244887}, 
eprint = {physics/0306063}, 
abstract = {{The major problem in information theoretic analysis of neural responses and other biological data is the reliable estimation of entropy--like quantities from small samples. We apply a recently introduced Bayesian entropy estimator to synthetic data inspired by experiments, and to real experimental spike trains. The estimator performs admirably even very deep in the undersampled regime, where other techniques fail. This opens new possibilities for the information theoretic analysis of experiments, and may be of general interest as an example of learning from limited data.}}, 
pages = {056111}, 
number = {5}, 
volume = {69}, 
journal = {Physical Review E}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Physical%20Review%20E-2004-Nemenman-Steveninck.pdf}, 
year = {2004}
}
@phdthesis{1993:Chan, 
title = {{A Statistical Study of Log-Gamma Distribution}}, 
author = {Chan, Shing Ping and Balakrishnan, Narayanaswamy}, 
keywords = {special functions}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Unknown-1993-Chan-Balakrishnan.pdf}, 
year = {1993}
}
@article{1996:Unknown, 
author = {Meng, Xiao-Li and Wong, Wing Hung}, 
title = {{Simulating Ratios of Normalizing Constants Via a Simple Identity: A Theoretical Exploration}}, 
url = {http://www.jstor.org/stable/24306045}, 
abstract = {{Let pi(w),i = 1,2, be two densities with common support where each density is known up to a normalizing constant: pi(w) = qi(w)/ci. We have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a computational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: (c₁/c₂) = ((E₂[q₁(w)α (w)])/(E₁[q₂(w)α (w)])) Here Ei denotes the expectation with respect to pi (i = 1,2), and α is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of α. Using a simple but informative example, we demonstrate that with sensible (not necessarily optimal) choices of α, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to α = 1/q2. We also introduce several generalizations of this identity for handling more complicated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.}}, 
pages = {831---860}, 
number = {4}, 
volume = {6}, 
journal = {Statistica Sinica}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Statistica%20Sinica-1996-Unknown-Wong.pdf}, 
year = {1996}
}
@article{1998:Gelman, 
author = {Gelman, Andrew and Meng, Xiao-Li}, 
title = {{Simulating normalizing constants: from importance sampling to bridge sampling to path sampling}}, 
issn = {0883-4237}, 
doi = {10.1214/ss/1028905934}, 
abstract = {{Computing (ratios of) normalizing constants of probability models is a fundamental computational problem for many statistical and scientific studies. Monte Carlo simulation is an effective technique, especially with complex and high-dimensional models. This paper aims to bring to the attention of general statistical audiences of some effective methods originating from theoretical physics and at the same time to explore these methods from a more statistical perspective, through establishing theoretical connections and illustrating their uses with statistical problems. We show that the acceptance ratio method and thermodynamic integration are natural generalizations of importance sampling, which is most familiar to statistical audiences. The former generalizes importance sampling through the use of a single "bridge" density and is thus a case of bridge sampling in the sense of Meng and Wong. Thermodynamic integration, which is also known in the numerical analysis literature as Ogata's method for high-dimensional integration, corresponds to the use of infinitely many and continuously connected bridges (and thus a "path"). Our path sampling formulation offers more flexibility and thus potential efficiency to thermodynamic integration, and the search of optimal paths turns out to have close connections with the Jeffreys prior density and the Rao and Hellinger distances between two densities. We provide an informative theoretical example as well as two empirical examples (involving 17- to 70-dimensional integrations) to illustrate the potential and implementation of path sampling. We also discuss some open problems.}}, 
pages = {163--185}, 
number = {2}, 
volume = {13}, 
journal = {Statistical Science}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Statistical%20Science-1998-Gelman-Meng.pdf}, 
year = {1998}, 
rating = {5}
}
@article{1994:Tierney, 
author = {Tierney, Luke}, 
title = {{Markov Chains for Exploring Posterior Distributions}}, 
issn = {0090-5364}, 
doi = {10.1214/aos/1176325750}, 
url = {http://www.jstor.org/stable/2242477}, 
abstract = {{Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.}}, 
pages = {1701--1728}, 
number = {4}, 
volume = {22}, 
journal = {The Annals of Statistics}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/The%20Annals%20of%20Statistics-1994-Tierney-Tierney.pdf}, 
year = {1994}
}
@article{1990:Carlin, 
author = {Carlin, Bradley P and Gelfand, Alan E and Smith, Adrian F}, 
title = {{Hierarchical Bayesian Analysis of Change Point Problems}}, 
doi = {10.21236/ada228179}, 
abstract = {{A general approach to hierarchical Bayes change point models is presented. In particular desired marginal posterior densities are obtained utilizing the Gibbs sampler, an iterative Monte Carlo method. This approach avoids sophisticated analytic and numerical high dimensional integration procedures. We include application to changing regressions, changing Poisson processes, and changing Markov chains. Within these contexts we handle several previously inaccessible problems. (kr)}}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Hierarchical%20Bayesian%20Analysis%20of%20Change%20Point%20Problems.pdf}, 
year = {1990}
}
@article{2020:Rumyantsev, 
author = {Rumyantsev, Oleg I and Lecoq, Jérôme A and Hernandez, Oscar and Zhang, Yanping and Savall, Joan and Chrapkiewicz, Radosław and Li, Jane and Zeng, Hongkui and Ganguli, Surya and Schnitzer, Mark J}, 
title = {{Fundamental bounds on the fidelity of sensory cortical coding.}}, 
issn = {0028-0836}, 
doi = {10.1038/s41586-020-2130-2}, 
pmid = {32238928}, 
abstract = {{How the brain processes information accurately despite stochastic neural activity is a longstanding question1. For instance, perception is fundamentally limited by the information that the brain can extract from the noisy dynamics of sensory neurons. Seminal experiments2,3 suggest that correlated noise in sensory cortical neural ensembles is what limits their coding accuracy4-6, although how correlated noise affects neural codes remains debated7-11. Recent theoretical work proposes that how a neural ensemble's sensory tuning properties relate statistically to its correlated noise patterns is a greater determinant of coding accuracy than is absolute noise strength12-14. However, without simultaneous recordings from thousands of cortical neurons with shared sensory inputs, it is unknown whether correlated noise limits coding fidelity. Here we present a 16-beam, two-photon microscope to monitor activity across the mouse primary visual cortex, along with analyses to quantify the information conveyed by large neural ensembles. We found that, in the visual cortex, correlated noise constrained signalling for ensembles with 800-1,300 neurons. Several noise components of the ensemble dynamics grew proportionally to the ensemble size and the encoded visual signals, revealing the predicted information-limiting correlations12-14. Notably, visual signals were perpendicular to the largest noise mode, which therefore did not limit coding fidelity. The information-limiting noise modes were approximately ten times smaller and concordant with mouse visual acuity15. Therefore, cortical design principles appear to enhance coding accuracy by restricting around 90\% of noise fluctuations to modes that do not limit signalling fidelity, whereas much weaker correlated noise modes inherently bound sensory discrimination.}}, 
pages = {100--105}, 
number = {7801}, 
volume = {580}, 
journal = {Nature}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Nature-2020-Rumyantsev-Schnitzer.pdf}, 
year = {2020}
}
@article{1997:Diciccio, 
author = {Diciccio, Thomas J and Kass, Robert E and Raftery, Adrian and Wasserman, Larry}, 
title = {{Computing Bayes Factors by Combining Simulation and Asymptotic Approximations}}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1997.10474045}, 
abstract = {{The Bayes factor is a ratio of two posterior normalizing constants, which may be difficult to compute. We compare several methods of estimating Bayes factors when it is possible to simulate observations from the posterior distributions, via Markov chain Monte Carlo or other techniques. The methods that we study are all easily applied without consideration of special features of the problem, provided that each posterior distribution is well behaved in the sense of having a single dominant mode. We consider a simulated version of Laplace's method, a simulated version of Bartlett correction, importance sampling, and a reciprocal importance sampling technique. We also introduce local volume corrections for each of these. In addition, we apply the bridge sampling method of Meng and Wong. We find that a simulated version of Laplace's method, with local volume correction, furnishes an accurate approximation that is especially useful when likelihood function evaluations are costly. A simple bridge sampling technique in conjunction with Laplace's method often achieves an order of magnitude improvement in accuracy.}}, 
pages = {903--915}, 
number = {439}, 
volume = {92}, 
journal = {Journal of the American Statistical Association}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20American%20Statistical%20Association-1997-Diciccio-Wasserman.pdf}, 
year = {1997}
}
@article{1994:Newton, 
author = {Newton, Michael A and Raftery, Adrian E}, 
title = {{Approximate Bayesian Inference with the Weighted Likelihood Bootstrap}}, 
issn = {0035-9246}, 
doi = {10.1111/j.2517-6161.1994.tb01956.x}, 
abstract = {{We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling‐importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR‐adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second‐order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models.}}, 
pages = {3--26}, 
number = {1}, 
volume = {56}, 
journal = {Journal of the Royal Statistical Society: Series B (Methodological)}, 
local-url = {file://localhost/Users/mr/Documents/Papers%20Library/Journal%20of%20the%20Royal%20Statistical%20Society-%20Series%20B%20(Methodological)-1994-Newton-Raftery.pdf}, 
year = {1994}
}