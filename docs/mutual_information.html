<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>mutual_information</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="styling.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="mutual-information-between-trajectories">Mutual Information between Trajectories</h1>
<h2 id="information-theory-for-trajectories">Information theory for trajectories</h2>
<p>A trajectory <span class="math inline">\(X\)</span> with <span class="math inline">\(N\)</span> steps is defined by a set of pairs <span class="math inline">\(X=\{(t_i, \mathbf{x}_i)\; |\; i=0\ldots N-1 \}\)</span> where <span class="math inline">\(\mathbf{x}_i\)</span> defines the trajectory value at time <span class="math inline">\(t_i\)</span>. We can also have random variables over trajectories and therefore probability distributions over the space of all trajectories.</p>
<p>As a next step we can make sense of the entropy of a trajectory. Let <span class="math inline">\(\mathcal{X}_N\)</span> be a random variable over trajectories of length <span class="math inline">\(N\)</span>. We call</p>
<p><span class="math display">\[
\mathrm H(\mathcal{X}_N) = - \int\limits_{X\in \sigma(\mathcal{X}_N)} dX\; \mathrm{P}(\mathcal{X}_N = X)\; \ln \mathrm{P} (\mathcal{X}_N = X)
\]</span></p>
<p>the entropy of <span class="math inline">\(\mathcal{X}_N\)</span> where <span class="math inline">\(\mathrm{P}(\mathcal{X}_N = X)\)</span> is the probability density function of a trajectory <span class="math inline">\(X=\{(t_i, \mathbf{x}_i)\; |\; i=0\ldots N-1 \}\)</span>. We can also define the conditional entropy for trajectories</p>
<p><span class="math display">\[
\mathrm H(\mathcal{X}_N | \mathcal{S}_M) = -\int\limits_{S\in \sigma(\mathcal{S}_N)} dS\; \mathrm{P} (\mathcal{S}_N = S) \int\limits_{X\in \sigma(\mathcal{X}_N)} dX\; \mathrm{P}(\mathcal{X}_N = X | \mathcal{S}_N = S)\; \ln \mathrm{P} (\mathcal{X}_N = X | \mathcal{S}_N = S) \,.
\]</span></p>
<p>With these two quantities we can express the <em>mutual information</em> between trajectories</p>
<p><span class="math display">\[
\mathrm{I}(\mathcal{X}_N; \mathcal{S}_M) = \mathrm H(\mathcal{X}_N) - \mathrm H(\mathcal{X}_N | \mathcal{S}_M) \,.
\]</span></p>
<p>The mutual information between two random variables quantifies by how much our certainty of the value of one variable increases if we know the other one.</p>
<p>To shorten the notation we write <span class="math inline">\(\mathrm{P} (\mathcal{X}_N = X)\)</span> as <span class="math inline">\(\mathrm P_{\mathcal{X}_N}(X)\)</span> and if the random variable is clear from the context we even drop the index and only write <span class="math inline">\(\mathrm P(X)\)</span>. With the short notation we can rewrite the mutual information</p>
<p><span class="math display">\[
\mathrm{I}(\mathcal{X}_N; \mathcal{S}_M) = \int\limits_{S\in \sigma(\mathcal{S}_N)} dS \int\limits_{X\in \sigma(\mathcal{X}_N)} dX\; \mathrm{P}( X , S)\; \ln \frac{\mathrm{P} ( X |  S)}{\mathrm P(X)} \,.
\]</span></p>
<p>To evaluate <span class="math inline">\(\mathrm P(X)\)</span> we have to expand it as follows</p>
<p><span class="math display">\[
\mathrm P(X) = \int\limits_{S\in \sigma(\mathcal{S}_N)} dS\; \mathrm P(X, S) = \int\limits_{S\in \sigma(\mathcal{S}_N)} dS\; \mathrm P(X|S) \ \mathrm P (S) \equiv \left\langle P(X | S) \right\rangle_{\mathcal{S}_N} \,.
\]</span></p>
<p>These relations let us state the mutual information as nested averages over the likelihood <span class="math inline">\(P(X|S)\)</span>:</p>
<p><span class="math display">\[
\mathrm{I}(\mathcal{X}; \mathcal{S}) = \left\langle \ln \frac{\mathrm{P} ( X |  S)}{\mathrm P(X)} \right\rangle_{\mathcal{X},\mathcal{S}} = \left\langle \ln \frac{\mathrm{P} ( X |  S)}{\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S}} \right\rangle_{\mathcal{X},\mathcal{S}} \,.
\]</span></p>
<p>These averages are mathematically integrals over the very high-dimensional space of integrals and thus very hard to evaluate analytically or numerically in the general case. Our goal is use Monte-Carlo sampling in the trajectory space to evaluate the above averages. To do this we have to sample trajectories from their probably distribution and we need to evaluate the likelihood for a response given a signal.</p>
<h2 id="estimating-the-likelihood">Estimating the likelihood</h2>
<p>The Probability density of a markovian trajectory can be expressed as</p>
<p><span class="math display">\[
\mathrm P(X) = \mathrm P(x_0,t_0;x_1,t_1;\ldots;x_{N-1},t_{N-1}) = \mathrm P(x_0,t_0 ) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}) \,.
\]</span></p>
<p>Therefore the problem of calculating the likelihood for a particular trajectory amounts to solving two independent problems:</p>
<ol type="1">
<li>estimating the probability density of the starting point <span class="math inline">\(\mathrm P (x_0, t_0)\)</span> of a response</li>
<li>calculating the transition probabilities <span class="math inline">\(\mathrm P(x_n,t_n|x_{n-1},t_{n-1})\)</span></li>
</ol>
<p>For a given chemical reaction network we can write down the chemical master equation. The chemical master equation contains all the information needed to compute the individual terms <span class="math inline">\(\mathrm P(x_n,t_n|x_{n-1},t_{n-1})\)</span> for the entire system.</p>
<p>To calculate the mutual information between <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal X\)</span> we have to consider the entire reaction network containing the components both in <span class="math inline">\(S\)</span> and in <span class="math inline">\(X\)</span>. The precise reaction dynamics of the response part of the chemical network crucially depend on the observed signal trajectory. Therefore the chemical master equation for the whole reaction network allows us to compute the likelihood of a response trajectory for a particular signal trajectory:</p>
<p><span class="math display">\[
\mathrm P(\mathcal X = X|\mathcal S = S) = \mathrm P(x_0,t_0;x_1,t_1;\ldots;x_{N-1},t_{N-1} | S) = \mathrm P(x_0,t_0 | S) \prod\limits^{N-1}_{n=1} \mathrm P(x_n,t_n|x_{n-1},t_{n-1}, S) \,.
\]</span></p>
<p><strong>TODO:</strong> describe how the transition rates follow from the master equation (probably follow the style of Cepeda-Humerez, et. al.)</p>
<h3 id="the-probability-density-for-the-starting-point-of-a-trajectory">The probability density for the starting point of a trajectory</h3>
<p>We first look at the term <span class="math inline">\(P_0 = \mathrm P(x_0,t_0 | S)\)</span>. Since <span class="math inline">\(S\)</span> is a trajectory in time we can directly conclude from causality that</p>
<p><span class="math display">\[
\mathrm P(x_0, t_0 | S) = \mathrm P(x_0, t_0 | S_{t \leq t_0})
\]</span></p>
<p>where <span class="math inline">\(S_{t \leq t_0}\)</span> is the temporal piece of the signal up to <span class="math inline">\(t_0\)</span>. We further suppose that the response network starts with no memory of the signal:</p>
<p><span class="math display">\[
\mathrm P(x_0, t_0 | S) = \mathrm P(x_0, t_0 | S_{t = t_0}) = \frac{\mathrm P((x_0, t_0), (s_0, t_0))}{\mathrm P(s_0, t_0)} \,.
\]</span></p>
<p>We estimate <span class="math inline">\(P_0\)</span> using gaussian kernel density estimation to approximate both, the joint distribution of <span class="math inline">\(X_0, S_0\)</span> and the marginal distribution of <span class="math inline">\(S_0\)</span>.</p>
<p>Knowing the probabilities of the initial condition of both response and signal we can directly estimate the mutual information of <span class="math inline">\(\mathcal{X}_{t=t_0}\)</span> and <span class="math inline">\(\mathcal{S}_{t=t_0}\)</span>:</p>
<p><span class="math display">\[
\mathrm I(\mathcal{X}_{t=t_0}, \mathcal{S}_{t=t_0}) = \int ds_0\int dx_0\; \mathrm{P}(x_0, s_0)\; \ln \frac{\mathrm{P} (x_0, s_0)}{\mathrm{P} (x_0) \mathrm{P} (s_0)}
\]</span></p>
<h3 id="estimating-the-logarithm-of-the-averaged-likelihood">Estimating the logarithm of the averaged likelihood</h3>
<p>To calculate the mutual information between trajectories we need to have a good estimate for <span class="math inline">\(\ln\left\langle \mathrm P(X | S) \right\rangle_\mathcal{S}\)</span>. We calculate this average by sampling of trajectories <span class="math inline">\((S^{(i)})_{i=1\ldots N_S}\)</span> from the probability distribution of <span class="math inline">\(\mathcal{S}\)</span>:</p>
<p><span class="math display">\[
\ln\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S} \approx \ln \frac{\sum^{N_S}_{i=1} \mathrm P(X|S^{(i)})}{N_S} = \ln \sum^{N_S}_{i=1} \mathrm P(X|S^{(i)}) - \ln N_S
\]</span></p>
<p>In practice (due to limited precision of floating-point arithmetic) it is only possible to evaluate the log-likelihood <span class="math inline">\(\ell(X|S) \equiv \ln\mathrm P(X|S)\)</span>. This means that the calculation of the averaged likelihood involves the quantity</p>
<p><span class="math display">\[
\ln \sum\limits^{N_S}_{i=1} \exp \ell(X|S^{(i)}) \equiv \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right)
\]</span></p>
<p>where <span class="math inline">\(\mathrm{LSE} : \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is called log-sum-exp . An interesting property of <span class="math inline">\(\mathrm{LSE}\)</span> is that it’s a smooth approximation to the <span class="math inline">\(\max\)</span> function. This means that for finite sample sizes the monte-carlo estimate of the averaged likelihood will always be too small!</p>
<p>We approximate the mutual information between trajectories as</p>
<p><span class="math display">\[
\mathrm{I}(\mathcal{X}; \mathcal{S}) = \left\langle \ln \frac{\mathrm{P} ( X |  S)}{\left\langle\mathrm P(X | S) \right\rangle_\mathcal{S}} \right\rangle_{\mathcal{X},\mathcal{S}} = \left\langle \ell ( X |  S) - \mathrm{LSE}\left( \ell(X|S^{(1)}),\ldots, \ell(X|S^{(N_S)})\right) + \ln N_S\right\rangle_{\mathcal{X},\mathcal{S}}
\]</span></p>
<p>which means that for finite amount of signal samples we will <em>systematically over-estimate</em> the mutual information. <strong>TODO: Is that really true? What about <span class="math inline">\(\ln N_S\)</span>?</strong> Even worse: the longer the trajectories the bigger the error becomes since the dimensionality of the space of possible signals is growing.</p>
<p>Another way to phrase this insight is that to get a good approximation for the logarithmic average likelihood, our set of signals that we use for monte-carlo sampling should contain many signals that produce a high likelihood. <strong>Therefore it probably is necessary to come up with a scheme to specifically sample signal trajectories for which the likelihood of a particular trajectory is high</strong>. On the other hand the results do not seem to get significantly better when averaging over more trajectories.</p>
<h2 id="simulating-chemical-networks">Simulating chemical networks</h2>
<p>As a model for the biochemical processing that takes place inside a cell we suppose that all interactions can be described by a chemical networks composed of a number of components (different types of molecules) and a number of reactions between them. As a very basic example we consider a highly simplified model of gene expression consisting of two components and four reactions:</p>
<p><span class="math display">\[\begin{align}
\emptyset &amp;\longrightarrow S \longrightarrow \emptyset \\
S &amp;\longrightarrow S + X \\
X &amp;\longrightarrow \emptyset \,.
\end{align}\]</span></p>
<p>We might interpret <span class="math inline">\(S\)</span> as some signal whose quantity varies stochastically. Then there is a certain chance that a signal molecule is registered by the cell which triggers the creation of an <span class="math inline">\(X\)</span>. The amount of <span class="math inline">\(X\)</span> then also decays over time. We call the trajectory of <span class="math inline">\(S\)</span> the “signal” and the trajectory of <span class="math inline">\(X\)</span> the “response”.</p>
</body>
</html>
